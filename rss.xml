<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0">
<channel>
<title>Arktronic.com</title>
<link>http://arktronic.com/</link>
<description>Arktronic.com blog posts</description>


        <item>
        <title>A redone blog, again</title>
        <link>http://arktronic.com/weblog/2018-12-28/a-redone-blog-again/</link>
        <pubDate>Fri, 28 Dec 2018 22:20:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2018-12-28T22:20:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;I&apos;m just about due for another recreation of my blog, so here we are. &lt;a href=&quot;/weblog/2014-08-09/new-blog-and-new-tea/&quot;&gt;Last time&lt;/a&gt;, I switched from a CMS to a statically generated website, using my own static site generator. This time, I&apos;m taking it a step further and, in addition to recreating my static site generator in .NET Core, harnessing the power of GitHub Actions for &lt;a href=&quot;https://en.wikipedia.org/wiki/CI/CD&quot;&gt;CI/CD&lt;/a&gt; purposes, and I&apos;m using GitHub Pages for hosting.&lt;/p&gt;
&lt;p&gt;My new static site generator isn&apos;t ready for a proper first release yet, so I won&apos;t talk much about it at this point. Instead, I&apos;ll focus on how I&apos;m using GitHub Actions to deploy blog changes to my staging blog and my production blog. First things first: GitHub Actions is currently in beta. I wouldn&apos;t be surprised if it goes through a lot of changes before it&apos;s released fully. While the beta is mostly functional, it has a number of limitations and bugs. That&apos;s beta software for you. &lt;strong&gt;What I&apos;m trying to get at is, the things I&apos;m going to describe below might not be applicable by the time this feature is out of beta.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The idea behind GitHub Actions is relatively straightforward: you configure &amp;quot;actions&amp;quot;, which are simply Docker containers, to do stuff for you. During the public beta, the only available trigger for these actions is a push. That&apos;s good enough for my purposes with this blog. Having never really worked with Docker before, I&apos;m probably doing a lot of stuff in a suboptimal manner, but it works, so there&apos;s that. I&apos;ve got two sections in my GitHub Actions workflow file: the staging section, and the production section. They are very similar, with just a few changes to point to different repos and such.&lt;/p&gt;
&lt;p&gt;Each section has two actions: a filter and a &amp;quot;do all the things&amp;quot; action. The filter simply ensures that the main action only executes when something is pushed to the correct branch - it&apos;s a branch filter. The setup for it looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    action &amp;quot;Filter to master branch&amp;quot; {
        uses = &amp;quot;actions/bin/filter@b2bea07&amp;quot;
        args = &amp;quot;branch master&amp;quot;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;uses&lt;/code&gt; directive says that &lt;a href=&quot;https://github.com/actions/bin/tree/b2bea0749eed6beb495a8fa194c071847af60ea1/filter&quot;&gt;this version of the Filter action&lt;/a&gt; should be used, with &lt;code&gt;args&lt;/code&gt; telling the Filter action that I want the branch name to match &amp;quot;master&amp;quot;.&lt;/p&gt;
&lt;p&gt;The &amp;quot;do all the things&amp;quot; action is where it gets weird. I didn&apos;t want to publish my own GitHub Action for building a static site using my generator for a few reasons. First, as I mentioned earlier, my generator isn&apos;t ready yet. Second, I was having trouble finding the right documentation for precisely how to publish a GitHub Action, what the necessary components are, and how everything works together. And third, I fully expect the publishing process and requirements to change during the beta period, and I&apos;m not overly interested in keeping up with all of these changes at the moment. So, I chose to do this in a bit of a backwards way. I decided to use the Docker CLI GitHub Action to build a Docker image that just happens to perform all the steps I want during the build. It&apos;s mildly horrifying, but here&apos;s what it currently looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    action &amp;quot;Docker-Staging&amp;quot; {
        uses = &amp;quot;actions/docker/cli@76ff57a&amp;quot;
        needs = [&amp;quot;Filter to master branch&amp;quot;]
        args = &amp;quot;build --build-arg GITHUB_TOKEN --build-arg GH_SOURCE_REPO=\&amp;quot;arktronic/arktronic.com--source\&amp;quot; --build-arg GH_SOURCE_BRANCH=\&amp;quot;master\&amp;quot; --build-arg GH_DEST_REPO=\&amp;quot;arktronic/staging.arktronic.com\&amp;quot; --build-arg GH_DEST_DEPLOY_KEY --build-arg GH_CNAME=\&amp;quot;staging.arktronic.com\&amp;quot; --build-arg ACCEPT_RISK=\&amp;quot;1\&amp;quot; .github/main_support&amp;quot;
        secrets = [&amp;quot;GITHUB_TOKEN&amp;quot;, &amp;quot;GH_DEST_DEPLOY_KEY&amp;quot;]
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&apos;s dissect that mess. First, we have the familiar &lt;code&gt;uses&lt;/code&gt; directive for &lt;a href=&quot;https://github.com/actions/docker/tree/76ff57a6c3d817840574a98950b0c7bc4e8a13a8/cli&quot;&gt;this action&lt;/a&gt;. The &lt;code&gt;needs&lt;/code&gt; directive helps to set up the chain of actions in my workflow. The &lt;code&gt;secrets&lt;/code&gt; directive tells GitHub which secret environment variables to make available to this action (there is a new tab in GitHub repo settings that lets you manage these secrets). Finally, &lt;code&gt;args&lt;/code&gt; is the messy part here. It&apos;s mostly build arguments, so if you ignore those, you&apos;re left with &lt;code&gt;build .github/main_support&lt;/code&gt;. That&apos;s not so bad. It&apos;s just telling Docker to use the Dockerfile located in the specified relative path. The Dockerfile located in &lt;code&gt;.github/main_support&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    FROM microsoft/dotnet:2.1-sdk

    ARG GITHUB_TOKEN
    ENV GITHUB_TOKEN=$GITHUB_TOKEN
    ARG GH_SOURCE_REPO
    ENV GH_SOURCE_REPO=$GH_SOURCE_REPO
    ARG GH_SOURCE_BRANCH
    ENV GH_SOURCE_BRANCH=$GH_SOURCE_BRANCH
    ARG GH_DEST_REPO
    ENV GH_DEST_REPO=$GH_DEST_REPO
    ARG GH_DEST_DEPLOY_KEY
    ENV GH_DEST_DEPLOY_KEY=$GH_DEST_DEPLOY_KEY
    ARG GH_PROD_DEPLOY_KEY
    ENV GH_PROD_DEPLOY_KEY=$GH_PROD_DEPLOY_KEY
    ARG GH_CNAME
    ENV GH_CNAME=$GH_CNAME
    ARG ACCEPT_RISK
    ENV ACCEPT_RISK=$ACCEPT_RISK

    COPY &amp;quot;entrypoint.sh&amp;quot; &amp;quot;/entrypoint.sh&amp;quot;
    RUN /entrypoint.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, I&apos;m using Microsoft&apos;s .NET Core 2.1 SDK Docker image to build and run my static site generator. After that, there are a whole bunch of &lt;code&gt;ARG&lt;/code&gt; and &lt;code&gt;ENV&lt;/code&gt; directives, which take all those &lt;code&gt;--build-arg&lt;/code&gt; arguments from the GitHub Actions workflow and transform them into environment variables that are available to the container. Finally, a single shell script is copied into the container and then executed. That shell script is where most of the magic happens:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    #!/bin/bash

    set -e

    cd /srv
    mkdir ~/.ssh
    echo -e &amp;quot;$GH_DEST_DEPLOY_KEY&amp;quot; &amp;gt;&amp;gt; ~/.ssh/id_rsa 2&amp;gt;/dev/null
    echo -e &amp;quot;$GH_PROD_DEPLOY_KEY&amp;quot; &amp;gt;&amp;gt; ~/.ssh/id_rsa 2&amp;gt;/dev/null
    chmod 600 ~/.ssh/id_rsa
    ssh-keyscan -t rsa github.com &amp;gt; ~/.ssh/known_hosts 2&amp;gt;/dev/null

    if [ &amp;quot;$ACCEPT_RISK&amp;quot; == &amp;quot;1&amp;quot; ]; then
    echo &amp;quot;Risk accepted - will force push!&amp;quot; &amp;gt;&amp;amp;2
    export PUSH_PARAMS=&amp;quot;-f&amp;quot;
    else
    echo &amp;quot;Risk not accepted - will perform dry run.&amp;quot; &amp;gt;&amp;amp;2
    export PUSH_PARAMS=&amp;quot;-n -f&amp;quot;
    fi

    git config --global user.name &amp;quot;BuildBot&amp;quot;
    git config --global user.email &amp;quot;noreply@example.com&amp;quot;

    git clone https://github.com/arktronic/genmaicha.git /srv/genmaicha
    dotnet publish -c Release -o /srv/genmaicha/publish /srv/genmaicha/Genmaicha/Genmaicha.csproj

    echo &amp;quot;Shallow cloning $GH_SOURCE_REPO, branch $GH_SOURCE_BRANCH&amp;quot;
    git clone --depth 1 --branch $GH_SOURCE_BRANCH https://$GITHUB_TOKEN@github.com/$GH_SOURCE_REPO.git input &amp;amp;&amp;gt;/dev/null
    dotnet genmaicha/publish/genmaicha.dll -o input/

    cd input/_build
    echo $GH_CNAME &amp;gt;CNAME
    git init
    git checkout -b master
    git add -A
    git commit -m &amp;quot;Recreate GitHub Pages&amp;quot;
    git remote add origin git@github.com:$GH_DEST_REPO.git
    echo &amp;quot;Pushing to $GH_DEST_REPO with params &apos;$PUSH_PARAMS&apos;&amp;quot;
    git push $PUSH_PARAMS origin master

    echo Done
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;set -e&lt;/code&gt; line tells Bash to stop executing the script after the first time it encounters a non-zero exit code, i.e., after the first error. This is important because otherwise we could potentially end up deploying something invalid.&lt;/p&gt;
&lt;p&gt;The next few lines set up SSH. There are two lines that try to throw a private key into &lt;code&gt;~/.ssh/id_rsa&lt;/code&gt;, but realistically, only one of them should succeed, since the workflow is set up to provide either the staging one or the production one - not both. That file then needs its permissions adjusted because SSH doesn&apos;t like it when private keys are world-readable. Go figure. And finally, in order to have SSH trust GitHub, its public key is retrieved and pushed to the &lt;code&gt;known_hosts&lt;/code&gt; file.&lt;/p&gt;
&lt;p&gt;After setting up SSH, I&apos;ve got some risk avoidance code, which will either let Git force push, or merely perform a dry run (&lt;code&gt;-n&lt;/code&gt;). Better safe than sorry. And then the local Git user info is set up for the eventual commit that will be created and force pushed.&lt;/p&gt;
&lt;p&gt;Building my static site generator just takes a couple of lines: cloning its repo, and executing &lt;code&gt;dotnet publish&lt;/code&gt; on it. After that, the blog source code is shallow cloned and then processed by the newly-built generator, with the output going into the &lt;code&gt;_build&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;Finally, a new Git repo is created in the &lt;code&gt;_build&lt;/code&gt; directory, everything is committed, and the contents are then force pushed to the target repo, which should already be set up to use GitHub Pages for hosting.&lt;/p&gt;
&lt;p&gt;All of this code (and, incidentally, this blog post) is currently located &lt;a href=&quot;https://github.com/arktronic/arktronic.com--source&quot;&gt;here&lt;/a&gt;. Feel free to take a look at it. And I would definitely recommend for people to check out GitHub Actions because it&apos;s an extremely powerful tool, which can be utilized for many useful purposes.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Optimize for cognitive load</title>
        <link>http://arktronic.com/weblog/2016-12-30/optimize-for-cognitive-load/</link>
        <pubDate>Fri, 30 Dec 2016 16:25:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-12-30T16:25:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;I recently read a rather interesting post by Martin Fowler regarding &lt;a href=&quot;http://martinfowler.com/bliki/FunctionLength.html&quot;&gt;function length&lt;/a&gt;, where he suggested that very small functions that encompass the implementation for a single intention are ideal. I have a somewhat different view of this argument, which also happens to touch on larger concerns of software design and even, to a certain extent, architecture. It is a holistic view, in the sense that the same goal is desired at multiple levels, from the function to the entire system.&lt;/p&gt;
&lt;p&gt;Specifically, I argue that instead of optimizing for specific low-level ideals such as function length, implementation vs. intention, dogmatic adherence to patterns or practices and so on, we should optimize for cognitive load. Let me start by explaining the general concept of cognitive load and how I interpret it.&lt;/p&gt;
&lt;h2&gt;Cognitive Load Theory&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Cognitive_load&quot;&gt;Cognitive Load Theory (CLT)&lt;/a&gt; was developed by John Sweller, an Australian professor specializing in educational psychology. According to CLT, when humans are learning, there are three types of cognitive load that occur: intrinsic, extraneous, and germane. Intrinsic cognitive load is effectively the difficulty that the topic being learned presents by itself. Not much can be done to affect that if the topic is to be learned. Extraneous cognitive load is, as the name suggests, considered unnecessary. It is created by the manner in which information is presented. For example, the extraneous cognitive load would be much higher if I were to verbally describe a geometric shape like a square, rather than just show a picture of one. Finally, germane cognitive load is involved in processing and construction of &lt;a href=&quot;https://en.wikipedia.org/wiki/Schema_(psychology)&quot;&gt;schemata&lt;/a&gt;. In the context of cognitive science, a schema is basically a grouping of learned information or a common pattern of processing such information.&lt;/p&gt;
&lt;p&gt;The idea of &lt;a href=&quot;https://www.nngroup.com/articles/minimize-cognitive-load/&quot;&gt;applying CLT to UX&lt;/a&gt; has been growing in popularity in recent years. Numerous articles have been written about optimizing visual and interaction design to reduce extraneous cognitive load. Unfortunately, I&apos;ve not seen much talk about this in regards to software design or architecture. It seems, we software engineers tend to focus more on the technical and less on the human side. But we need to take both into account when working on real (i.e., not personal/toy) projects if we want to increase maintainability and ease of development.&lt;/p&gt;
&lt;h2&gt;Application of CLT&lt;/h2&gt;
&lt;p&gt;My interpretation of cognitive load as it applies to software design is rooted in how many steps you must go through to understand the code involved in executing an API call, a workflow, a use case. This is, of course, a multifaceted problem with no clear generic solution. In most situations, in order to understand a particular flow to a &lt;em&gt;sufficient degree&lt;/em&gt;, you don&apos;t need to know all the minutiae involved in it. For example, if you&apos;re working on a RESTful API, you rarely (hopefully never) need to debug down to the level of TCP connections or Ethernet frames. Often, you don&apos;t even need to know the exact processes your framework of choice uses to translate an HTTP request to an appropriate function call in your code. And, depending on what aspect of the codebase you&apos;re trying to understand, you can often skip other important code in order to focus on what currently matters to you.&lt;/p&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;p&gt;So how does all this affect software design? Let&apos;s start with function length and go up from there. From a CLT perspective, a very long function that encompasses a significant amount of data processing will have high extraneous cognitive load when you analyze it because it will likely deal with multiple states that you have to keep in mind at all times while mentally processing the various permutations of conditionals and loops, what happens inside each of them, and how those previous decisions affect further conditionals and loops later on. This is a lot of information to keep track of, and so is inefficient for us to process.&lt;/p&gt;
&lt;p&gt;At the same time, very small functions will also have high extraneous cognitive load. The above load of dealing with state is replaced with the load of incessant context switching when you have to look at many different functions, then back again down a stack, then forward again from the next step, and so forth. This causes the same problem of presenting too much information to keep track of, and so is still inefficient.&lt;/p&gt;
&lt;p&gt;The ideal function length is somewhere inbetween. I hesitate to give concrete numbers, since there are multiple conflicting models of human &lt;a href=&quot;https://en.wikipedia.org/wiki/Working_memory&quot;&gt;working memory&lt;/a&gt;, which have different implications for how many items we can hold in our minds while working on a problem. Instead, I&apos;d suggest that you rely on your intuition to help determine the right balance. Look at examples of very long functions and of sets of very short ones, and try to analyze the flow through them. Seeing the issues with both by looking at examples at each extreme will help you to find a balance.&lt;/p&gt;
&lt;p&gt;Of course, there are other factors that play into your ability to analyze code. Descriptive function names, for example, are very important, as is a well thought out hierarchical (class/file/project) separation.&lt;/p&gt;
&lt;h3&gt;Design and architecture&lt;/h3&gt;
&lt;p&gt;Speaking of hierarchical separation, this can affect cognitive load in a different way: well-designed separation can improve germane cognitive load. If you&apos;re working on a well-designed codebase for a significant amount of time, your mind will use schemata to quickly guide you to the correct project or directory or file &amp;quot;without thinking&amp;quot;. You are probably familiar with this phenomenon already: working on such codebases will let you quickly find the location of some code in question even if you&apos;re not sure where it is precisely, because you&apos;re familiar with the overall design of the system.&lt;/p&gt;
&lt;p&gt;Conversely, codebases that are not well-designed will hamper your ability to find code whose precise location you don&apos;t already remember. This can be attributed to the inability to form a cohesive schema related to this codebase, since code is haphazardly separated without a clear hierarchy or other organizational means.&lt;/p&gt;
&lt;p&gt;Architectural and design patterns will often help to organize code in a way that we can process more easily, but we must be careful not to apply too many such patterns or apply them improperly to avoid confusion. The use of well-known patterns enhances our ability to process and understand a codebase because we have already developed (or can begin to develop) schemata to deal with these patterns.&lt;/p&gt;
&lt;h2&gt;Bringing everything together&lt;/h2&gt;
&lt;p&gt;All this human-centric discussion doesn&apos;t negate technical needs. Certain choices must be made for technical reasons, and sometimes these choices will make part of a codebase more difficult to analyze. As always, a balance must be struck. Modern compilers and interpreters are extraordinarily adept at optimizing code for execution performance, so low level optimizations are rarely needed these days. Technical needs will most often be expressed at higher levels. As an example, when system extensibility is required, certain architectural and design decisions must be made to support this requirement. Unfortunately, these decisions may lead to worsened readability, but you don&apos;t always have a great way to balance out system needs with human analysis needs.&lt;/p&gt;
&lt;p&gt;I urge you to keep the human factors dicussed here in mind when performing any task from the writing of functions to the design of systems. While different goals may take precedence at different times, simply keeping these concerns in mind will allow you to create better software.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Virtual Hackintosh, part 3: the hard route</title>
        <link>http://arktronic.com/weblog/2016-12-18/virtual-hackintosh-part-3-the-hard-route/</link>
        <pubDate>Mon, 19 Dec 2016 00:15:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-12-19T00:15:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;In &lt;a href=&quot;/weblog/2016-12-10/virtual-hackintosh-part-1-the-concepts/&quot;&gt;part 1&lt;/a&gt; I explained some of the basic concepts behind the hackintosh, and in &lt;a href=&quot;/weblog/2016-12-11/virtual-hackintosh-part-2-the-easy-route/&quot;&gt;part 2&lt;/a&gt; I showed the easy way to create a hackintosh VM using VirtualBox. In this post I&apos;ll show the harder, but more flexible, way, which will allow you to have a custom screen resolution as well as connectivity to iCloud and iMessage.&lt;/p&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;As I mentioned last time, there is also currently no way to have accelerated graphics in VirtualBox due to a lack of drivers, and there may be problems with audio as well. I am still unaware of any ways to work around this.&lt;/p&gt;
&lt;h2&gt;The guide&lt;/h2&gt;
&lt;p&gt;The instructions here work as of macOS Sierra 10.12.1/10.12.2 and VirtualBox 5.1.10. If you are using other versions, things may have changed, and so there is no guarantee that this guide will be accurate.&lt;/p&gt;
&lt;h3&gt;Step 0: Prerequisites&lt;/h3&gt;
&lt;p&gt;In order to follow this guide, you will need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The 16GB or higher installer disk with the macOS installer from &lt;a href=&quot;/weblog/2016-12-11/virtual-hackintosh-part-2-the-easy-route/&quot;&gt;part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A real Mac with Internet access&lt;/li&gt;
&lt;li&gt;A VM host machine with VirtualBox installed and plenty of free space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 1: Download and install Clover&lt;/h3&gt;
&lt;p&gt;Download the latest &lt;a href=&quot;https://sourceforge.net/projects/cloverefiboot/files/&quot;&gt;Clover&lt;/a&gt; installer (r3961 at time of writing) to your real Mac. Right-click the &lt;code&gt;.pkg&lt;/code&gt; (package) file and select Open. Change the install location to the installer disk, which should be named &amp;quot;Install macOS Sierra&amp;quot;. After that, click the &lt;code&gt;Customize&lt;/code&gt; button and ensure that the following options are selected:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-clover-install.jpg&quot; alt=&quot;Clover options&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Click &lt;code&gt;Install&lt;/code&gt; and enter your password to proceed. macOS might show a warning that the &amp;quot;package is incompatible with this version of macOS and may fail to install&amp;quot;. It should be safe to ignore that warning and click &lt;code&gt;Install Anyway&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;After the installation completes, copy the downloaded Clover installation package to the &amp;quot;Install macOS Sierra&amp;quot; installer disk - you&apos;ll need it again later.&lt;/p&gt;
&lt;h3&gt;Step 2: Configure Clover&lt;/h3&gt;
&lt;p&gt;As I explained in &lt;a href=&quot;/weblog/2016-12-10/virtual-hackintosh-part-1-the-concepts/&quot;&gt;part 1&lt;/a&gt;, Clover is a bootloader capable of emulating EFI and various related firmware components. These components need to be configured so that upon bootup Clover sends the necessary information to macOS for correct operation. These configuration settings are stored in a file called &lt;code&gt;config.plist&lt;/code&gt;. When placed in the correct location, this file (and optionally other associated files) will be read by Clover and used to configure the information sent to macOS.&lt;/p&gt;
&lt;p&gt;In order to expedite matters, I&apos;ve created a &amp;quot;base&amp;quot; configuration file that can be used as a starting point. &lt;a href=&quot;https://gist.githubusercontent.com/arktronic/de40f5bf99b2d56b47bdb60b32210f7a/raw/config.plist&quot;&gt;Download this file&lt;/a&gt; to your real Mac. Ensure that it&apos;s named &lt;code&gt;config.plist&lt;/code&gt; after the download and that its contents haven&apos;t been mangled by your browser.&lt;/p&gt;
&lt;p&gt;Modifying the configuration file by hand can be a bit of a pain, so we&apos;ll use Clover Configurator to automate this process a bit. &lt;a href=&quot;http://mackie100projects.altervista.org/download-clover-configurator/&quot;&gt;Download the Vibrant edition&lt;/a&gt; of the Clover Configurator.&lt;/p&gt;
&lt;p&gt;Open the downloaded &lt;code&gt;config.plist&lt;/code&gt; file in Clover Configurator.&lt;/p&gt;
&lt;p&gt;In the &amp;quot;SMBIOS&amp;quot; section, click the &amp;quot;magic wand&amp;quot; button to create an emulated Mac profile. Select the type of device you want to emulate, and then select the specific version from the primary dropdown. After that, click the two &amp;quot;shake&amp;quot; buttons to create a random serial number. After clicking &lt;code&gt;OK&lt;/code&gt;, copy the &amp;quot;Serial Number&amp;quot; field to the &amp;quot;Board Serial Number&amp;quot; field and append five random letters or numbers to the end. Then copy that resulting board serial number into the &amp;quot;MLB&amp;quot; field under the &amp;quot;Rt Variables&amp;quot; section. This is all demonstrated here for reference:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-clover-config.gif&quot; alt=&quot;Clover configuration&quot; /&gt;&lt;/p&gt;
&lt;p&gt;After you&apos;ve completed the above steps, open a new Terminal window and type in &lt;code&gt;uuidgen&lt;/code&gt; (this will output a UUID on the next line). Copy this UUID to the clipboard. Go back to Clover Configurator and the &amp;quot;SMBIOS&amp;quot; section, and paste the UUID into the &amp;quot;SmUUID&amp;quot; field.&lt;/p&gt;
&lt;p&gt;To set your desired screen resolution, go to the &amp;quot;Gui&amp;quot; section, and select the screen resolution you want from the dropdown. The default resolution is 1920x1080.&lt;/p&gt;
&lt;p&gt;To set the amount of RAM you want to give to the VM, go to the &amp;quot;SMBIOS&amp;quot; section, and change the Memory information accordingly. The amount (marked &amp;quot;Size&amp;quot;) is the only value that you should change if you&apos;d like to choose anything other than the default of 4096MB.&lt;/p&gt;
&lt;p&gt;Then you can close Clover Configurator. The &lt;code&gt;config.plist&lt;/code&gt; file should have your modifications saved in it.&lt;/p&gt;
&lt;h3&gt;Step 3: Finalize the macOS installer changes&lt;/h3&gt;
&lt;p&gt;You&apos;ll need to copy the &lt;code&gt;config.plist&lt;/code&gt; file to two locations: one on the main &amp;quot;Install macOS Sierra&amp;quot; partition next to the Clover installation package, and one on the EFI partition so that Clover will see it. The installer disk&apos;s EFI partition should already be mounted after Clover&apos;s installation process. If not, you can mount it like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkdir /Volumes/EFI
sudo mount -t msdos /dev/disk2s1 /Volumes/EFI
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If necessary, replace &lt;code&gt;disk2s1&lt;/code&gt; above with the appropriate disk number. Note that &lt;code&gt;s1&lt;/code&gt; should still be used, as the EFI System Partition should always be partition number 1.&lt;/p&gt;
&lt;p&gt;Copy the file to &lt;code&gt;EFI/CLOVER&lt;/code&gt; on the EFI partition, replacing the existing &lt;code&gt;config.plist&lt;/code&gt; there. After you&apos;ve copied the file to both locations, eject the installer disk, unmounting all partitions.&lt;/p&gt;
&lt;h3&gt;Step 4: Create the VM&lt;/h3&gt;
&lt;p&gt;Open VirtualBox on your VM host machine and create a new VM:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-vbox-create.jpg&quot; alt=&quot;Create VM&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, use the above settings. In later screens, select the same amount of RAM as you chose in Clover Configurator (4096MB by default), and create a dynamically allocated 60GB VDI virtual hard drive.&lt;/p&gt;
&lt;h3&gt;Step 5: Configure the VM&lt;/h3&gt;
&lt;p&gt;Certain VirtualBox settings are easier to configure via the command line, and some are just unavailable in the GUI. The &lt;code&gt;VBoxManage&lt;/code&gt; command line tool allows for terminal-based control over VMs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Before using VBoxManage, close all VirtualBox windows! Otherwise, certain settings may fail to apply, or you might even end up with a corrupted VM.&lt;/p&gt;
&lt;p&gt;Enter the following commands to further configure the &lt;code&gt;macOS&lt;/code&gt; VM, replacing the screen resolution with what you chose in Clover Configurator and the &lt;code&gt;(copyrighted Apple key)&lt;/code&gt; with the actual key that you can easily find online:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage modifyvm macOS --firmware bios --vram 128 --usb on --usbehci off --usbxhci off

VBoxManage setextradata macOS &amp;quot;CustomVideoMode1&amp;quot; &amp;quot;1920x1080x32&amp;quot;

VBoxManage setextradata macOS &amp;quot;VBoxInternal/Devices/smc/0/Config/DeviceKey&amp;quot; &amp;quot;(copyrighted Apple key)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Step 6: Connect your installer disk&lt;/h3&gt;
&lt;p&gt;This should be a simple step, but unfortunately VirtualBox makes it difficult. Although it is possible to connect USB devices to virtual machines, you cannot normally boot from them, and that is precisely what we need to do. Luckily, VirtualBox does provide a &lt;a href=&quot;https://www.virtualbox.org/manual/ch09.html#rawdisk&quot;&gt;workaround&lt;/a&gt; for this via a virtual raw disk access file.&lt;/p&gt;
&lt;p&gt;First, connect your installer disk to your VM host machine. Next, you&apos;ll need to determine its identifier on your machine. In Windows, you can do so on the command line by entering the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wmic diskdrive list brief
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In *nix, you can inspect the output from the &lt;code&gt;mount&lt;/code&gt;, &lt;code&gt;df -h&lt;/code&gt;, &lt;code&gt;lsblk&lt;/code&gt;, or &lt;code&gt;parted -l&lt;/code&gt; commands.&lt;/p&gt;
&lt;p&gt;Determine which disk represents your installer disk and remember its Windows DeviceID (usually in the format of &lt;code&gt;\\.\PHYSICALDRIVE#&lt;/code&gt;) or *nix device name (usually in the format of &lt;code&gt;/dev/sdX&lt;/code&gt; or &lt;code&gt;/dev/disk#&lt;/code&gt;). It is important to note that you should use the name of the device itself and not of its partitions.&lt;/p&gt;
&lt;p&gt;Next, create the raw disk access file. In Windows, you will need to launch a new command prompt as Administrator. In *nix, you may need to prefix the command with &lt;code&gt;sudo&lt;/code&gt;. Enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage internalcommands createrawvmdk -filename C:\usb.vmdk -rawdisk \\.\PHYSICALDRIVE#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace the filename above with an appropriate *nix path if you&apos;re not on Windows, and replace the physical drive identifier as needed.&lt;/p&gt;
&lt;p&gt;In the same administrative command prompt, enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage storageattach macOS --storagectl SATA --port 2 --type hdd --hotpluggable on --medium C:\usb.vmdk
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace the filename above if necessary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE: This differs from the equivalent step in part 2.&lt;/strong&gt; Launch VirtualBox as Administrator in order to allow it to access the raw disk. Go to the macOS VM&apos;s settings, and look at the &amp;quot;Storage&amp;quot; section. Ensure that you have two hard disks and one optical drive under the SATA controller. Then, select the main macOS drive that should be the first item under the controller and change its SATA port from 0 to 3. This is necessary for the installer disk to be able to boot in BIOS mode.&lt;/p&gt;
&lt;h3&gt;Step 7: Install macOS&lt;/h3&gt;
&lt;p&gt;Start the macOS VM. If VirtualBox asks to pick a startup disk or ISO, just cancel out of that dialog.&lt;/p&gt;
&lt;p&gt;Once macOS boots up, use the Disk Utility to format the 60GB disk, naming it &amp;quot;Macintosh HD&amp;quot;, and then install macOS onto it.&lt;/p&gt;
&lt;p&gt;After the installer completes, there will be a first run setup wizard. When prompted to sign in with your Apple ID, you should be able to sign in if you so choose. When prompted to send diagnostics and usage data to Apple, uncheck that box. Since this isn&apos;t real Mac hardware, it would not be helpful for Apple to try diagnosing any issues from unsupported configurations.&lt;/p&gt;
&lt;h3&gt;Step 8: Install Clover in the VM&lt;/h3&gt;
&lt;p&gt;Although you are already able to boot the VM, VirtualBox is currently using the installer disk&apos;s copy of Clover. You&apos;ll need to install it on the 60GB drive before removing the installer disk.&lt;/p&gt;
&lt;p&gt;Open the &amp;quot;Install macOS Sierra&amp;quot; partition. You should see the Clover installation package and the &lt;code&gt;config.plist&lt;/code&gt; file there, which you copied earlier. Install Clover on &amp;quot;Macintosh HD&amp;quot; with the same options selected as in step 1 above.&lt;/p&gt;
&lt;p&gt;After the Clover installation, the EFI partition of the main drive should be mounted. If not, mount it using the same commands as in step 3 above, but make sure that you are mounting the main disk&apos;s EFI partition and not that of the installer disk.&lt;/p&gt;
&lt;p&gt;Copy the &lt;code&gt;config.plist&lt;/code&gt; file from the installer disk to the &lt;code&gt;EFI/CLOVER&lt;/code&gt; folder on the EFI partition, replacing the existing one there.&lt;/p&gt;
&lt;h3&gt;Finishing up&lt;/h3&gt;
&lt;p&gt;At this point, it&apos;s a good idea to shut down the VM, go to its storage settings, and remove the raw disk access VMDK file. You may want to remove it from VirtualBox&apos;s Virtual Media Manager as well.&lt;/p&gt;
&lt;p&gt;You will want to disable power saving in the VM. VirtualBox seems to dislike it when the guest OS tries to sleep. Go to System Preferences, select Energy Saver, and disable both the computer and the display from going to sleep in the guest OS.&lt;/p&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;In this guide I showed you how to create a mostly-functional macOS VM. I suspect that its functionality can be improved with further modifications to the &lt;code&gt;config.plist&lt;/code&gt; file and possibly with a custom DSDT. I leave these possibilities as an exercise for the reader.&lt;/p&gt;
&lt;p&gt;Lastly, these posts wouldn&apos;t have been possible without the various sources of information that I used to gain a better understanding of macOS internals and the various jargon involved. &lt;a href=&quot;http://www.insanelymac.com/forum/topic/309654-run-vanilla-os-x-el-capitan-sierra-yosemite-or-mavericks-in-virtualbox-5010-on-a-windows-host/&quot;&gt;This thread&lt;/a&gt; (and many others) on InsanelyMac, the Clover &lt;a href=&quot;https://clover-wiki.zetam.org/Configuration&quot;&gt;configuration reference&lt;/a&gt;, and helpful pointers from Auri&apos;s post on &lt;a href=&quot;https://aurir.wordpress.com/2016/11/20/how-to-setting-up-macos-x-sierra-on-virtualbox-for-xamarin-development-with-visual-studio/&quot;&gt;setting up a virtual macOS Xamarin environment&lt;/a&gt; are among those I found quite useful. And it goes without saying that those who developed the tools and systems used here deserve the most credit.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Virtual Hackintosh, part 2: the easy route</title>
        <link>http://arktronic.com/weblog/2016-12-11/virtual-hackintosh-part-2-the-easy-route/</link>
        <pubDate>Mon, 12 Dec 2016 01:15:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-12-12T01:15:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;In &lt;a href=&quot;/weblog/2016-12-10/virtual-hackintosh-part-1-the-concepts/&quot;&gt;part 1&lt;/a&gt; I explained some of the basic concepts behind the hackintosh. In this post I&apos;ll show the easy way to get a VirtualBox-based hackintosh system up and running.&lt;/p&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;Although the steps in this post are relatively simple and straightforward, that simplicity comes with a price. There are at least two issues that will detract from the experience: available screen resolutions are limited, and iCloud/iMessage connectivity does not work.&lt;/p&gt;
&lt;p&gt;Aside from the above issues, there is also currently no way to have accelerated graphics in VirtualBox due to a lack of drivers, and there may be problems with audio as well. I&apos;ve not figured out any way around that, unfortunately.&lt;/p&gt;
&lt;h2&gt;The guide&lt;/h2&gt;
&lt;p&gt;The instructions here work as of macOS Sierra 10.12.1 and VirtualBox 5.1.10. If you are using other versions, things may have changed, and so there is no guarantee that this guide will be accurate.&lt;/p&gt;
&lt;h3&gt;Step 0: Prerequisites&lt;/h3&gt;
&lt;p&gt;In order to follow this guide, you will need:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An empty 16GB or higher flash drive or external hard drive&lt;/li&gt;
&lt;li&gt;A real Mac with plenty of free space and Internet access&lt;/li&gt;
&lt;li&gt;A VM host machine with VirtualBox installed and plenty of free space&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 1: Create an installer disk&lt;/h3&gt;
&lt;p&gt;The first step in making a hackintosh is the creation of installation media. Start by downloading macOS Sierra from the App Store on your real Mac:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-sierra-download.jpg&quot; alt=&quot;Download Sierra&quot; /&gt;&lt;/p&gt;
&lt;p&gt;You should see it in Applications after the download has completed:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-sierra-in-applications.jpg&quot; alt=&quot;Sierra in Applications&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Connect your external drive and use the Disk Utility to erase it, using the following settings:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-erase-drive.jpg&quot; alt=&quot;Erase drive&quot; /&gt;&lt;/p&gt;
&lt;p&gt;You should leave the name &amp;quot;Untitled&amp;quot; as is, since it&apos;ll make the next step easier. And besides, it&apos;ll be overwritten in the next step anyway.&lt;/p&gt;
&lt;p&gt;Open a new Terminal window and enter the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;sudo /Applications/Install\ macOS\ Sierra.app/Contents/Resources/createinstallmedia --volume /Volumes/Untitled --applicationpath /Applications/Install\ macOS\ Sierra.app&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;After entering your password and confirming the action, you&apos;ll need to wait while your drive is reformatted and macOS installer data is copied onto it. If you are using a slow USB flash drive, be prepared to wait.&lt;/p&gt;
&lt;p&gt;Once the process completes, eject the drive.&lt;/p&gt;
&lt;h3&gt;Step 2: Create the VM&lt;/h3&gt;
&lt;p&gt;Open VirtualBox and create a new VM:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-vbox-create.jpg&quot; alt=&quot;Create VM&quot; /&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, use the above settings. In later screens, select 4096MB RAM, and create a dynamically allocated 60GB VDI virtual hard drive.&lt;/p&gt;
&lt;h3&gt;Step 3: Configure the VM&lt;/h3&gt;
&lt;p&gt;Certain VirtualBox settings are easier to configure via the command line, and some are just unavailable in the GUI. The &lt;code&gt;VBoxManage&lt;/code&gt; command line tool allows for terminal-based control over VMs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;IMPORTANT&lt;/strong&gt;: Before using VBoxManage, close all VirtualBox windows! Otherwise, certain settings may fail to apply, or you might even end up with a corrupted VM.&lt;/p&gt;
&lt;p&gt;Enter the following commands to further configure the &lt;code&gt;macOS&lt;/code&gt; VM, replacing &lt;code&gt;(copyrighted Apple key)&lt;/code&gt; with the actual key that you can easily find online:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage modifyvm macOS --vram 128 --usb on --usbehci off --usbxhci off

VBoxManage setextradata macOS &amp;quot;VBoxInternal/Devices/smc/0/Config/DeviceKey&amp;quot; &amp;quot;(copyrighted Apple key)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, choose a &lt;a href=&quot;https://www.virtualbox.org/manual/ch03.html#efividmode&quot;&gt;screen resolution&lt;/a&gt; integer for the VM:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;0: 640x480&lt;/li&gt;
&lt;li&gt;1: 800x600&lt;/li&gt;
&lt;li&gt;2: 1024x768&lt;/li&gt;
&lt;li&gt;3: 1280x1024&lt;/li&gt;
&lt;li&gt;4: 1440x900&lt;/li&gt;
&lt;li&gt;5: 1920x1200&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Enable your selected resolution like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage setextradata macOS &amp;quot;VBoxInternal2/EfiGopMode&amp;quot; 4
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Step 4: Connect your installer disk&lt;/h3&gt;
&lt;p&gt;This should be a simple step, but unfortunately VirtualBox makes it difficult. Although it is possible to connect USB devices to virtual machines, you cannot normally boot from them, and that is precisely what we need to do. Luckily, VirtualBox does provide a &lt;a href=&quot;https://www.virtualbox.org/manual/ch09.html#rawdisk&quot;&gt;workaround&lt;/a&gt; for this via a virtual raw disk access file.&lt;/p&gt;
&lt;p&gt;First, connect your installer disk to your VM host machine. Next, you&apos;ll need to determine its identifier on your machine. In Windows, you can do so on the command line by entering the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;wmic diskdrive list brief
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In *nix, you can inspect the output from the &lt;code&gt;mount&lt;/code&gt;, &lt;code&gt;df -h&lt;/code&gt;, &lt;code&gt;lsblk&lt;/code&gt;, or &lt;code&gt;parted -l&lt;/code&gt; commands.&lt;/p&gt;
&lt;p&gt;Determine which disk represents your installer disk and remember its Windows DeviceID (usually in the format of &lt;code&gt;\\.\PHYSICALDRIVE#&lt;/code&gt;) or *nix device name (usually in the format of &lt;code&gt;/dev/sdX&lt;/code&gt; or &lt;code&gt;/dev/disk#&lt;/code&gt;). It is important to note that you should use the name of the device itself and not of its partitions.&lt;/p&gt;
&lt;p&gt;Next, create the raw disk access file. In Windows, you will need to launch a new command prompt as Administrator. In *nix, you may need to prefix the command with &lt;code&gt;sudo&lt;/code&gt;. Enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage internalcommands createrawvmdk -filename C:\usb.vmdk -rawdisk \\.\PHYSICALDRIVE#
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace the filename above with an appropriate *nix path if you&apos;re not on Windows, and replace the physical drive identifier as needed.&lt;/p&gt;
&lt;p&gt;In the same administrative command prompt, enter the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage storageattach macOS --storagectl SATA --port 2 --type hdd --hotpluggable on --medium C:\usb.vmdk
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Replace the filename above if necessary.&lt;/p&gt;
&lt;h3&gt;Step 5: Install macOS&lt;/h3&gt;
&lt;p&gt;Launch VirtualBox as Administrator in order to allow it to access the raw disk. Then, start the macOS VM. If VirtualBox asks to pick a startup disk or ISO, just cancel out of that dialog.&lt;/p&gt;
&lt;p&gt;Once macOS boots up, use the Disk Utility to format the 60GB disk and then install macOS onto it.&lt;/p&gt;
&lt;p&gt;After the installer completes, there will be a first run setup wizard. When prompted to sign in with your Apple ID, select &amp;quot;Don&apos;t sign in&amp;quot; since you won&apos;t be able to anyway. Also, when prompted to send diagnostics and usage data to Apple, uncheck that box. Since this isn&apos;t real Mac hardware, it would not be helpful for Apple to try diagnosing any issues from unsupported configurations.&lt;/p&gt;
&lt;h3&gt;Finishing up&lt;/h3&gt;
&lt;p&gt;Once macOS is up and running, you&apos;re effectively done. Congratulations!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hackintosh-running.jpg&quot; alt=&quot;Running&quot; /&gt;&lt;/p&gt;
&lt;p&gt;At this point, it&apos;s a good idea to shut down the VM, go to its storage settings, and remove the raw disk access VMDK file. You may want to remove it from VirtualBox&apos;s Virtual Media Manager as well.&lt;/p&gt;
&lt;p&gt;If you would like to see the normal Apple start up screen instead of the verbose text mode, you can enter the following &lt;em&gt;after&lt;/em&gt; closing all VirtualBox windows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;VBoxManage setextradata macOS &amp;quot;VBoxInternal2/EfiBootArgs&amp;quot; &amp;quot;usb=0x800 keepsyms=1 -serial=0x1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The default boot arguments contain &amp;quot;-v&amp;quot;, which enables verbose mode. The above command will overwrite those defaults to remove verbose mode.&lt;/p&gt;
&lt;p&gt;Finally, you will want to disable power saving. VirtualBox seems to dislike it when the guest OS tries to sleep. Go to System Preferences, select Energy Saver, and disable both the computer and the display from going to sleep in the guest OS.&lt;/p&gt;
&lt;h2&gt;Next time&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&quot;/weblog/2016-12-18/virtual-hackintosh-part-3-the-hard-route/&quot;&gt;part 3&lt;/a&gt;, I&apos;ll explain the more difficult route of creating a virtual hackintosh using Clover, which will allow for more screen resolutions and iCloud/iMessage connectivity.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Virtual Hackintosh, part 1: the concepts</title>
        <link>http://arktronic.com/weblog/2016-12-10/virtual-hackintosh-part-1-the-concepts/</link>
        <pubDate>Sat, 10 Dec 2016 17:35:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-12-10T17:35:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;Virtualization powers a lot of infrastructure today, and there have been countless advances made in this field during the last few years. CPUs support more and more advanced hypervisor scenarios, while operating systems gain better and better native virtualization capabilities. Today, even macOS is virtualizable to a significant degree using free, open source tools.&lt;/p&gt;
&lt;p&gt;In this post, as well as parts &lt;a href=&quot;/weblog/2016-12-11/virtual-hackintosh-part-2-the-easy-route/&quot;&gt;2&lt;/a&gt; and &lt;a href=&quot;/weblog/2016-12-18/virtual-hackintosh-part-3-the-hard-route/&quot;&gt;3&lt;/a&gt;, I&apos;ll be focusing on using VirtualBox as the VM host, because it is open source, cross-platform, and it generally works well for this purpose. There are ways to get VMware and QEMU/KVM to host macOS; if you&apos;re already in one of those ecosystems, you should be able to find an online guide to help. I&apos;m unaware of any successful attempts at getting Hyper-V to host it, but it could conceivably be done.&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The only reason why I decided to attempt virtualizing macOS is because it sounded like an interesting challenge. I have no particular desire to use that operating system, but finding a way to get it working is intriguing to me.&lt;/p&gt;
&lt;h2&gt;The theory and the terminology&lt;/h2&gt;
&lt;p&gt;If you&apos;re new to the hackintosh scene, you might feel a bit overwhelmed with jargon: DSDT, Clover, kext, Chameleon, SMC, and so forth. Let&apos;s start with macOS hardware requirements and go from there. &lt;em&gt;Disclaimer: I can&apos;t guarantee the accuracy of the information provided here. This is just my attempt to reconcile the various pieces of info floating around on the web. If something is incorrect, please let me know, and I&apos;ll fix it.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Hardware requirements&lt;/h3&gt;
&lt;p&gt;Over the years Apple has released quite a few devices that run modern versions of macOS. They all have a couple of things in common: they run on Intel CPUs, and they have specialized hardware to differentiate a Mac from a regular Intel-based machine.&lt;/p&gt;
&lt;p&gt;This brings us to the first term: &lt;strong&gt;SMC (System Management Controller)&lt;/strong&gt;. The SMC has multiple functions, including controlling LEDs and power management, but for our purposes it only has one interesting function: identifying the hardware that it&apos;s running on as a genuine Mac. macOS checks the SMC for a specific string upon booting up, and if the string is missing or corrupted, it will crash with a kernel panic.&lt;/p&gt;
&lt;p&gt;This string check is done using an aptly-named &lt;strong&gt;kext (kernel extension)&lt;/strong&gt; called &lt;code&gt;Dont_Steal_Mac_OS_X&lt;/code&gt;. A kernel extension is effectively a dynamically loadable kernel module - code running with very high privileges inside kernel space. There are kexts that function as hardware drivers, for example.&lt;/p&gt;
&lt;p&gt;Macs have other common hardware found in many non-Mac machines, such as audio cards, graphics cards, wireless NICs, and so on. Since Apple only has to support a limited number of these devices, it can sometimes be a challenge to get certain functionality working on unsupported hardware. Custom kexts are often the solution.&lt;/p&gt;
&lt;p&gt;In order to determine what hardware is present, macOS enlists the help of Intel&apos;s &lt;strong&gt;EFI (Extensible Firmware Interface)&lt;/strong&gt;, a modern replacement for the venerable &lt;strong&gt;BIOS (Basic Input/Output System)&lt;/strong&gt; firmware, which dates back to the 1970s. In addition to allowing macOS to boot, EFI contains (or allows access to) a lot of metadata in various tables that macOS uses to look up what hardware it needs to initialize. One of these sets of tables is the &lt;strong&gt;SMBIOS (System Management BIOS)&lt;/strong&gt;. It contains information about the currently running firmware, as well as metadata about the motherboard and certain peripherals that are attached to it. For example, macOS uses SMBIOS information to determine how much RAM the computer has and which RAM slot(s) are occupied.&lt;/p&gt;
&lt;p&gt;EFI also allows access to various parts of &lt;strong&gt;ACPI (Advanced Configuration and Power Interface)&lt;/strong&gt;, which has more information about the computer. Part of ACPI is the &lt;strong&gt;DSDT (Differentiated System Description Table)&lt;/strong&gt;, which contains system metadata as well as executable code to allow for correct functionality of the operating system, especially when it comes to things like power management.&lt;/p&gt;
&lt;h3&gt;Software solutions&lt;/h3&gt;
&lt;p&gt;On a real Mac, the EFI, SMBIOS, ACPI tables, and various other firmware components are all tuned for best compatibility with macOS. On regular machines, they tend to be tuned for compatibility with Windows, or sometimes not tuned at all, or even tuned incorrectly. In order to solve the various issues created by these incompatibilities, a custom bootloader can be used to tweak the firmware information that macOS sees. &lt;strong&gt;Chameleon&lt;/strong&gt; is one such bootloader. &lt;strong&gt;Clover&lt;/strong&gt; is another. I&apos;ll be focusing on Clover in these posts, as it is the current preferred bootloader in the hackintosh community.&lt;/p&gt;
&lt;p&gt;Clover functions as an EFI emulator, presenting its customizable view of the computer, including all of the aforementioned tables and other firmware components, to the operating system that it boots. It&apos;s able to function within an existing EFI or UEFI environment, as well as within a legacy BIOS one.&lt;/p&gt;
&lt;p&gt;The successful creation of a hackintosh essentially boils down to correctly configuring Clover. Although there is sometimes a need for custom kexts to enable more or better functionality of various components, that is generally not needed for basic operation. Custom DSDT modifications, which are presented by Clover to the operating system, will often solve many problems ranging from graphics card initialization to power management, and a lot more.&lt;/p&gt;
&lt;p&gt;Configuring a virtual hackintosh is similar to configuring a physical one, with an important difference in that the emulated hardware is nearly identical across most machines. Pretty much the only hardware that is not the same is the CPU, and there are workarounds to address CPU-specific issues.&lt;/p&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&quot;/weblog/2016-12-11/virtual-hackintosh-part-2-the-easy-route/&quot;&gt;next part&lt;/a&gt;, I&apos;ll explain the easy, though limited, route to creating a virtual hackintosh. In the &lt;a href=&quot;/weblog/2016-12-18/virtual-hackintosh-part-3-the-hard-route/&quot;&gt;third part&lt;/a&gt;, I&apos;ll explain the more difficult, though more functional, route. Stay tuned!&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Project PiNES</title>
        <link>http://arktronic.com/weblog/2016-02-11/project-pines/</link>
        <pubDate>Thu, 11 Feb 2016 06:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-02-11T06:00:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;There are quite a few projects out there that stuff a &lt;a href=&quot;https://en.wikipedia.org/wiki/Raspberry_Pi&quot;&gt;Raspberry Pi&lt;/a&gt; inside a Nintendo Entertainment System case to be able to play emulated NES games. My friend &lt;a href=&quot;https://twitter.com/Jaxidian&quot;&gt;Shane&lt;/a&gt; and I decided to do one of these projects. But we didn&apos;t want just another RPi emulation system. We wanted something a little more unique. So a plan was hatched to increase the nostalgia factor by not only interfacing with NES controllers, but also making use of NES cartridges.&lt;/p&gt;
&lt;h2&gt;The hardware&lt;/h2&gt;
&lt;p&gt;In order to use an RPi inside an NES console, we needed to figure out how to expose necessary connections - HDMI and power. So off to eBay we went and ordered some mountable connectors. Having decided not to touch the original NES connector locations due to space and cabling limitations, we opted for making new holes in the case, starting with templates:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-templates.jpg&quot; alt=&quot;Hole templates&quot; /&gt;&lt;/p&gt;
&lt;p&gt;We explored different options for hole locations, as well as the possibility of adding generic USB ports:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-routing-initial.jpg&quot; alt=&quot;Initial cable routing&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The board sitting on top of the RPi is the &lt;a href=&quot;http://blog.petrockblock.com/2014/12/29/controlblock-power-switch-and-io-for-the-raspberry-pi/&quot;&gt;ControlBlock&lt;/a&gt;, which we&apos;re using for both power management (to be able to safely power on and shut down the RPi) and for connecting the original NES controllers.&lt;/p&gt;
&lt;p&gt;After practicing cutting into some very melty plastic, we were relieved to discover that the NES case plastic wasn&apos;t nearly as melty and allowed for relatively painless drilling and filing. The results were satisfying:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-ports-initial.jpg&quot; alt=&quot;Ports&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The connectors we got from eBay were then easily secured using screws:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-ports-completed.jpg&quot; alt=&quot;Completed ports&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The vertical USB connector is used for power. We planned to add one or two USB data connectors that would have been horizontal, but that didn&apos;t get into version 1 of Project PiNES.&lt;/p&gt;
&lt;p&gt;Once everything was hooked up, we tested the system:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-test.jpg&quot; alt=&quot;Test&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I mentioned earlier that we wanted to make use of NES cartridges. Since both console and cartridge pins are often worn down and unreliable, we decided to come up with an alternate way of using the cartridges. As can be seen in the above photo, there is an NFC reader (another eBay purchase) in the middle of the NES. We opened a couple of the NES cartridges we have and put NFC stickers inside them. I wrote some Python code to interface with the NFC reader and simply run the game ROM file name that is written to the NFC sticker.&lt;/p&gt;
&lt;p&gt;This is what the whole thing looks like inside:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-routing-completed.jpg&quot; alt=&quot;Completed cable routing&quot; /&gt;&lt;/p&gt;
&lt;p&gt;And here it is assembled, with an opened cartridge that has an NFC sticker placed inside:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/pines-assembled.jpg&quot; alt=&quot;Assembled&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;The software&lt;/h2&gt;
&lt;p&gt;The Raspberry Pi 2 Model B inside the NES is running Arch Linux, tweaked to minimize boot time. The emulation software is &lt;a href=&quot;http://www.libretro.com/&quot;&gt;RetroArch&lt;/a&gt;. However, instead of using the &amp;quot;standard&amp;quot; front-ends like EmulationStation, I wrote a custom one. It&apos;s designed specifically to have a &amp;quot;retro&amp;quot; look and feel, and it&apos;s relatively lightweight. You can see it in action here:&lt;/p&gt;
&lt;iframe width=&quot;780&quot; height=&quot;440&quot; src=&quot;https://www.youtube.com/embed/JTtkKQAr3a8&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;The custom front-end was written using &lt;a href=&quot;http://www.pygame.org/&quot;&gt;pygame&lt;/a&gt; and, since this is my first real attempt at a Python project, I&apos;m not exactly proud of how the code looks. It does its job, though.&lt;/p&gt;
&lt;h2&gt;The future&lt;/h2&gt;
&lt;p&gt;We may add to this project some day if we find the time and motivation. Those USB data ports would be nice and, since they would allow for USB controllers with more buttons to be connected, more advanced game consoles could be emulated. Or maybe next time we&apos;ll upgrade to an SNES case!&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 6: system testing</title>
        <link>http://arktronic.com/weblog/2016-01-19/automated-software-testing-part-6-system-testing/</link>
        <pubDate>Wed, 20 Jan 2016 02:30:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2016-01-20T02:30:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; | part 6&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;In &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; we discussed how integration testing is used to improve the quality of software at a higher level than unit testing and when it should - and shouldn&apos;t - be used. As I mentioned in that post, integration tests aren&apos;t suited for testing functionality that relies on slow external systems like databases and web services or functionality that touches the UI. System testing is most useful in these areas.&lt;/p&gt;
&lt;p&gt;While integration testing provides valuable feedback on the interaction between units, system testing provides equally valuable feedback on the functionality of the application as a whole. Recall the pyramid structure from &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt;. Just as integration tests see more of the application than unit tests, so do system tests see more than integration tests. Where integration tests can catch issues that unit tests cannot, system tests can catch issues that integration tests cannot: they exercise the interactions between major application components as well as interactions with external services.&lt;/p&gt;
&lt;h2&gt;Adding system testing to your process&lt;/h2&gt;
&lt;p&gt;At the most basic level, system testing involves getting your application installed in an environment that mimics production as closely as possible and using an automation tool or framework to access that application in the same way that a real user would. This type of testing is very similar to how an application would be manually tested. However, setting up an appropriate environment for system testing can be challenging.&lt;/p&gt;
&lt;h3&gt;Where to run system tests&lt;/h3&gt;
&lt;p&gt;System tests should be run on local development machines, at a minimum, in order to facilitate easy and quick development as well as maintenance of the tests themselves. Ideally, the tests should also run as part of a &lt;a href=&quot;https://en.wikipedia.org/wiki/Continuous_delivery&quot;&gt;continuous delivery&lt;/a&gt; setup. If they cannot be run on development machines directly because the application itself can&apos;t run in such an environment, a virtual machine may help. If specialized hardware is required to run the application, it may be worthwhile to invest in automatable hardware simulation tools that can be set up to respond as the real hardware would in various states.&lt;/p&gt;
&lt;h3&gt;Environment restrictions&lt;/h3&gt;
&lt;p&gt;The test environment in which your application runs must be able to handle multiple executions of your system tests. This means that, if your tests add or delete information in the application, those changes have to be reverted between successive test runs - otherwise, such add or delete operations can fail because of previous modifications to the data. If at all possible, the full expected initial state of the application and its dependencies should be configured during the setup phase of your system tests. This could involve copying a &amp;quot;known good&amp;quot; database file to a configured location, running a local web server to respond to web service calls made by your application, cleaning up an output directory, and other such actions. Avoid executing important steps as part of a teardown procedure because catastrophic failures during test runs may prevent teardown actions from executing.&lt;/p&gt;
&lt;p&gt;If you&apos;re testing a network-connected application, then it likely relies on external web services and/or databases. It is sometimes impractical, especially for larger systems, to set up all of the external dependencies locally for system testing. In these situations, having dedicated test environments hosted elsewhere can be very useful. You could then point your local application instance to this test environment and run your system tests against that. You must be careful, though, not to affect the shared environment when running your system tests: after all, you don&apos;t want to cause issues for other people or automated test systems by deleting important data that they may be relying on.&lt;/p&gt;
&lt;p&gt;To avoid such issues with shared environments while still exercising your application&apos;s data modification functionality, it can be useful to set up a kind of test double for the external services that your application needs. This could be as simple as a local database server that is initialized to a known good state before running the system tests. The application can then be pointed at this local database, and you wouldn&apos;t have to worry about affecting a shared test environment&apos;s database.&lt;/p&gt;
&lt;h2&gt;Common issues&lt;/h2&gt;
&lt;p&gt;While developing and running system tests you&apos;ll likely encounter a number of common issues. Some of them may be tricky to deal with, while others not so much. The important thing to remember is, &lt;strong&gt;do not accept flaky tests&lt;/strong&gt;. Tests that occasionally fail for no apparent reason will happen. If a &amp;quot;random&amp;quot; failure occurs once in every 100 runs, it should be looked at, but it&apos;s certainly not the end of the world. If, however, a failure occurs every third run, it needs to be addressed quickly. As I mentioned in &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt;, people may stop caring about failures if they happen so often, and then your automated testing is all but useless.&lt;/p&gt;
&lt;h3&gt;Bad timing&lt;/h3&gt;
&lt;p&gt;One of the most common and most annoying issues with system tests is a timing-related problem. The test tried clicking a button on the main screen before a modal dialog finished closing. The test didn&apos;t wait long enough before checking the value of a text box while the application was updating the screen. The test tried typing into an entry field before the screen finished loading. These are all realistic examples, and they can be rather frustrating. There is more than one way to address these timing issues. The most obvious is to add a simple delay (or &amp;quot;sleep&amp;quot;) to the execution of the problematic test. Unfortunately, this is usually also the worst way, in terms of both stability and performance. A delay of one second, as an example, may solve a particular timing failure on your machine, but if someone is running the same test on a slower (or busier) machine, it may not be enough time. Conversely, if you &amp;quot;play it safe&amp;quot; and add a ten second delay, then you slow down the execution of these tests needlessly for everyone who runs them.&lt;/p&gt;
&lt;p&gt;A better way to address timing issues is to have your tests look for specific events to happen before executing the next step. For example, you could verify that the window handle of the modal dialog no longer points to a valid window before trying to click the button on the main screen. In the case of a textual update happening too slowly, you could either wait for a signal from the application that everything is ready (if such a signal exists) or you could check for the expected value in a loop with a reasonable timeout. In this case, having a timeout of ten seconds wouldn&apos;t be nearly as bad as forcing a ten second wait on everyone because the loop would exit as soon as the correct value is seen, and if the correct value is not seen in the ten seconds, the test simply fails.&lt;/p&gt;
&lt;h3&gt;Unrealistic steps&lt;/h3&gt;
&lt;p&gt;Another issue that happens with system tests is the execution of unrealistic steps. This often involves clicking a button that is invisible or otherwise obscured such that a real user would not be able to click it. This usually happens because a test was written to push an event directly to an object via the automation framework, which generally doesn&apos;t know whether the object is truly available for the user to interact with. Executing unrealistic steps will sometimes cause failures because the application isn&apos;t yet ready to accept whatever input the test is sending it. Other times the application may get into an unpredictable state because a series of events occurs that would normally be impossible for a real user to execute.&lt;/p&gt;
&lt;p&gt;These issues can be avoided by simulating input as realistically as possible. For example, instead of sending a click event directly to a button, you could determine the button&apos;s coordinates and then send a mouse click event to the main application window at those coordinates and let the application propagate the event down to that button - or to whatever else may be there on top of the button.&lt;/p&gt;
&lt;h3&gt;Systemic fixes&lt;/h3&gt;
&lt;p&gt;Very often the reason for many of these common issues is that the testing or automation tools don&apos;t provide a good way to do what you need to. This is a systemic problem, and the best way to avoid such problems is to create systemic fixes. If your UI automation tool requires three method calls to be able to get the coordinates of an object and send a click event to the application at those coordinates, you should create a single wrapper method for this and encourage everyone to use it. If the automation tool doesn&apos;t provide an easy way to wait for a text box to contain an expected value by constantly polling it until a timeout occurs, create this method yourself. Essentially, if you find that you&apos;re writing the same boilerplate test code in different tests, extract it to a helper class or module. This way you can improve the stability and reliability of most, if not all, of your system tests.&lt;/p&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In this series of blog posts I&apos;ve explained the value of automated tests at the unit, integration, and system levels. I&apos;ve also discussed the types of issues that you&apos;ll often encounter with these tests and how to solve - or at least mitigate - many of them. If you implement a robust testing strategy for your application, you will benefit immensely from it: not only will it be easier for you and others to implement changes and fixes without breaking existing functionality, but you will also have increased confidence in the overall stability of your codebase.&lt;/p&gt;
&lt;p&gt;Go forth and boldly create!&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 5: integration testing</title>
        <link>http://arktronic.com/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/</link>
        <pubDate>Sun, 27 Dec 2015 03:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-12-27T03:00:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; | part 5 | &lt;a href=&quot;/weblog/2016-01-19/automated-software-testing-part-6-system-testing/&quot;&gt;part 6&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;As you will recall from &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt;, we left off with a problem. All the unit tests for the &lt;a href=&quot;http://js.codeit.live/js-testing-demo/?relativeTime2&quot;&gt;relative time formatter v2&lt;/a&gt; passed, but I explained that there was, in fact, a bug in how the day formatter was being called. The bug wasn&apos;t caught by the unit tests because of faulty assumptions made while writing the primary formatter &amp;quot;class&amp;quot; and its tests.&lt;/p&gt;
&lt;h2&gt;The purpose of integration testing&lt;/h2&gt;
&lt;p&gt;Although I created the aforementioned bug deliberately to show how unit tests alone aren&apos;t enough, it&apos;s very easy for such a bug to occur naturally during the development cycle. In more strictly-typed languages, a compiler error could have occurred if common interfaces were used to build the hour and day formatter stubs, and this exact scenario could be avoided. However, that wouldn&apos;t address the entire problem. An issue with call arguments is an issue of data flow from one unit to another. It could also manifest itself in more subtle ways, such as with improperly formatted strings or with unexpected nulls, and those bugs would not necessarily be highlighted by a compiler.&lt;/p&gt;
&lt;p&gt;Integration testing is how we can mitigate data flow issues. Instead of isolating every unit and testing it individually, we instantiate multiple units and test how they function together. This gives us confidence in our code at a higher level: it tells us that an entire section or functional area of the code is working as intended.&lt;/p&gt;
&lt;h2&gt;The tests&lt;/h2&gt;
&lt;p&gt;Let&apos;s take a look at an integration test for relative time formatter v2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://js.codeit.live/js-testing-demo/?relativeTime2int&quot;&gt;Relative time formatter v2 integration testing&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The implementation portion in the above link is unchanged from the relative time formatter v2 implementation introduced in &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; - bug included. The tests, however, are quite different. You can see that we&apos;re still mocking out the current date, since the need for a stable and reproducible environment still exists, but instead of having separate suites testing each unit individually, there is a single integration suite whose setup function instantiates all three classes and passes the day and hour formatter instances to the primary formatter exactly as a real system would.&lt;/p&gt;
&lt;p&gt;If you run the tests, you&apos;ll see that the first two pass, while the second two fail, thus exposing the bug. Note that, while there were a lot of unit tests for these three classes, there are only four integration tests. This is by design. Integration testing is not intended to cover every possible scenario that could occur in the individual units. Its focus is on covering the major interactions between the units. Arguably, the integration suite could have only contained two tests - one for interacting with the hour formatter, and one for interacting with the day formatter. I chose to have two tests per formatter to verify that dates in the past and in the future are working as intended. I could see a possible desire to split those up into separate classes, so I figured that a couple of extra tests might help.&lt;/p&gt;
&lt;p&gt;Let&apos;s fix the implementation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://js.codeit.live/js-testing-demo/?relativeTime2intfix&quot;&gt;Relative time formatter v2 integration testing, with fix&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The integration suite in the above link is identical to the one in the previous link, and the only change in the implementation is the call to the day formatter: &lt;code&gt;this.dayFormatter.format(now, target);&lt;/code&gt;. Now the integration tests pass. However, if you were to run the previously created unit tests against this code, there would be failures in the primary formatter&apos;s test suite. Those unit tests would then need to be updated to work with the fixed primary formatter. Since no code was changed in the hour and day formatters, their respective unit tests don&apos;t need to be updated.&lt;/p&gt;
&lt;h2&gt;Cascading failures and improper fixes&lt;/h2&gt;
&lt;p&gt;The interplay between different types of automated software tests is such that a failure in one type (e.g., integration) will sometimes cause failures in other types (e.g., unit) after a fix is applied to correct the first failure. You must always be cautious when facing this situation, and you should pay close attention to the exact kinds of failures you&apos;re encountering and fixing. You may discover, for example, that after applying a fix for an integration failure, you end up with a failed unit test with a very specific defined scenario. If your fix broke that scenario, you have to determine whether the scenario was incorrect, or whether your fix has now broken something important, in which case you need to reconsider how - and where - to implement your fix.&lt;/p&gt;
&lt;p&gt;It&apos;s easy to get caught up in fixing one area of code and to not pay much attention to &amp;quot;side effects&amp;quot; of broken tests. If a test failed on an expectation of &lt;code&gt;True&lt;/code&gt; for some value that now returns &lt;code&gt;False&lt;/code&gt;, the way to make that test pass once again is clear. Unfortunately, simply changing the expectation is not always the correct course of action, as tempting as it may be.&lt;/p&gt;
&lt;p&gt;What you must do is examine two things: why there was a test for the value in question, and why that value changed. If the reason for the test isn&apos;t obvious and the description of the test or comments around the failed expectation don&apos;t yield useful information, then it&apos;s possible that the expectation is extraneous and could be removed. On the other hand, it could have been added hastily as part of a bug fix at some point in time. Version control systems can help pinpoint when the expectation was added, and it&apos;s often useful to see the entire changeset where this addition happened in order to see it in context and gain a greater understanding of the change as a whole.&lt;/p&gt;
&lt;p&gt;If you&apos;ve determined that the reason for the expectation is valid, you must then figure out why the value has changed. If the change was a direct result of your fix, then perhaps the expectation needs to be updated, and if that&apos;s the case, you should make sure this change doesn&apos;t negatively impact code downstream and then update the expectation. However, if the expectation is still entirely correct, or if this change will introduce problems in other parts of the system, then your fix is likely improper.&lt;/p&gt;
&lt;p&gt;Unit and integration testing, used together, can be a powerful tool to help determine the viability of bug fixes as described above. In a complex system, an initial attempt at a bug fix may not always result in a viable solution due to unforeseen effects down the line. Writing good integration tests that encompass potentially obscure functionality will give you greater confidence that bug regressions in that area of code can be avoided.&lt;/p&gt;
&lt;h2&gt;Determining boundaries&lt;/h2&gt;
&lt;p&gt;It&apos;s important to define the boundaries in your integration tests before you write them. In the case of the relative time formatter v2, the primary formatter, the hour formatter, and the day formatter are being tested together. Everything else that happens to be a dependency, such as the clock, must still be replaced with a test double.&lt;/p&gt;
&lt;p&gt;Determining the boundaries is rarely an exact science, unfortunately. In the above example, all the units that we have form a distinct piece of functionality, the relative time formatter, so the boundaries are pretty obvious. In real-world projects, you&apos;ll often find similar sets of units that work together to accomplish a particular task. Those are good candidates for integration testing. In an &lt;a href=&quot;https://en.wikipedia.org/wiki/Multitier_architecture&quot;&gt;n-tier&lt;/a&gt; application, you&apos;ll often want to test the interaction between tiers as well.&lt;/p&gt;
&lt;p&gt;Avoid creating integration tests that need to communicate with slow external systems such as databases, remote file systems, and web services. Creating a temporary in-memory database and using it in an integration test is perfectly fine, but if you need to communicate with a real instance of a remote database, you&apos;re introducing a complex, relatively slow, and potentially brittle dependency. Integration tests should be reasonably fast, and they should not fail just because some database instance on another server was down for maintenance.&lt;/p&gt;
&lt;p&gt;Most of the time you should also avoid creating integration tests that touch the UI. Those tend to get overly complicated and brittle when isolating the set of units you&apos;re trying to test. It&apos;s generally better to leave that to system testing, which will be covered in part 6.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 4: unit tests and test doubles</title>
        <link>http://arktronic.com/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/</link>
        <pubDate>Thu, 03 Dec 2015 03:30:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-12-03T03:30:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; | part 4 | &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; | &lt;a href=&quot;/weblog/2016-01-19/automated-software-testing-part-6-system-testing/&quot;&gt;part 6&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;In &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; I discussed some of the good habits surrounding unit testing, such as determining how much to cover in your tests and avoiding the temptation to test internal implementation details. In this part we&apos;ll continue to explore unit testing with a focus on verifying external interactions in a controlled and isolated environment.&lt;/p&gt;
&lt;p&gt;First, I&apos;d like to clarify some terminology. When I&apos;m talking about &amp;quot;external interactions&amp;quot;, I&apos;m mostly referring to a unit interacting with another unit. Generally, this does &lt;em&gt;not&lt;/em&gt; include a unit interacting with an underlying framework or language feature because most of the time that kind of interaction falls under the scope of &amp;quot;internal implementation details&amp;quot; and, as such, shouldn&apos;t matter to our tests.&lt;/p&gt;
&lt;h2&gt;Verify external interactions&lt;/h2&gt;
&lt;p&gt;In the last post, I showed how testing the outputs of a unit based on its inputs can work. An external interaction is just another type of input and/or output, so it&apos;s just as important to verify: units should be able to handle their respectively defined inputs and produce expected outputs, but you need to be certain of what those inputs and outputs actually are. While integration tests verify that multiple units are communicating correctly with each other, they are limited in two important ways: they cannot - and should not - see the exact data that flows between the units, and they cannot - and should not - cover all possible inter-unit communication scenarios. The second point there is especially important because it&apos;s easy to get stuck trying to create the &amp;quot;perfect&amp;quot; scenario where the data transforms and flows in just the right way to get to a very specific outcome, but that shouldn&apos;t be your focus when crafting integration tests. More on that in the next post.&lt;/p&gt;
&lt;p&gt;Both of the above limitations can be addressed to a sufficient extent with unit testing. Instead of instantiating real dependencies, a unit test should create &lt;a href=&quot;http://martinfowler.com/articles/mocksArentStubs.html&quot;&gt;test doubles&lt;/a&gt; that sit in place of the real external units and act just enough like them that the unit you&apos;re testing doesn&apos;t know the difference. You&apos;re effectively isolating your unit, which allows you to verify the exact data that is input to and output from each unit. This gives you enough flexibility and control to cover primary interactions as well as edge case scenarios in regards to inputs and outputs.&lt;/p&gt;
&lt;h2&gt;Time formatting, revisited&lt;/h2&gt;
&lt;p&gt;It&apos;s time (no pun intended) to once again dive into some code and tests.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://js.codeit.live/js-testing-demo/?relativeTime2&quot;&gt;Relative time formatter v2&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The relative time formatter was first introduced in &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt;. In this upgraded version, the single &amp;quot;class&amp;quot; has been split up into three different ones - the primary formatter, the hour formatter, and the day formatter. While the first version could only format days, this one can format hours as well. Getting the desired result string from the relative time formatter still consists of instantiating a &lt;code&gt;RelativeTimeFormatter&lt;/code&gt; and calling its &lt;code&gt;format&lt;/code&gt; function. What&apos;s different is that you now need to pass an instance of the hour formatter and an instance of the day formatter to the primary formatter&apos;s constructor. I should also mention that this isn&apos;t necessarily idiomatic JavaScript. The concepts demonstrated and discussed here are generic and applicable to many languages and frameworks, which means we can&apos;t take advantage of some of the inherent strengths unique to JavaScript.&lt;/p&gt;
&lt;p&gt;Looking at the test section, you&apos;ll notice that there are three test suites, since we now have three classes. If you click the run button, you&apos;ll see their outputs and that they all pass. The most interesting test to look at is the one that exercises the &lt;code&gt;RelativeTimeFormatter&lt;/code&gt; itself. In addition to the mocked out clock that was discussed in the last post, it has an &lt;code&gt;hourFormatterStub&lt;/code&gt; and a &lt;code&gt;dayFormatterStub&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Setup, execution, verification&lt;/h3&gt;
&lt;p&gt;Let&apos;s look at the &amp;quot;should format less than 8 hours in the future using the hour formatter&amp;quot; test. After defining a couple of variables, we have the following line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hourFormatterStub.and.returnValue(expectedValue);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This line configures the &lt;code&gt;hourFormatterStub&lt;/code&gt; to return &lt;code&gt;expectedValue&lt;/code&gt; whenever that stub is called. Since &lt;code&gt;expectedValue&lt;/code&gt; is defined earlier in the test as a generic string (that happens to look very different from what the real formatter might return) we can check for it later in the test to verify that this string was returned to us from our stubbed out hour formatter via the real &lt;code&gt;RelativeTimeFormatter&lt;/code&gt;. Naturally, this line of code must be executed before we try calling &lt;code&gt;RelativeTimeFormatter.format&lt;/code&gt;, since it sets up required functionality on a dependency. Hence, it could be called a setup step.&lt;/p&gt;
&lt;p&gt;Once the necessary setup steps are taken, we call into our instance of the real &lt;code&gt;RelativeTimeFormatter&lt;/code&gt;, and save the return value for later. This is the execution step.&lt;/p&gt;
&lt;p&gt;After calling the &lt;code&gt;format&lt;/code&gt; function on our instance of the &lt;code&gt;RelativeTimeFormatter&lt;/code&gt;, we have some &lt;code&gt;expect&lt;/code&gt; lines, which check that the stub was called the correct number of times, that its invocations contained the expected arguments, and finally, that the &lt;code&gt;RelativeTimeFormatter&lt;/code&gt; returned what it got from the &lt;code&gt;hourFormatterStub&lt;/code&gt;. Collectively, these lines could be called the test&apos;s verification step.&lt;/p&gt;
&lt;p&gt;You may have noticed that the relative time formatter tests tend to conform to the setup-execution-verification structure. This is a pretty typical way to write unit tests that are more complex than a single-line return value verification test.&lt;/p&gt;
&lt;h3&gt;Refactoring tests&lt;/h3&gt;
&lt;p&gt;After writing your tests, you may notice that many of them have the same setup steps. That&apos;s a good indication that you should extract those steps into the &lt;code&gt;beforeEach&lt;/code&gt; (or equivalent) of the test suite. Similarly, if common tear down steps are needed, those can go into &lt;code&gt;afterEach&lt;/code&gt;. Such refactoring will simplify the tests, making them easier to read, modify, and debug. You can see how this common setup and tear down was done in the relative time formatter test suite. Common verification steps can also be refactored into separate functions that you call from your tests, but you should be careful with this, as it is entirely possible to over-optimize and make the tests more complicated instead of simplifying them.&lt;/p&gt;
&lt;h3&gt;An uncaught bug&lt;/h3&gt;
&lt;p&gt;Although all the tests pass, there is actually a major bug in the code of the new relative time formatter. The problem lies in the fact that the hour formatter and the day formatter take different arguments in their respective &lt;code&gt;format&lt;/code&gt; functions: the hour formatter takes a single &lt;code&gt;diffValue&lt;/code&gt; in milliseconds, while the day formatter takes &lt;code&gt;now&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt; Date objects. Individually, these differences make sense, since the calculations that are performed are different between the hour formatter and the day formatter. However, the primary formatter that we&apos;ve been testing with the hour and day formatter stubs isn&apos;t calling the day formatter correctly. We&apos;ve assumed that the calls are identical and wrote the implementation as well as its tests with this faulty assumption in mind.&lt;/p&gt;
&lt;p&gt;The important takeaway here is that this is &lt;em&gt;not&lt;/em&gt; strictly a unit testing problem. While it&apos;s true that the &amp;quot;should format more than 8 hours [...] using the day formatter&amp;quot; tests are flawed, as is the implementation that is being tested, this kind of mistake is easy to make. An act as innocent as copy-pasting the previous hour formatter tests and trivially modifying them to use the day formatter would cause this problem, and copy-pasting is done a lot more often in the software development world than we like to admit.&lt;/p&gt;
&lt;p&gt;The solution to the problem does not lie in disabling Ctrl-C and Ctrl-V. Rather, it is in testing the interactions between real units. This is integration testing, and it will catch such bugs. We&apos;ll discuss it in more detail in &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt;.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 3: unit tests</title>
        <link>http://arktronic.com/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/</link>
        <pubDate>Tue, 10 Nov 2015 04:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-11-10T04:00:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; | part 3 | &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; | &lt;a href=&quot;/weblog/2016-01-19/automated-software-testing-part-6-system-testing/&quot;&gt;part 6&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;The first and second parts of this blog post series are an overview of automated software testing, with &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; focusing on the &lt;em&gt;why&lt;/em&gt; and &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; a pretty high-level &lt;em&gt;how&lt;/em&gt;. Now, with the third part, let&apos;s delve deeper into the &lt;em&gt;how&lt;/em&gt; of unit tests specifically.&lt;/p&gt;
&lt;p&gt;As I explained in part 2, unit tests shouldn&apos;t see the &amp;quot;big picture&amp;quot;. They must be purposely designed to test their units and nothing else. It&apos;s often tempting to create unit tests that span multiple units in order to get a more realistic representation of state, but you should strive to avoid this. Such tests should be added at the integration level instead.&lt;/p&gt;
&lt;h2&gt;Let&apos;s get interactive&lt;/h2&gt;
&lt;p&gt;Part of my goal is to explain many of the concepts here with real examples instead of just words, because I think that running a test for yourself and seeing it pass or fail is much more powerful than simply reading about it. To that end, I created an online JavaScript runner/tester, which lets you write JavaScript code and tests and run them immediately in your browser.&lt;/p&gt;
&lt;p&gt;I&apos;ve chosen Jasmine, a popular testing framework, to help show the concepts below. Please take a moment to look at Jasmine&apos;s &lt;a href=&quot;https://jasmine.github.io/2.3/introduction.html&quot;&gt;terminology and syntax&lt;/a&gt; so you can follow along more easily. You can also keep that page open in another browser window or tab and refer back to it if you&apos;re unclear on how a particular Jasmine feature works.&lt;/p&gt;
&lt;p&gt;Now let&apos;s take a look at a very basic coding and testing example, &amp;quot;hello world&amp;quot;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://js.codeit.live/js-testing-demo/?helloWorld&quot;&gt;Hello World example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On the left, you&apos;ll see a function that returns the familiar string. This is the code that is to be tested. On the right, you&apos;ll see a test suite defined using the &lt;code&gt;describe&lt;/code&gt; function, and two tests (&amp;quot;specs&amp;quot; in Jasmine parlance) defined using the &lt;code&gt;it&lt;/code&gt; function inside. If you run the tests, you should see something similar to the following in the test output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Jasmine v2.3.4 started.
Hello World function
    should exist
        [passed]
    should return the expected value
        [passed]
Hello World function: finished
Test run complete.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Both tests passed, hooray! You can play around with the &lt;code&gt;helloWorld&lt;/code&gt; function and its tests, and run them again to see what happens when failures occur. Jasmine&apos;s syntax is intended to assist developers with using &lt;a href=&quot;https://en.wikipedia.org/wiki/Behavior-driven_development&quot;&gt;BDD&lt;/a&gt;, but that&apos;s a topic that is far outside the scope of this post, so we can gloss over the details, such as why some things are so verbose. Suffice it to say, natural language constructs are an important aspect. In fact, as you can see from the output above, the suite and spec text could be read as &amp;quot;Hello World function should exist&amp;quot; and &amp;quot;Hello World function should return the expected value&amp;quot;.&lt;/p&gt;
&lt;p&gt;Of course, Hello World isn&apos;t a real-world example. Not only are there no code-containing units to speak of, there&apos;s virtually no functionality, either. This makes for a great example of how to use the tools at a mechanical level, but not any deeper.&lt;/p&gt;
&lt;h2&gt;A better example&lt;/h2&gt;
&lt;p&gt;To demonstrate something a bit more advanced, we&apos;ll need to start making use of code encapsulation in JavaScript, often implemented using a &lt;a href=&quot;http://www.samselikoff.com/blog/some-Javascript-constructor-patterns/&quot;&gt;constructor pattern&lt;/a&gt;. This also serves as a decent approximation of classes with public and private methods in languages that support these concepts natively. For the next example, then, let&apos;s say we want to create a relative time formatter - something that will take a date object and return, for example, &amp;quot;5 days ago&amp;quot;, based on the difference between that date object and now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://js.codeit.live/js-testing-demo/?relativeTime&quot;&gt;Relative time formatter example&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As you can see, the formatter&apos;s code is a lot more complicated than Hello World, but the tests in this case are pretty simple and straightforward: they just pass in various dates and verify the &amp;quot;correctness&amp;quot; of the output. Notice that I put quotation marks around the word correctness. The reason is that, with unit tests or any other tests for that matter, what you consider to be correct may not match what your users consider to be correct. It may be an obvious statement, but it is nonetheless important to keep in mind that just because the tests are passing doesn&apos;t mean that &lt;a href=&quot;https://en.wikipedia.org/wiki/Bugzilla#Zarro_Boogs&quot;&gt;there are no issues&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Test doubles&lt;/h3&gt;
&lt;p&gt;While the above example is relatively isolated from external dependencies, it does rely on one: the current date/time. Since we can&apos;t predict when our tests run, we have to have a way to control what the formatter &lt;em&gt;thinks&lt;/em&gt; is the current date/time in order to have meaningful tests. Luckily, Jasmine provides a way to &amp;quot;stop time&amp;quot; at a desired point by replacing the real &lt;code&gt;Date&lt;/code&gt; object with a fake one. Faking objects is an example of using &lt;a href=&quot;http://martinfowler.com/articles/mocksArentStubs.html&quot;&gt;test doubles&lt;/a&gt;. As the linked article explains, test doubles include mocks, stubs, and fakes. Although Jasmine uses a function named &lt;code&gt;mockDate&lt;/code&gt; to accomplish its goal, it&apos;s really creating a stub rather than a mock. However, terminology surrounding test doubles isn&apos;t universally consistent, so you should expect differences in definitions when reading articles or talking with people about them.&lt;/p&gt;
&lt;p&gt;It&apos;s important to know when a test double requires a tear down procedure - and even more important to then implement it. In this case, &lt;code&gt;jasmine.clock().uninstall()&lt;/code&gt; is called after each spec run. This is done in order to prevent &amp;quot;leakage&amp;quot; of test doubles from specs that run earlier in the execution order to ones that run later. Such leakage can cause very confusing test results and problems that are often difficult to track down. Since there is only one suite in this example, and all specs rely on the test double, the tear down is not strictly needed here, but it is nevertheless a good practice.&lt;/p&gt;
&lt;h3&gt;Public vs. private&lt;/h3&gt;
&lt;p&gt;The only public function exposed by the relative time formatter is &lt;code&gt;format&lt;/code&gt;, due to it being assigned on &lt;code&gt;this&lt;/code&gt;. The other functions in it are private. That means they cannot be tested directly. It can be tempting to expose additional functions publicly in order to test them directly, but you should avoid doing so (unless those functions are actually needed by non-test code). One reason is that, as you are testing a unit, you should only be concerned with the unit&apos;s inputs, outputs, and external side-effects (if any). The internal state of the unit shouldn&apos;t matter, since that isn&apos;t the purpose of unit testing: what you should be testing is the &lt;em&gt;what&lt;/em&gt;, not the &lt;em&gt;how&lt;/em&gt;. Another reason is, if you start tying a unit&apos;s internal functionality to external dependencies (tests in this case), the unit becomes extremely brittle. For example, right now you could rename &lt;code&gt;getMidnightOfDate&lt;/code&gt; and the calls to it, or even get rid of the function entirely and duplicate its code in the two places it&apos;s called, and no tests should break. If you were to expose this function publicly, you would now be tied to the current implementation, and any change would mean fixing tests that failed for no good reason. Not only that, but other people may start using this exposed function when you hadn&apos;t intended it, which effectively prevents you from changing it without costly and time-consuming refactoring.&lt;/p&gt;
&lt;p&gt;So, how do you test private functions? The answer is, you test them indirectly. Going back to the example of the &lt;code&gt;getMidnightOfDate&lt;/code&gt; function, it is tested by calling &lt;code&gt;format&lt;/code&gt; with a date that is 24 hours in the past or older. This indirect testing can be painful to do at times, but when that pain becomes too much, it&apos;s a strong indicator that your unit may need to be refactored or broken up into multiple smaller ones. If you&apos;re at the point of drawing diagrams just to figure out the precise set of inputs you need to test a certain code path, you should take a step back and ask yourself whether the unit is just too big.&lt;/p&gt;
&lt;h3&gt;Too much vs. not enough&lt;/h3&gt;
&lt;p&gt;At what point do you say that your unit has enough coverage? (And don&apos;t tell me that it&apos;s when your code coverage tool says 100%! See part 2 for a refresher on that.) The way to determine the answer can be considered partly art and partly science. You should strive to test most, if not all, of your unit&apos;s code paths. However, a meticulous analysis of the possible paths and their tests can be very time consuming. Sometimes the detailed output of a code coverage tool will be able to point out paths that aren&apos;t tested, and you can judge for yourself whether to add tests there.&lt;/p&gt;
&lt;p&gt;For this example, I chose to write tests that exercise the major code paths (i.e., the different words for days, a future date, and generic past days), as well as an extreme (365 days) that is still well within the realm of possibility. I did not choose to write a test that, for example, verifies that an exception is thrown if something other than a &lt;code&gt;Date&lt;/code&gt; object is passed to &lt;code&gt;format&lt;/code&gt;. While that could conceivably happen, I feel that there&apos;s no need to check this case because I consider it undefined behavior and, as such, unimportant.&lt;/p&gt;
&lt;p&gt;I also didn&apos;t test leap years because I expect the browser&apos;s JavaScript implementation to handle that for me (so if I were to compare 2017-01-01 and 2016-01-01 the difference should be 366 days). There is generally no need to verify the correctness of the basic frameworks or language features that you&apos;re using. While it is possible to encounter bugs in them, this is an exceedingly rare occurrence, and not one you should spend time worrying about or writing tests against.&lt;/p&gt;
&lt;p&gt;If you write many tests that exercise the same code paths, that makes it difficult to modify the unit in the future, because a lot of tests would break. On the other hand, if your tests don&apos;t exercise the important code paths of your unit, you could encounter bugs either in the original implementation or after a modification, if such a modification accidentally changed the result of an untested code path. Striking a balance is key here.&lt;/p&gt;
&lt;h2&gt;Next time&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt;, I&apos;ll to go into more detail on test doubles (and spies) and how to use them to verify a unit&apos;s external interactions.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 2: the types</title>
        <link>http://arktronic.com/weblog/2015-11-02/automated-software-testing-part-2-the-types/</link>
        <pubDate>Tue, 03 Nov 2015 04:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-11-03T04:00:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt; | part 2 | &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; | &lt;a href=&quot;/weblog/2016-01-19/automated-software-testing-part-6-system-testing/&quot;&gt;part 6&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;In &lt;a href=&quot;/weblog/2015-11-01/automated-software-testing-part-1-reasoning/&quot;&gt;part 1&lt;/a&gt;, I explained some of the reasoning behind automated software testing. Now let&apos;s explore a few of the more common types of automated tests that we often deal with.&lt;/p&gt;
&lt;h2&gt;An overview of the common test types&lt;/h2&gt;
&lt;p&gt;As I mentioned in the previous post, the common types that I&apos;ll focus on are unit testing, integration testing, and system testing.&lt;/p&gt;
&lt;p&gt;A unit test, as the name implies, is intended to test a small &amp;quot;unit&amp;quot; of code. Most often, in object-oriented programming, such a unit would be equivalent to a class. It is very important to only test that unit and not increase the scope in these tests.&lt;/p&gt;
&lt;p&gt;An integration test&apos;s purpose, fundamentally, is to test the interaction between two or more units. Realistically, integration tests are useful when multiple units (often from multiple tiers, to borrow a term from &lt;a href=&quot;https://en.wikipedia.org/wiki/Multitier_architecture&quot;&gt;n-tier architecture&lt;/a&gt;) are tested to see whether a specific task will successfully execute with all units relevant to this task instantiated and talking to each other.&lt;/p&gt;
&lt;p&gt;Finally, a system test runs the entire software product, unmodified, and exercises it to verify that the application actually works in the &amp;quot;real world&amp;quot; and that its critical functionality is accessible and working.&lt;/p&gt;
&lt;h2&gt;Determine when each type is appropriate&lt;/h2&gt;
&lt;p&gt;It&apos;s very important to test various pieces of functionality at the correct level. Unfortunately, I know of no simple set of rules to guarantee success in this decision. There are always unique circumstances that influence how a particular feature or its parts should be tested. However, I can provide some general guidelines.&lt;/p&gt;
&lt;p&gt;Unit tests should not see &amp;quot;the big picture&amp;quot;, as it were. They should be blissfully unaware of anything outside the scope of the unit - with the important exception of the interfaces with which the current unit interacts. Such unit tests are, by their very nature, rather limited. Pretty much all they can, and should, do is verify that the unit under test is performing its calculations or other data or state manipulations correctly under various circumstances that could conceivably arise. A useful thing to remember when designing a unit (and writing its tests) is that in the future, it might not be used in the same context as what you&apos;re currently designing it for. Other people might instantiate it in another part of the product and pass all kinds of stuff to it that you may not have expected. Writing robust unit tests helps to flesh out edge cases and to handle them more explicitly.&lt;/p&gt;
&lt;p&gt;Integration tests should be written when non-trivial interactions occur between units. They help to verify that a somewhat larger piece of &amp;quot;the big picture&amp;quot; works as intended. For example, when one unit receives data, modifies it according to its rules, and sends it to another unit for storage in a database, you probably want to be sure that whatever got into the database is what you expected. This isn&apos;t guaranteed to be the case even when the two units in question have good tests themselves, because the database handling unit could be expecting a slightly different format of data than it gets from the first unit, and when it gets the unexpected format, it doesn&apos;t store it correctly. This is where an integration test can help to spot the problem.&lt;/p&gt;
&lt;p&gt;System tests should only be written as necessary, as they generally take a relatively long time to run (i.e., not milliseconds) and are often more difficult to write and maintain, especially when the entire system is itself complex. These tests should exercise &amp;quot;the big picture&amp;quot;. They should cover major tasks that the product is designed to handle and, unlike integration tests, they should be run in a real (or at least as realistic as possible) environment, with the entire application, as I mentioned above. These tests can verify that all the interactions between the various internal and external components of your product happen as expected. Yes, external components factor into system tests as well. For example, if your product is a website that has a Twitter feed, a system test could ensure that the twitter feed is displayed correctly. This makes system tests more susceptible to failures caused by external issues (such as Twitter feeds being inaccessible), but the upside is that confidence in the overall product is increased.&lt;/p&gt;
&lt;p&gt;Generally speaking, you should test at the &lt;strong&gt;lowest responsible level&lt;/strong&gt;. If a unit test will do, then don&apos;t put your behavior verification in an integration test. And if an integration test is good enough to verify a certain flow, then don&apos;t create a system test for it. It can be difficult to determine what the lowest responsible level is for a particular piece of functionality, but referring to the above distinctions can help.&lt;/p&gt;
&lt;p&gt;Another way to guide this decision is to think of the levels as a pyramid:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;               /\
              /  \
             /    \
            /      \
           / SYSTEM \
          /__________\
         /            \
        /              \
       /  INTEGRATION   \
      /                  \
     /____________________\
    /                      \
   /                        \
  /           UNIT           \
 /                            \
/______________________________\
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The topmost level, system testing, gives you the best view of the world (if you were to climb this pyramid, and if it were not just an ASCII drawing) and it has the smallest area, so system tests should be the fewest in number. Consequently, there should be more integration tests than system tests, and they should have a more limited view. And last but not least, there should be a whole lot of unit tests, and they should see very little.&lt;/p&gt;
&lt;h2&gt;Don&apos;t overdo it&lt;/h2&gt;
&lt;p&gt;Sometimes it&apos;s very easy to get carried away with testing and write way too many tests that either don&apos;t provide much value, or even decrease value. As I mentioned in the last post, brittle tests cause maintainability problems. Testing every little aspect of a unit in a unit test, or testing a lot of the same functionality in both unit and integration tests, is a waste of time and productivity. When application behavior needs to be modified later on, you (or someone else) will be spending too much time fixing all the tests that broke just because a single code path was modified. Naturally, it&apos;s probably a good idea to write a test for the new code path, but that&apos;s different.&lt;/p&gt;
&lt;p&gt;Along the same lines, don&apos;t be afraid to delete tests that have lost their value. You&apos;ll speed up your test runs and remove cruft. Just be sure that the tests you are removing are indeed useless and aren&apos;t there to exercise some obscure code path. (This is where test naming becomes very important, but that&apos;s a topic for another time.)&lt;/p&gt;
&lt;p&gt;On a related note, I recommend &lt;em&gt;not&lt;/em&gt; paying much attention to &lt;a href=&quot;http://martinfowler.com/bliki/TestCoverage.html&quot;&gt;code coverage&lt;/a&gt; metrics. At best, they can function as merely a hint that something might be wrong. For example, 20% code coverage is probably bad. Crucially, 100% code coverage is also bad! Remember all that stuff about brittle tests? That&apos;s what 100% coverage will usually get you.&lt;/p&gt;
&lt;h2&gt;Design matters&lt;/h2&gt;
&lt;p&gt;The design and architecture of your application can make or break your ability to write good tests. In general, a more &lt;a href=&quot;https://en.wikipedia.org/wiki/Coupling_%28computer_programming%29&quot;&gt;decoupled&lt;/a&gt; design will lead to greater testability. &lt;a href=&quot;https://en.wikipedia.org/wiki/Dependency_injection&quot;&gt;Dependency injection&lt;/a&gt; will allow stubbing/mocking to be downright simple and fun, and let you easily isolate units for testing.&lt;/p&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;All of this theory is nice, but until you get enough practice determining what tests to write and how to write them, you&apos;ll make silly mistakes. It&apos;s okay, we&apos;ve all been there. Keep practicing and you&apos;ll get better at it!&lt;/p&gt;
&lt;p&gt;Check out &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt;, a more in-depth discussion of the first type, unit tests.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Automated software testing, part 1: reasoning</title>
        <link>http://arktronic.com/weblog/2015-11-01/automated-software-testing-part-1-reasoning/</link>
        <pubDate>Mon, 02 Nov 2015 04:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-11-02T04:00:00 on http://arktronic.com</guid>
        <description>&lt;nopreview&gt;
&lt;p&gt;&lt;em&gt;Navigation: part 1 | &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-02/automated-software-testing-part-4-unit-tests-and-test-doubles/&quot;&gt;part 4&lt;/a&gt; | &lt;a href=&quot;/weblog/2015-12-26/automated-software-testing-part-5-integration-testing/&quot;&gt;part 5&lt;/a&gt; | &lt;a href=&quot;/weblog/2016-01-19/automated-software-testing-part-6-system-testing/&quot;&gt;part 6&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/nopreview&gt;
&lt;p&gt;Let&apos;s talk about automated software testing. I realize that a lot has already been said on this subject, but it seems that there&apos;s still a significant amount of misunderstanding in this important area of software development. I&apos;m going to attempt to shine some light on a few of the core concepts of testing and how they should be applied in a typical, non-formal software project.&lt;/p&gt;
&lt;p&gt;It should be noted that this post is &lt;em&gt;not&lt;/em&gt; about &lt;a href=&quot;https://en.wikipedia.org/wiki/Test-driven_development&quot;&gt;TDD&lt;/a&gt;. Although they&apos;re undoubtedly related, there is a world of difference between that and what I&apos;m talking about here.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note to experienced devs:&lt;/strong&gt; The part 1 and part 2 posts are intended for people who aren&apos;t very familiar with automated testing. If you&apos;re looking for more in-depth information on specific test types, stick around for &lt;a href=&quot;/weblog/2015-11-09/automated-software-testing-part-3-unit-tests/&quot;&gt;part 3&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why test?&lt;/h2&gt;
&lt;p&gt;Or, rather, why bother with automated tests? I know, I know, it&apos;s the obligatory introductory speech... in written form... but not everybody understands this, so it&apos;s important to mention.&lt;/p&gt;
&lt;p&gt;There are many reasons why automated tests are valuable. Here are three:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Early detection of problems (near compile time; way before they get to production)&lt;/li&gt;
&lt;li&gt;Increased trust in codebase&lt;/li&gt;
&lt;li&gt;Easier maintainability and extensibility&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&apos;s go through these one by one. Early detection of problems is good because the last thing you want is your customers complaining that you released crap. Manual testing helps, of course, but it&apos;s far from perfect, and many issues can and will be missed, so huge gaps will exist in coverage of your code - gaps in which bugs are bound to occur. Adding automated testing fills a lot of these gaps. Also, manual testing is much more expensive than automated testing, since an automated test can be run any number of times essentially for free, while you have to hire people to do manual tests.&lt;/p&gt;
&lt;p&gt;Having automated tests increases your trust in the codebase. When you know that the critically important functionality of your product is tested every time somebody commits code, you feel more at ease. (Tests could be run by a build server when it sees new commits, and they can be run locally as part of a normal build process.) You don&apos;t dread releases and the inevitable issues that QA or your users are going to find and complain about. Maybe you even have more pride in your software.&lt;/p&gt;
&lt;p&gt;Maintainability and extensibility are made immeasurably better, faster, and generally easier when you have automated tests. Simply put, there is very little, if any, concern that the new feature you&apos;re adding (or the bug you&apos;re fixing) is going to negatively impact other areas of the code. This is very much related to the aforementioned trust in the codebase: when you&apos;re confident that there is sufficient testing, you are not paranoid about what could possibly happen when you change some obscure value.&lt;/p&gt;
&lt;h2&gt;When does the above not apply?&lt;/h2&gt;
&lt;p&gt;The three reasons mentioned above don&apos;t magically become true when you add automated tests. They are only true when a lot of things come together. If you have terrible automated tests, they will not detect problems early. If your automated tests are so brittle that changing a single constant breaks half of them, you won&apos;t have easier maintainability. If they randomly fail for no discernible reason on every third run, you may even &lt;a href=&quot;https://en.wikipedia.org/wiki/Broken_windows_theory&quot;&gt;stop caring&lt;/a&gt; about them. And when you don&apos;t believe that the tests in your codebase are valuable, you just won&apos;t trust the codebase any more than you would without any tests whatsoever.&lt;/p&gt;
&lt;h2&gt;How do you make it apply?&lt;/h2&gt;
&lt;p&gt;That&apos;s the difficult part, isn&apos;t it?&lt;/p&gt;
&lt;p&gt;Let&apos;s start by looking broadly at the automated testing landscape. What major types or phases are there? You&apos;ve got &lt;a href=&quot;http://martinfowler.com/bliki/UnitTest.html&quot;&gt;unit testing&lt;/a&gt;. There&apos;s integration testing. And don&apos;t forget about system testing. Others exist as well, but for the most part, these are the essential ones every non-toy software project needs.&lt;/p&gt;
&lt;p&gt;Check out &lt;a href=&quot;/weblog/2015-11-02/automated-software-testing-part-2-the-types/&quot;&gt;part 2&lt;/a&gt; for an exploration of these types.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Passion in software engineering</title>
        <link>http://arktronic.com/weblog/2015-06-28/passion-in-software-engineering/</link>
        <pubDate>Mon, 29 Jun 2015 03:30:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-06-29T03:30:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;I&apos;m lucky enough to follow a good number of people on Twitter who, unwittingly, generate great writing prompts for me. Recently, I&apos;ve seen a few discussions that focus on questioning modern criteria used by software companies in their hiring policies/processes. Passion is one such criterion. Why is it that passion appears to be so highly prized in our field? Is it, perhaps, one of those bullet point criteria that doesn&apos;t actually mean much; one that we simply put on our job descriptions because it sounds good and everyone else is doing it? What does it even mean to be passionate in a software field?&lt;/p&gt;
&lt;h2&gt;The answer is 42&lt;/h2&gt;
&lt;p&gt;Of course, I don&apos;t have the ultimate answer to the questions posed above, but I do hold a few (hopefully insightful) opinions on them. Before discussing the rationale and reasoning behind passion as a criterion for software development work, we need to start by standardizing on our definitions; otherwise, I&apos;ll be discussing one thing, and people reading this could be thinking another.&lt;/p&gt;
&lt;p&gt;When I think about passion - in the non-romantic sense, that is - I think of it as unbridled enthusiasm for a subject: in this case, software. The connotations of this software-focused definition include, but are not limited to, the desire to make the best software that you can, the desire to improve your users&apos; lives through the use of this software, the desire to improve your and your fellow engineers&apos; lives by making the software as painless to develop as possible, and the frustration that is felt when dealing with software that doesn&apos;t feel like it was developed with passion.&lt;/p&gt;
&lt;p&gt;That last connotation is quite important in my mind, as it helps you to focus on what should be done by giving you counter-examples of what should &lt;em&gt;not&lt;/em&gt; be done in software.&lt;/p&gt;
&lt;h2&gt;A passionate engineer&lt;/h2&gt;
&lt;p&gt;Given the above definition of passion, it&apos;s easy to see why companies find it a desirable quality in employees. After all, why &lt;em&gt;wouldn&apos;t&lt;/em&gt; a company want someone who cares deeply for what they do? And doesn&apos;t everyone want to have coworkers who are just as passionate as they are about their work?&lt;/p&gt;
&lt;p&gt;So how can we determine if someone is passionate about software? That&apos;s where things get tricky and murky and generally uncertain. One indication that someone is passionate about software is if they actively participate in open source projects on, e.g., GitHub. Another is participation on StackOverflow or StackExchange. Having personally-developed apps or games in an app store is a great indication as well.&lt;/p&gt;
&lt;p&gt;But what if none of these is the case? What if a candidate doesn&apos;t have a GitHub or StackOverflow account? What if they have no published apps in any app store? Should that be a negative mark against their potential for employment at a software company?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I say, no.&lt;/strong&gt; These indications are just that - indications. They provide no clear guidance either way. All they are is hints, suggestions. There are perfectly good reasons for someone to not have a public presence as described above. NDAs and non-compete clauses are just one kind of example. Spending all your energy on your day job is another. My point is, these simple measures are inadequate in determining someone&apos;s passion for software.&lt;/p&gt;
&lt;h2&gt;Delve deeper&lt;/h2&gt;
&lt;p&gt;In order to determine whether someone is passionate about software, you have to dig. Not into their public presence, but into them. In person. By talking and getting to know them. From personal experience, I know that I wouldn&apos;t always be able to answer questions that relate to my being passionate about software during a formal interview; I tend to be somewhat terrified and mentally frozen at that stage. An informal discussion, over coffee or lunch or even beer might work better in figuring out such a complex criterion.&lt;/p&gt;
&lt;p&gt;When someone is passionate about software, they should be able to give you examples of them going above and beyond their strict job description to improve something related to that job (in varying degrees of detail due to NDAs and such), or of helping others understand certain concepts better in the context of their job, or of them volunteering to take on a little extra (or maybe just different) work to help their team.&lt;/p&gt;
&lt;h2&gt;Not just a bullet point&lt;/h2&gt;
&lt;p&gt;Getting back to the core of this post, having passion as a criterion for hiring a software engineer can indeed make sense, given a certain definition of passion as well as a willingness to genuinely search for that passion in each candidate. However, it&apos;s very much possible for a company to simply list the passion criterion as a &amp;quot;me too&amp;quot; bullet point, or to be lazy in evaluating whether a candidate is indeed passionate about software.&lt;/p&gt;
&lt;p&gt;To companies out there, I say, either be truthful about your desire for real passion in software development, or just remove that bullet point. And to passionate candidates, I say, remember to judge the company you&apos;re interviewing with just as critically as they should be judging you. It&apos;s easy to add such a bullet point to a job requirements document, but it&apos;s a lot harder to prove that it&apos;s genuine. Ask about it. Ask to speak to an employee whom the company considers passionate. If you have passion for software, then so should your coworkers.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Encryption and you</title>
        <link>http://arktronic.com/weblog/2015-05-08/encryption-and-you/</link>
        <pubDate>Fri, 08 May 2015 05:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-05-08T05:00:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;As software developers, we often have to deal with encrypting sensitive data. These days, for most of us, that simply means integrating with and enabling existing security frameworks. For web developers, it&apos;s often as easy as configuring the web server to use HTTPS. &lt;em&gt;(Please note that due to the relatively narrow focus of this post I&apos;m purposely ignoring all the other measures that need to be taken to secure websites. For a quick intro, check out the &lt;a href=&quot;https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project&quot;&gt;OWASP Top Ten&lt;/a&gt;).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One big problem that we software developers face is that encryption is notoriously difficult to get right, and a lot of us don&apos;t even realize just how many things can go wrong. Even if you encrypt a message with your secret key, then decrypt it and verify that the message is the same, you may not be securing your encrypted data sufficiently.&lt;/p&gt;
&lt;h2&gt;What can go wrong?&lt;/h2&gt;
&lt;p&gt;I don&apos;t have an exhaustive list of the possible things that can go wrong when implementing encryption; I&apos;m not sure one even exists. I do, however, know of a few things to watch out for.&lt;/p&gt;
&lt;h3&gt;Side-channel attacks&lt;/h3&gt;
&lt;p&gt;A side-channel attack is essentially an attack on a cryptosystem based on weaknesses in its implementation. Probably the most well known side-channel attack method is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Timing_attack&quot;&gt;timing attack&lt;/a&gt;. I won&apos;t go into details on how it works (that&apos;s what the Wikipedia link is for), but suffice it to say that the best way to avoid it is to &lt;strong&gt;never create your own encryption implementation&lt;/strong&gt;. There are lots and lots of frameworks and other projects that provide encryption capabilities. Use them instead of implementing the encryption code yourself.&lt;/p&gt;
&lt;p&gt;Even if you are an amazingly talented developer, you may not know everything that you need to watch out for in order to create a secure implementation of an algorithm. For example, having constant-time byte comparisons in some situations is crucial to avoiding timing attacks. There are many other nuances (most of which I don&apos;t know either) that can affect the security of your implementation. Please, just use existing code that has been vetted and, better yet, formally audited by a trustworthy organization.&lt;/p&gt;
&lt;p&gt;It should go without saying that creating your own encryption algorithms is a terrible idea. Even encryption schemes created by respected cryptographers have been found to be cryptographically weak before, during peer review.&lt;/p&gt;
&lt;h3&gt;Misconfiguration&lt;/h3&gt;
&lt;p&gt;Did you hear about Sony&apos;s PS3 hack a few years back? They had a pretty &lt;a href=&quot;http://www.engadget.com/2010/12/29/hackers-obtain-ps3-private-cryptography-key-due-to-epic-programm/&quot;&gt;epic misconfiguration&lt;/a&gt; of their firmware signature verification code. Instead of randomizing a particular variable needed in ECDSA, they accidentally kept it constant. This simple mistake allowed people to mathematically solve for the original signing key!&lt;/p&gt;
&lt;p&gt;Just like with side-channel attacks above, the best way to avoid misconfiguration issues is to rely on an existing framework&apos;s security implementation, assuming the framework itself is trustworthy. If you can&apos;t do that, then you need to do sufficient research into the algorithm and the APIs you&apos;re using to make sure that you are providing all the necessary information in the correct format and securely randomized, as the algorithm requirements may dictate. Remember, just because you can encrypt and then decrypt some string, doesn&apos;t necessarily mean that your system is secure.&lt;/p&gt;
&lt;h3&gt;Misunderstanding of the cryptosystem&lt;/h3&gt;
&lt;p&gt;Even if you do your due diligence and &amp;quot;correctly&amp;quot; use a cryptographic algorithm or system, your implicit assumptions about it may lead you astray. For example, are you familiar with the concept of authenticated encryption? If not, then you might be in for a shock. You see, without authenticated encryption, it&apos;s possible to modify the encrypted contents of a message in such a way that, when decrypted, the message may look well-formed, but it can have incorrect information.&lt;/p&gt;
&lt;p&gt;If you&apos;re familiar with the &lt;a href=&quot;http://whatis.techtarget.com/definition/Confidentiality-integrity-and-availability-CIA&quot;&gt;CIA triangle&lt;/a&gt;, you should know that encryption generally falls under the C, or confidentiality. But in addition to it, you also need the I, or integrity. (Naturally, all of this is moot without the A, or availability, but that&apos;s a tad out of scope.) Let&apos;s get a little more concrete.&lt;/p&gt;
&lt;h2&gt;Confidentiality without integrity: an example&lt;/h2&gt;
&lt;p&gt;To restate my earlier point, when you don&apos;t verify integrity, your otherwise valid encryption scheme can fall victim to malicious modification. Here&apos;s a somewhat realistic example. Feel free to follow along in your own console if you have OpenSSL installed. (Hint: it&apos;s available via Git Bash if you&apos;re on Windows.)&lt;/p&gt;
&lt;p&gt;Let&apos;s say you&apos;re sending Alice an encrypted message about a money transfer to Bob. Start by creating a cleartext message.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ echo Give Bob \$500 &amp;gt; clear.txt
$ cat clear.txt
Give Bob $500
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, encrypt that message with OpenSSL, using 128-bit AES in &lt;a href=&quot;http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#CBC&quot;&gt;CBC mode&lt;/a&gt;, with an IV of all zeros and password &amp;quot;hunter2&amp;quot;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl enc -aes-128-cbc -iv 00000000000000000000000000000000 -pass pass:hunter2 -in clear.txt -out cipher.txt
$ xxd -c 32 -p cipher.txt
53616c7465645f5f1bfb00c57919b0082ca0dc3816b12a7ae78eddec8deb736f
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you&apos;re running this on Windows and you don&apos;t have &lt;code&gt;xxd&lt;/code&gt;, try this instead:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ perl -e &apos;local $/; print unpack &amp;quot;H*&amp;quot;, &amp;lt;&amp;gt;&apos; cipher.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let&apos;s decode that message with the same settings.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl enc -d -aes-128-cbc -iv 00000000000000000000000000000000 -pass pass:hunter2 -in cipher.txt
Give Bob $500
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order for Alice to successfully decrypt the message like we did above, she needs to know both the IV and the password. For proper security, the IV must be random each time you encrypt something and, as it generally isn&apos;t considered to be a secret, it&apos;s often sent along with the encrypted message. The password, of course, is not sent.&lt;/p&gt;
&lt;p&gt;But we have a problem. Mallory, a malicious person, has intercepted the encrypted message and very slightly modified it. The only change made was to the IV: Alice received the IV as 00000000000f0e0e0000030000000000. So, she decrypted the message like we&apos;ve done above.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ openssl enc -d -aes-128-cbc -iv 00000000000f0e0e0000030000000000 -pass pass:hunter2 -in cipher.txt
Give Mal $600
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Oh no! Suddenly, the decrypted message has been changed to make Alice think that she needs to give a different amount of money to a different person! Note that in this particular example, the IV consisting of all zeros is unimportant; this demonstration relies only on relative changes to it.&lt;/p&gt;
&lt;p&gt;So how can we fix this problem? Is there a way for Alice to verify that the message hasn&apos;t been tampered with?&lt;/p&gt;
&lt;h2&gt;Authenticated encryption&lt;/h2&gt;
&lt;p&gt;To ensure the integrity of an encrypted message, it must be authenticated. There are a number of ways to do this, some more difficult than others. But one of the simplest ways is to not use CBC mode like we did above, and instead use an authenticated mode like &lt;a href=&quot;http://en.wikipedia.org/wiki/Galois/Counter_Mode&quot;&gt;GCM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, due to the way GCM is handled in OpenSSL, a proper demo of it using the &lt;code&gt;openssl enc&lt;/code&gt; command isn&apos;t possible. Regardless, GCM and other ways to authenticate ciphertext are extremely important to know about and understand.&lt;/p&gt;
&lt;h2&gt;Go forth and encrypt&lt;/h2&gt;
&lt;p&gt;In conclusion, please be aware of the various pitfalls surrounding the correct use of encryption. Understand what encryption provides and what it doesn&apos;t provide. And finally, if you&apos;re unsure about how to implement something related to security, please seek the advice of others. Just about the worst thing you can do is blindly guess - you could easily end up with a system that looks secure to the naked eye, but is completely broken when properly inspected.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>The why and how of personal VPNs</title>
        <link>http://arktronic.com/weblog/2015-04-21/the-why-and-how-of-personal-vpns/</link>
        <pubDate>Wed, 22 Apr 2015 03:30:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-04-22T03:30:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;Virtual private networks, or VPNs, are used by lots of companies, big and small, to connect offices to other offices around the world and to allow people to connect from remote locations into their corporate networks for a variety of reasons - usually having something to do with productivity. But a VPN isn&apos;t just for corporate connectivity. VPNs are used by individuals around the world to securely access resources at their homes and to enhance the security of their online activities.&lt;/p&gt;
&lt;h2&gt;What&apos;s the point (-to-point)?&lt;/h2&gt;
&lt;p&gt;The way I see it, there are two major reasons to have a VPN connection to your home. The first and rather straightforward one is resource access. If you have computers or servers or printers at home that you need to access remotely, VPNs let you do that. Now, if you only have a single computer at home that you might want to connect to, it&apos;s probably easier just to set up something like GoToMyPC or LogMeIn. But if you have other resources, or if you&apos;re a little paranoid and don&apos;t want to install proprietary always-on software, then a VPN might work better.&lt;/p&gt;
&lt;p&gt;The second, and arguably more important, reason to use a VPN is secure Internet access. Although it&apos;s not always obvious, there are many people who use the Internet insecurely without realizing it. This is usually a problem with Web browsing specifically, and especially when it&apos;s done in a public place. If you&apos;ve ever accessed a public Wi-Fi hotspot, you&apos;ve likely accessed some part of the Internet insecurely.&lt;/p&gt;
&lt;h2&gt;Public Wi-Fi is bad?&lt;/h2&gt;
&lt;p&gt;Public Wi-Fi hotspots that are available from your favorite coffee joint, or your hotel room, or the airport aren&apos;t bad per se. The problem is, they&apos;re not very secure. In fact, there&apos;s a hint to that effect in their very name: &lt;em&gt;public&lt;/em&gt;. A lot of the activity you do while on a &lt;em&gt;public&lt;/em&gt; hotspot is going to be public itself, even if nobody can see your laptop screen. If you browse to any website that isn&apos;t served over HTTPS, that entire information exchange is completely out in the open. There are free tools out there that make it extraordinarily easy to see what people nearby are sending and receiving over their wireless connections.&lt;/p&gt;
&lt;p&gt;And then, of course, there&apos;s malicious alteration of data. Depending on a number of factors, it&apos;s possible for someone to modify your Internet requests and responses in real-time as you&apos;re browsing online. Using tools like &lt;a href=&quot;http://www.thoughtcrime.org/software/sslstrip/&quot;&gt;sslstrip&lt;/a&gt;, it&apos;s often easy to trick people into thinking they&apos;re on a secure website, when in fact the security was &amp;quot;stripped&amp;quot; from it. Yes, this applies to banking.&lt;/p&gt;
&lt;p&gt;Even if you don&apos;t bank in coffee shops, simply going to entertainment websites can be dangerous. People can modify the websites&apos; contents to send you malware and infect your computer. This isn&apos;t science fiction; it&apos;s actually pretty easy to do when you have the right tools - most of which are, again, free.&lt;/p&gt;
&lt;h2&gt;VPN to save the day&lt;/h2&gt;
&lt;p&gt;Using a properly configured personal VPN, all your Internet traffic that might otherwise be susceptible to monitoring or unauthorized modification becomes (reasonably) secure once again. That&apos;s because all that traffic gets encrypted and sent through your home&apos;s Internet connection. Let me explain this another way, using state-of-the-art ASCII drawings.&lt;/p&gt;
&lt;p&gt;Here is what your Internet traffic looks like normally on a public Wi-Fi hotspot:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-------    P    -----------    P    -----------
| YOU |  &amp;lt;---&amp;gt;  | HOTSPOT |  &amp;lt;---&amp;gt;  | WEBSITE |
-------         -----------         -----------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The problem here is that the &lt;code&gt;HOTSPOT&lt;/code&gt; section is, for the purposes of this example anyway, completely insecure. If somebody wants to tap into your communications there, they can do it, and you won&apos;t be able to stop them. You won&apos;t even know that they&apos;ve done it, when it&apos;s done right. The &lt;code&gt;P&lt;/code&gt; above the communication channels stands for &amp;quot;plaintext&amp;quot;, or in other words, the normal way that &lt;code&gt;WEBSITE&lt;/code&gt; communicates. If either your plaintext request or the website&apos;s plaintext response is modified in that hotspot section, you&apos;re in trouble, and you won&apos;t necessarily know it, either.&lt;/p&gt;
&lt;p&gt;And here is what VPN traffic looks like in a similar setting:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;-------    C    -----------    C    --------    P   -----------
| YOU |  &amp;lt;---&amp;gt;  | HOTSPOT |  &amp;lt;---&amp;gt;  | HOME |  &amp;lt;---&amp;gt; | WEBSITE |
-------         -----------         --------        -----------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;C&lt;/code&gt; above the communication channels between &lt;code&gt;YOU&lt;/code&gt;, &lt;code&gt;HOTSPOT&lt;/code&gt;, and &lt;code&gt;HOME&lt;/code&gt; stands for &amp;quot;ciphertext&amp;quot;. This is encrypted data, which generally nobody can understand when listening passively at the hotspot, and more importantly, nobody can modify without repercussions. Any unauthorized modification to the ciphertext at the hotspot will be detected and cause the receiver of this bad data (either &lt;code&gt;YOU&lt;/code&gt; or &lt;code&gt;HOME&lt;/code&gt;, depending on direction) to discard it. &lt;code&gt;WEBSITE&lt;/code&gt; will still send and receive plaintext, as it&apos;s designed to do, but instead of communicating with the insecure &lt;code&gt;HOTSPOT&lt;/code&gt;, it will communicate with &lt;code&gt;HOME&lt;/code&gt;, which will in turn communicate with &lt;code&gt;YOU&lt;/code&gt; via &lt;code&gt;HOTSPOT&lt;/code&gt; in a secure manner using ciphertext.&lt;/p&gt;
&lt;h2&gt;I&apos;m convinced. How do I get started?&lt;/h2&gt;
&lt;p&gt;It is important to know that there are many different VPN protocols, servers, and clients out there, and they all have their pros and cons, as well as their supported server and client operating systems. &lt;a href=&quot;http://en.wikipedia.org/wiki/Layer_2_Tunneling_Protocol#L2TP.2FIPsec&quot;&gt;L2TP/IPsec&lt;/a&gt; can be considered an &amp;quot;enterprise&amp;quot; standard; it&apos;s secure, but often very difficult to set up on both the server and client side. &lt;a href=&quot;https://openvpn.net/&quot;&gt;OpenVPN&lt;/a&gt; is a popular open-source SSL/TLS-based VPN protocol, which is available cross-platform and sometimes even built into home routers. PPTP, though still widely available in modern higher-end routers, should be avoided as it is &lt;a href=&quot;http://en.wikipedia.org/wiki/Point-to-Point_Tunneling_Protocol#Security&quot;&gt;not secure&lt;/a&gt;. There is also &lt;a href=&quot;http://en.wikipedia.org/wiki/Secure_Socket_Tunneling_Protocol&quot;&gt;SSTP&lt;/a&gt;, a modern SSL/TLS-based VPN protocol from Microsoft; it&apos;s not widely supported outside of Windows.&lt;/p&gt;
&lt;p&gt;For personal use, my recommendation is to get a &lt;a href=&quot;https://www.raspberrypi.org/&quot;&gt;Raspberry Pi&lt;/a&gt;, or better yet, a &lt;a href=&quot;http://odroid.com/dokuwiki/doku.php?id=en:odroid-c1&quot;&gt;Pi clone with gigabit Ethernet&lt;/a&gt;. Put your favorite Linux distro on it, and then install &lt;a href=&quot;https://www.softether.org/&quot;&gt;SoftEther&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The most versatile VPN server&lt;/h2&gt;
&lt;p&gt;From my research into various VPN solutions, SoftEther appears to be the most versatile. It&apos;s open source, available for Linux (not just x86) and Windows as both server and client, provides free dynamic DNS if your home Internet connection doesn&apos;t have a static IP, and it supports multiple VPN protocols!&lt;/p&gt;
&lt;p&gt;If you have different devices you wish to use to connect to your personal VPN, this works out great. In addition to the built-in SoftEther VPN protocol, you can enable SSTP and OpenVPN - all on the same port, even. For Android and Linux, you can download OpenVPN client software and connect to your VPN through that. This should, in theory, work for Macs as well. For Windows, you could do that too, or you can make use of the built-in support for SSTP to establish a native VPN connection.&lt;/p&gt;
&lt;h2&gt;Do it now&lt;/h2&gt;
&lt;p&gt;Whether you want to browse the Web securely in public places, or you want to access resources in your home, or you just want to play around with VPN tunneling, now is a good time to start. There are, of course, potential gotchas and other issues, as with any sufficiently complex technology. For example, depending on your VPN configuration, only some traffic might go through it, leaving your Internet traffic insecure. There are lots of resources online explaining how all of this stuff works. Check them out. Have fun. Be more secure.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>API design: adequate vs. awesome</title>
        <link>http://arktronic.com/weblog/2015-03-29/api-design-adequate-vs-awesome/</link>
        <pubDate>Mon, 30 Mar 2015 02:31:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-03-30T02:30:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;Inspired by a Twitter conversation regarding REST APIs, I wanted to better express my thoughts and feelings about API design (and I use the term API to not just mean REST API, but also frameworks and anything else that you might create that has some sort of interface for other developers to work with) in a format that allows for more than 140 characters per post. Let&apos;s get straight to the point. What makes an API good, bad, adequate, usable, unpleasant, or whatever other term you prefer that essentially still boils down to good or bad?&lt;/p&gt;
&lt;h2&gt;UX? For an API?&lt;/h2&gt;
&lt;p&gt;To say that the answer is &amp;quot;UX&amp;quot; might seem a little strange, since we don&apos;t generally think of developer-facing stuff as needing a facelift, but that really &lt;em&gt;is&lt;/em&gt; the answer, in my opinion. The main difference is in who the &amp;quot;U&amp;quot; is in &amp;quot;UX&amp;quot; here. The developer is your user. So, UX in this instance could be called DX (not to be confused with 486DX).&lt;/p&gt;
&lt;h2&gt;But why?&lt;/h2&gt;
&lt;p&gt;Why would you want to design your API to be good, though? Isn&apos;t it enough to simply expose the functionality you&apos;re intending to expose, and then abandon the project and move on to the next thing? Some people/companies clearly think that is enough. However, I&apos;m of the opinion that designing an API that is pleasant to use will develop goodwill between you/your company and your API users. If you&apos;re charging for this API, they&apos;d be more willing to pay. If you&apos;re using it to show off your skills, they&apos;d be more impressed. If you&apos;re doing it purely out of the goodness of your heart, then releasing a bad API would be a disservice to yourself, as you want to release something that people can use.&lt;/p&gt;
&lt;h2&gt;Introducing Some Random Developer&lt;/h2&gt;
&lt;p&gt;The biggest problem in designing a good API is being able to let go of all your innate knowledge of the underlying system and how all of its pieces fit together, and pretending to be Some Random Developer (or SRD for short) who has stumbled across your API in hopes of doing something useful with it. When SRD finds your API, what is the first thing they&apos;ll do? Probably try to determine whether your API is what they want in the first place. Does it seem to solve their problem? Is it available for their platform? Does it have licensing or cost restrictions? Making all of this information easily accessible makes SRD&apos;s life that much simpler.&lt;/p&gt;
&lt;h2&gt;Provide &lt;em&gt;good&lt;/em&gt; examples&lt;/h2&gt;
&lt;p&gt;The next thing SRD will look for is examples of using your API to solve the exact problem they&apos;re trying to solve. Unless you&apos;re prescient, you won&apos;t be able to anticipate every SRD&apos;s problem, but that&apos;s all right. If you have lots of examples (that are well documented!) of various uses of your API, there&apos;s a good chance that SRD will either find the exact solution they need, or at least be able to extrapolate it from the multitude of examples you&apos;ve provided.&lt;/p&gt;
&lt;h2&gt;Tedious, detailed documentation&lt;/h2&gt;
&lt;p&gt;After figuring out the API endpoints (or methods or interfaces or whatever they may be) that SRD needs to solve their problem, they&apos;ll want to see exactly how their data need to be sent in to your API and exactly what your API will output, down to the gory details such as encoding, headers, and any surprises that SRD might encounter while working with your API. This is all pretty tedious to document, and there are projects out there that strive to &lt;a href=&quot;http://swagger.io/&quot;&gt;make it easier&lt;/a&gt;. They&apos;re slowly getting there, but I&apos;m sure that not every kind of API can be auto-documented - at least not particularly well. However, creating this highly detailed documentation is extremely important to making a good API. If you put yourself in SRD&apos;s shoes, can you imagine how much time you can save when all of this information that you need is right at your fingertips?&lt;/p&gt;
&lt;h2&gt;Keep it updated&lt;/h2&gt;
&lt;p&gt;That brings us to another sticking point with documentation: keeping it accurate and up to date. &lt;strong&gt;I cannot stress enough just how important this is.&lt;/strong&gt; If SRD finds that your documentation is inaccurate (whether that&apos;s due to it being outdated or not), then they will lose all trust in it. All of that potential for saved time is thrown out the window at that point, as SRD has to double-check everything that is documented to ensure that the behavior is exactly what the documentation claims it is.&lt;/p&gt;
&lt;h2&gt;Keep it consistent&lt;/h2&gt;
&lt;p&gt;After SRD is comfortable enough with your API, they&apos;ll want to write a quick proof-of-concept app. So now we come to the design itself. Since this post isn&apos;t specifically about REST APIs or language- or framework-specific libraries, I&apos;ll refrain from making overly concrete suggestions and instead focus on principles. The first principle is consistency. Whatever else you may do, make your API consistent. &lt;em&gt;(As an aside, this is where the Git CLI fails miserably.)&lt;/em&gt; If your API is organized into logical sections and has consistent naming for everything, SRD&apos;s job becomes a lot easier. It is important to note that by &amp;quot;logical&amp;quot; I mean something that seems logical to SRD, being an outsider, and not necessarily to you. You must abstract internal implementation details away from SRD so that, no matter whether you&apos;ve got any ugly hacks or bad architecture in your code, SRD only sees the pristine, carefully crafted API that makes sense at first glance. This consistency should extend to naming, modularization, accepted arguments, and return values.&lt;/p&gt;
&lt;h2&gt;Easy to use, hard to abuse&lt;/h2&gt;
&lt;p&gt;The second principle is fool-proofing. I don&apos;t mean to call SRD a fool, but when dealing with an unfamiliar API, it can sometimes be easy to use it wrong. &lt;strong&gt;You should take care to make it easy to use the API correctly and to make it hard to misuse it.&lt;/strong&gt; For example, if your API returns IDs with certain objects, where such IDs are transient and can change at some point in time, you should strive to get rid of this potential &amp;quot;gotcha&amp;quot;. Instead of simply documenting it as an &amp;quot;important detail&amp;quot; and risking SRD skipping over that part of the docs, make your API avoid this functionality completely. Maybe you don&apos;t really need to return these transient IDs at all. Or maybe you can return something else instead of these IDs that is more permanent. The fewer &amp;quot;gotchas&amp;quot; your API has, the fewer headaches SRD will have, and the less time you&apos;ll spend troubleshooting &amp;quot;foolish&amp;quot; misuses of your API.&lt;/p&gt;
&lt;h2&gt;Versioning and compatibility&lt;/h2&gt;
&lt;p&gt;Speaking of fool-proofing, what about versioning and backward- and forward-compatibility? That&apos;s also something you have to take into account when creating a good API. There is no simple answer here, but in general you still want to make it easy to use your API correctly and difficult to misuse it. In the case of REST APIs, if you cannot guarantee backward-compatibility, you should find a way to ensure that older clients do not break when you introduce changes. Unfortunately, this often means that even the action of fixing an existing version of your API should be considered very carefully, as some clients may be relying on the &lt;a href=&quot;https://technet.microsoft.com/en-us/magazine/2007.04.windowsconfidential.aspx&quot;&gt;current buggy behavior&lt;/a&gt;. Again with REST APIs, allowing access to the &amp;quot;latest and greatest&amp;quot; version through a special endpoint doesn&apos;t work well, and &lt;a href=&quot;http://blog.wordnik.com/wordnik-api-has-gone-beta&quot;&gt;it has been tried&lt;/a&gt;. Wordnik found that it became very confusing both for internal developers and for API users, and gave up on that idea. In the case of libraries, SRD at least has the option of continuing to use an older version.&lt;/p&gt;
&lt;h2&gt;Sunsetting&lt;/h2&gt;
&lt;p&gt;Finally, you should never retire an API without giving public notice sufficiently in advance, to let SRD move off to something else. In the case of large organizations employing SRD, sufficient may mean as much as a year or more. But you can judge that for yourself. If your API is pretty popular, a longer deprecation time is generally warranted. This applies to both the retirement of entire API projects and to the retirement of mere versions of your API.&lt;/p&gt;
&lt;p&gt;To summarize this post, you should apply the &lt;a href=&quot;http://en.wikipedia.org/wiki/Golden_Rule&quot;&gt;Golden Rule&lt;/a&gt; to the development of an API. Pretend that you are that mythical SRD. When you encounter an API that you might want to use, what would make your life easier when looking into that API? Would you appreciate excellent, accurate, in-depth documentation with plenty of examples? Would a consistent naming and data passing scheme appeal to you? Be willing to suffer a little pain and teduim in order to make all of your users suffer that much less. They&apos;ll thank you for it.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>YAPoP (Yet Another Post on Pairing)</title>
        <link>http://arktronic.com/weblog/2015-02-03/yapop-yet-another-post-on-pairing/</link>
        <pubDate>Wed, 04 Feb 2015 04:30:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2015-02-04T04:30:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;I almost named this post &amp;quot;Yet Another Personal Opinion On Pairing&amp;quot;, but the resulting acronym didn&apos;t seem entirely appropriate. So, what&apos;s the big deal with pairing - or pair programming, as it were? Overall, it can be a very useful tool. It often helps to flesh out the design of a solution that&apos;s about to be implemented, since one person usually does more thinking while the other person does more typing. It helps to keep people focused on the task at hand. It can also be very effective at providing instant gut-checks of code and of concepts. And, of course, it&apos;s extremely useful in bringing new people up to speed on the codebase and domain knowledge.&lt;/p&gt;
&lt;p&gt;The last point above is perhaps the most important reason to pair program. In my experience, the &amp;quot;onboarding&amp;quot; process for a new team member is made much more straightforward and expeditious when that person is pairing with someone more experienced or familiar with the project. In addition to providing valuable context for the code and the domain, the more experienced person will impart lots of undocumented knowledge during the task that the pair is working on, which is incredibly helpful not only for the new person, but also for the team, as interesting facts and processes that have never been formally defined come out, and this allows the team to critically examine them and perhaps to modify/improve, or at the very least to document them for future use.&lt;/p&gt;
&lt;p&gt;Thanks to all of the benefits that pairing provides, as well as the positive press it&apos;s been getting in the last few years, it&apos;s no wonder that more and more people and companies are seriously looking into how best to use pair programming techniques. And it seems that many have decided to embrace it to such an extent as to make it a mandatory part of their development process.&lt;/p&gt;
&lt;p&gt;But permit me to be an iconoclast for a bit and stray from the teachings of the Patron Priests of Pair Programming. Pairing is not all rainbows and sunshine. There are real reasons why it should sometimes be avoided. If we ignore those reasons and embrace pairing dogmatically, we&apos;re doing everyone - including ourselves - a great disservice. Let&apos;s start with an easy to comprehend reason: scheduling. Different people work at different times. They have different meetings, lunch hours, medical appointments, and so forth. If one person in a pair is unavailable for a couple of hours, should work on the task stop? Should another person be brought in to replace the missing one either temporarily or permanently? I would argue that the most productive answer is to be pair-less for those two hours, and once the missing person comes back, they can be quickly brought up to speed on the task&apos;s progress, since they&apos;re already familiar with it. Now let&apos;s take a closer look at the alternatives to this option.&lt;/p&gt;
&lt;p&gt;Bringing in another person temporarily would be disruptive. Everyone has a unique way of communicating, typing (Dvorak?), and behaving in general. Adjusting to pairing with a different person for a couple of hours only to have to adjust again when the original missing person returns is a huge time sink and would cause productivity to plummet. I write from experience: when I&apos;m working on a task, a disruption can cost me anywhere between zero and twenty minutes of downtime. If I&apos;m &lt;a href=&quot;http://heeris.id.au/2013/this-is-why-you-shouldnt-interrupt-a-programmer/&quot;&gt;mentally juggling&lt;/a&gt; a lot of objects, keeping them all in my mind in order to figure out the appropriate way to connect them and isolate them and test them, and suddenly &lt;em&gt;BAM!!&lt;/em&gt; I&apos;m interrupted, it will take me a while to get back to that state, where I can once again continue to solve the problems at hand. (This wouldn&apos;t be a quick interruption, either - I&apos;d have to explain the overall task, the progress made, the current status, and what needs to be done next.) Constantly switching pairs like this multiple times a day is effectively the concept of Promiscuous Pairing - something I&apos;ve never seen work, and also something that, outside of the original &lt;a href=&quot;http://user.it.uu.se/%7Ecarle/softcraft/notes/PromiscuousPairing.pdf&quot;&gt;Arlo Belshee paper&lt;/a&gt;, I&apos;ve never read about working any better than normal pairing.&lt;/p&gt;
&lt;p&gt;Bringing in another person as a permanent pair replacement, in addition to causing a one-time disruption, does a disservice to the missing person because they have invested a not-insignificant amount of time learning the task&apos;s requirements, challenges, and proposed solutions, only to find that knowledge unusable since they&apos;re no longer on the task. How frustrating! Everyone knows that there is a certain amount of satisfaction in implementing your vision and seeing it work. There is also a certain amount of frustration when the task ends up being implemented differently from how you envisioned it - especially after you have personally invested in it by having begun work on that task. Yes, I&apos;m hinting at the concept of code ownership, which some feel is misguided, but I have found that it helps tremendously in keeping code quality up. The reasoning for this is rather simple: if you care about the code that you helped to design or write, you will fight to keep it clean.&lt;/p&gt;
&lt;p&gt;Continuing on to the next reason to not pair: fatigue. Good, high-productivity pairing isn&apos;t just tiring; it&apos;s exhausting. I know that I, for one, don&apos;t want to come home every day exhausted. I&apos;ve been there before, and it&apos;s not good. I&apos;m not a robot whose job is to work tirelessly until its gears rust and disintegrate; there&apos;s only so much exhaustion that I can take. Doing too much of this is a surefire way to get burnt out. (I&apos;ve been there, too, and believe me, it&apos;s so much worse than just being exhausted.) Sometimes, a nice, long break is needed from your pairing. So why not take on a new task solo and see it all the way through before going back to pairing? There is no need for unquestioning orthodox adherence to pairing.&lt;/p&gt;
&lt;p&gt;And what about productivity? Sure, there are times when pairing increases it. But on the other hand, I know I&apos;m not the only one who&apos;ll occasionally stay at the office late, after everyone&apos;s gone, and implement a whole lot of the task in a fraction of the time it would have taken if I were pairing with someone. I believe that&apos;s referred to as being &amp;quot;in the zone&amp;quot;. It is possible for this to happen while pairing, but from my personal experience, as well as what I&apos;ve seen &lt;a href=&quot;http://programmers.stackexchange.com/questions/120407/how-can-you-achieve-and-maintain-flow-while-pair-programming&quot;&gt;and&lt;/a&gt; &lt;a href=&quot;https://peniwize.wordpress.com/2013/11/17/pair-programming-give-it-a-rest/&quot;&gt;read&lt;/a&gt;, it&apos;s exceedingly rare.&lt;/p&gt;
&lt;p&gt;There is even a downside to the incredibly helpful pairing during the onboarding process. When an experienced person and one who is newer to coding (perhaps someone just out of college) are pairing on tasks in an unfamiliar codebase, this can cause issues. The reason for that is, the experienced person will want to dig into the code to determine how it currently works in order to modify it appropriately and within the established practices in that codebase. However, doing this while pairing with someone newer doesn&apos;t always work well. The newer person will often want to jump straight to coding, which is rarely what you should be doing in an unfamiliar codebase. If the more experienced person is &amp;quot;driving&amp;quot;, then they&apos;ll likely be going a mile-a-minute between different functions, files, and projects, determining how everything fits together, while the newer person is lost and bored. If the newer person is at the keyboard and mouse, then the more experienced person is &lt;em&gt;still&lt;/em&gt; doing the code exploration, but through the other&apos;s hands. That doesn&apos;t help the newer person, since they&apos;re only following directions at that point.&lt;/p&gt;
&lt;p&gt;Finally, let&apos;s look at some of the positives from the first paragraph in a different light. The first two, to be more specific. When pairing helps to flesh out your solution&apos;s design, that could be a sign of bad practices: it&apos;s simply not a good idea to start coding before you have a plan of action. We&apos;ve all done it at one point or another (I know I have) only to regret it later on, when we realize the ramifications of the choices that we implicitly made while coding up the solution with the first thoughts that came to us. I would, in fact, say that pairing is more useful when coming up with solutions to tough problems, rather than when coding them up. Indeed, I&apos;ve worked on tasks in the past where the task would start with the pair figuring out the what and the how, and then splitting up and doing different parts of the work simultaneously. Toward the end of the task, we would merge our code, resolve any rarely-occurring outstanding issues, and move on. I wouldn&apos;t call this proper &amp;quot;pairing&amp;quot;, but it&apos;s worked remarkably well time and again.&lt;/p&gt;
&lt;p&gt;Along the same lines, keeping people focused through the use of pairing is another sign of bad practices (or, dare I say, bad personnel - but I won&apos;t go into that here). Personally, if I can&apos;t stay focused on a task, the reason is usually that the task itself isn&apos;t defined well, and I have to keep asking various people for clarifications. Another reason for this is tooling and environment failure, where the tests take over half an hour to run or the continuous integration server randomly fails builds. These disheartening events have a negative impact on productivity as well as morale, and they discourage people from being vigilant about keeping the codebase clean and tidy. Forcing progress to be made in these situations is nothing more than a band-aid that&apos;s used to hide the underlying issues.&lt;/p&gt;
&lt;p&gt;So, what am I trying to say? That pairing is bad and we should stop doing it? No. Pairing has clear benefits to it; the aforementioned gut-checks and faster code familiarization are extremely useful to the pair, the team, and the project as a whole. But it also has its own set of problems such as increased fatigue, and it allows teams to sweep some severe pre-existing problems under the rug, like skipping the design phase of tasks and getting used to &lt;a href=&quot;http://en.wikipedia.org/wiki/Broken_windows_theory&quot;&gt;broken windows&lt;/a&gt;, as mentioned above. What I &lt;em&gt;am&lt;/em&gt; trying to say is that pairing needs to happen when it is both needed and wanted - not when it&apos;s mandated by policy or contract or anything of the sort. And please don&apos;t tell me that, in order to experience the best that pairing has to offer, I have to be fully indoctrinated. That&apos;s akin to telling me that I have to have faith in your religion in order to truly embrace it. It&apos;s nothing more than circular reasoning. As professionals in the field of software engineering, we should use our best judgment to determine when pairing is desired and act accordingly, instead of blindly worshipping at its altar.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;EDIT: I&apos;d like to thank &lt;a href=&quot;https://larry-price.com/&quot;&gt;Larry&lt;/a&gt; for reading the drafts of this post and giving really useful feedback.&lt;/em&gt;&lt;/p&gt;
</description>
        </item>

        <item>
        <title>SSL yourself</title>
        <link>http://arktronic.com/weblog/2014-11-06/ssl-yourself/</link>
        <pubDate>Thu, 06 Nov 2014 14:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2014-11-06T14:00:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;Hey, you! Do you have a blog? Or a website? Now, tell me this: is it accessible over HTTPS? If you&apos;re like most people, your answer is going to be something along the lines of, &amp;quot;No, why should I bother? I have nothing security-critical on my site, and I&apos;m certainly not accepting credit cards for people to enter.&amp;quot; That&apos;s a perfectly valid response. But I ask you to be just a little more forward-looking. Wouldn&apos;t it be wonderful if you didn&apos;t have to worry about whether you&apos;re really accessing your bank&apos;s website and not that of some scammer? Or if the information you&apos;re entering at the local cafe is being intercepted by someone sitting two tables away?&lt;/p&gt;
&lt;p&gt;These are not simple issues to solve, and serving your personal blog over SSL seems almost entirely unrelated. But it is indeed related: we want to get to a point where everything on the web is &lt;em&gt;secure by default&lt;/em&gt;. That means, among other things, that everything is transferred securely without the ability to fall back to the old, insecure protocols from the 1980s and 1990s. And you can take a small, but significant, step in that direction by serving your website over SSL. If enough people do this, then perhaps someday HTTP without the S at the end of it can be deprecated and eventually removed. I know, it&apos;s quite a lofty goal. But it can be achieved with small steps taken by everyone together.&lt;/p&gt;
&lt;p&gt;&amp;quot;All right, let&apos;s say I want to be part of this Utopian vision of yours,&amp;quot; you say, &amp;quot;but I don&apos;t want to pay for a dedicated IP address or for an SSL certificate. It&apos;s still not worth it for me.&amp;quot; Well, I have good news for you. Neither of those is required anymore - thanks to &lt;a href=&quot;https://www.cloudflare.com&quot;&gt;CloudFlare&lt;/a&gt;. If you don&apos;t have a dedicated IP address for your site or if you don&apos;t have a valid SSL certificate, you can still serve your website over SSL and have it work in the majority of browsers and operating systems. To overcome the IP address problem, CloudFlare has implemented &lt;a href=&quot;http://en.wikipedia.org/wiki/Server_Name_Indication&quot;&gt;SNI&lt;/a&gt;, and to overcome the valid SSL certificate problem they&apos;ve started issuing their own SSL certificates for everyone who signs up - even those with free accounts! (The certificates are from Comodo, but CloudFlare has a sub-CA.)&lt;/p&gt;
&lt;p&gt;&amp;quot;This is all very interesting, but I don&apos;t host my own blog. Someone else does it for me.&amp;quot; In that case, I humbly ask that you become an activist. Don&apos;t worry, I don&apos;t want to start an &amp;quot;occupy the blogosphere&amp;quot; movement. But a single email or support ticket requesting SSL support can go a long way - especially if a whole bunch of people do it.&lt;/p&gt;
&lt;p&gt;The World Wide Web is constantly evolving, and you can help it evolve in the right direction. Taking a small step towards being secure by default right now will allow for larger steps in the future, and maybe, just maybe, someday we won&apos;t have to be concerned so much with the security of our information, and we could focus on tackling more pertinent issues.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>HashProp - a better way to MD5 (and SHA) in Windows</title>
        <link>http://arktronic.com/weblog/2014-09-27/hashprop---a-better-way-to-md5-and-sha-in-windows/</link>
        <pubDate>Sat, 27 Sep 2014 15:00:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2014-09-27T15:00:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;HashProp is a Windows shell extension that I decided to write at the previous Carmel Code and Coffee meetup.
Basically, it adds a new tab to the File Properties dialog for every file in Windows with the ability to calculate the file&apos;s hashes:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/page-data/hashprop.png&quot; alt=&quot;HashProp screenshot&quot; /&gt;&lt;/p&gt;
&lt;p&gt;It&apos;s a simple interface, so people can quickly and easily calculate MD5, SHA-1, SHA-256, and SHA-512 for any file.&lt;/p&gt;
&lt;p&gt;I created this thing after getting some inspiration from &lt;a href=&quot;https://twitter.com/Jaxidian&quot;&gt;@Jaxidian&lt;/a&gt;. He showed me a similar app that hasn&apos;t
been updated in years, and that didn&apos;t support SHA-256 or SHA-512. Personally, I&apos;ve been using an even older app that integrated into
every file&apos;s right-click context menu - again, without support for SHA-256 or SHA-512. I remembered that, with the introduction of .NET 4,
Windows shell extensions could be created in a managed environment, and I found a wonderfully &lt;a href=&quot;http://sharpshell.codeplex.com/&quot;&gt;easy to use SDK&lt;/a&gt; to do just that.&lt;/p&gt;
&lt;p&gt;One thing I intentionally focused on was overall UX. Is the file being hashed very slowly? The percentage is shown while the calculation is
occurring to let users know that the system is working. This is especially useful when hashing large files that are served over the network.
What do I often do after hashing a file? I copy the hash to the clipboard. Clicking the textbox with the hash (or leaving the textbox
selected until hashing completes) will auto-select all text in the textbox. Oh yeah, clicking the Calculate button disables it while the
calculation is occurring, and the corresponding textbox is selected. And finally, if the hashing is taking a long time and the user
closes the File Properties dialog, the hashing is stopped and the file handle is released. This is all a part of good UX.&lt;/p&gt;
&lt;p&gt;HashProp is released under the ISC License. The source code and links to binaries are on &lt;a href=&quot;https://github.com/arktronic/hashprop/&quot;&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Well, it looks like I released the first version a tad too early - before doing extensive testing. Turns out, the initial version
of HashProp had two issues: the installer didn&apos;t always register the DLL correctly in the Registry due to UAC, and the property page code
that I&apos;m relying on from SharpShell doesn&apos;t seem to work on all platforms. I&apos;m releasing version 1.0.1 now with a different way to access
the hashing UI (file context menu) and a fixed installer. Sorry about that.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>New blog and new tea</title>
        <link>http://arktronic.com/weblog/2014-08-09/new-blog-and-new-tea/</link>
        <pubDate>Sat, 09 Aug 2014 23:50:00 +0000</pubDate>
        <guid isPermaLink="false">ID 2014-08-09T23:50:00 on http://arktronic.com</guid>
        <description>&lt;p&gt;It seems that I recreate my blog every few years, painstakingly migrating old posts to the new blog engine, ensuring that they continue to look decent, and generally improving things.
It&apos;s no different this time: I&apos;ve retired my Drupal-based blog, mainly because I&apos;m tired of having to constantly make sure that it&apos;s updated to the latest version,
so that newly discovered vulnerabilities in its codebase are fixed. Sure, its security is not nearly as bad as that of WordPress, but it&apos;s still annoying enough that I wanted something better.&lt;/p&gt;
&lt;p&gt;Most blog engines out there must be kept updated because they run complex dynamic code and are thus potentially vulnerable to various attacks - anything from simple &lt;a href=&quot;https://www.owasp.org/index.php/SQL_injection&quot;&gt;SQL injection&lt;/a&gt; to &lt;a href=&quot;https://www.owasp.org/index.php/Cross-site_Scripting_%28XSS%29&quot;&gt;XSS&lt;/a&gt; and &lt;a href=&quot;https://www.owasp.org/index.php/Cross-Site_Request_Forgery_%28CSRF%29&quot;&gt;CSRF&lt;/a&gt;.
(As an aside, if you make websites, you &lt;em&gt;need&lt;/em&gt; to check out &lt;a href=&quot;https://www.owasp.org&quot;&gt;OWASP&lt;/a&gt;.)
So, how can I have a blog and avoid having to keep its engine up to date to avoid security problems? By removing the dynamic on-the-fly processing! Enter the &lt;strong&gt;static site generator.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;For those unfamiliar with this concept, it&apos;s pretty simple: you have a bunch of source files and an application that performs a one-time (offline) task of transforming those source files into a functioning website,
complete with CSS, images, scripts, and whatever else a site needs - except for dynamically generated content such as PHP or ASP.NET or Rails code. You&apos;re left with just static HTML and resources,
which dramatically reduces your website&apos;s attack surface. In addition to the improved security, you get more benefits like faster page loads and simpler, more effective caching.
This is again due to the lack of dynamic content processing.&lt;/p&gt;
&lt;p&gt;Of course, static sites do have their limitations. Any kind of user-generated content cannot be hosted on them (although things like comments can be outsourced quite effectively to services such as &lt;a href=&quot;https://disqus.com&quot;&gt;Disqus&lt;/a&gt;).
Trivial functions like site search become impossible without relying on a third party to index your content.&lt;/p&gt;
&lt;p&gt;I am willing to work around issues caused by the lack of dynamic processing, so a static blog works for me. Once I came to this realization, I needed to pick the right static site generator. There are quite a few
choices out there, with varying degrees of maintenance, power, and flexibility, and many runtimes. I think it&apos;s safe to say that &lt;a href=&quot;http://jekyllrb.com&quot;&gt;Jekyll&lt;/a&gt; is currently the most popular one, especially given that it powers &lt;a href=&quot;https://pages.github.com&quot;&gt;GitHub Pages&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Unfortunately, I found Jekyll to be too restrictive in what it allows me to do, and most of the other generators ended up being just as restrictive, difficult to set up, or simply abandoned.
All that made me come to the conclusion that I needed to write my own. So I did.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://bitbucket.org/Arktronic/genmaicha&quot;&gt;Genmaicha&lt;/a&gt; is a static website generator written in C# for .NET and Mono, using Markdown for content markup and Razor for templating. It currently has no official releases, because it&apos;s in active development and
its APIs have not yet stabilized. But it&apos;s far enough along that it currently powers this blog. If you&apos;re wondering about the name, &lt;a href=&quot;http://en.wikipedia.org/wiki/Genmaicha&quot;&gt;genmaicha&lt;/a&gt; is a delicious Japanese green tea with roasted rice.&lt;/p&gt;
&lt;p&gt;I used an existing HTML to Markdown converter (and tweaked it a bit) to convert my old posts from my Drupal blog to Markdown files, and then I created a completely new design from scratch.
I think it looks a lot better than the old blog. I would appreciate any feedback on Genmaicha or on the design of the new blog!&lt;/p&gt;
&lt;p&gt;By the way, Genmaicha is licensed under the permissive, OSI-approved &lt;a href=&quot;http://opensource.org/licenses/ISC&quot;&gt;ISC License&lt;/a&gt;. I&apos;m not a fan of strong copyleft licenses such as the GPL: my definition of software freedom, unlike that of the FSF,
includes the freedom to make proprietary customizations to software.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>How to run an elevated privilege ("as administrator") app on Windows startup</title>
        <link>http://arktronic.com/weblog/2013-11-06/how-to-run-an-elevated-privilege-as-administrator-app-on-windows-startup/</link>
        <pubDate>Wed, 06 Nov 2013 19:26:09 +0000</pubDate>
        <guid isPermaLink="false">ID node/38 on http://arktronic.com</guid>
        <description>&lt;p&gt;Running an elevated application at startup time on Windows 8 is rather annoying. Normally, the easiest way to run anything at startup is to simply create a shortcut to it in &lt;code&gt;%AppData%\Microsoft\Windows\Start Menu\Programs\Startup&lt;/code&gt;.
Unfortunately when UAC is on, Windows will simply refuse to launch any  shortcut at startup time if its properties are configured to run the  application to which it points as administrator. In Windows 7 and below, disabling UAC let everything work just fine. Windows 8 changes matters by requiring UAC to be enabled in order for the Metro app sandbox to function - in other words, disabling UAC kills all Metro apps. You can still silence UAC (I do) but it&apos;s no longer reasonable to disable it.&lt;/p&gt;
&lt;p&gt;There are multiple ways to get around this restriction, such as using the Task Scheduler to launch the app as a startup task or writing a Windows service to launch the app. However, the most straightforward way is still to use the Startup directory. The big difference is, instead of calling a shortcut, execute a script!&lt;/p&gt;
&lt;p&gt;In my case, I want to launch the OpenVPN GUI. It needs elevated privileges in order to control its virtual network interface. Instead of creating a shortcut, I created a new file named OpenVPN.vbs in the Startup directory with the following two lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Set UAC = CreateObject(&amp;quot;Shell.Application&amp;quot;)  
UAC.ShellExecute &amp;quot;C:\Program Files\OpenVPN\bin\openvpn-gui.exe&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;runas&amp;quot;, 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That script launches the OpenVPN GUI with elevated privileges. Simply replace the first parameter for ShellExecute with whatever app you&apos;re trying to launch and you&apos;re good to go! And if you need to pass command-line arguments, that&apos;s what the second parameter is for.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Beware of silent failures</title>
        <link>http://arktronic.com/weblog/2013-09-09/beware-of-silent-failures/</link>
        <pubDate>Mon, 09 Sep 2013 04:37:17 +0000</pubDate>
        <guid isPermaLink="false">ID node/37 on http://arktronic.com</guid>
        <description>&lt;p&gt;What&apos;s worse than experiencing a failure in a production system? I&apos;ll tell you: not knowing that it occurred.&lt;/p&gt;
&lt;p&gt;I was inspired to write this post after seeing a neat presentation at work about a new internal tool at the office. One fact that jumped out at me during the presentation was that, if a failure occurred while running a periodic background task, an email would be fired off informing the admin(s) of this problem. On the surface, that seems like a reasonable action to take; be silent unless something&apos;s wrong.&lt;/p&gt;
&lt;p&gt;But there&apos;s a fundamental flaw in this concept. If something does indeed go wrong, how can you be sure that email will still function? Or to say this in a more generic manner, if a failure occurs, how can you be sure that a push-based notification of this failure can still happen? The answer, of course, is that you can&apos;t. And that&apos;s why you shouldn&apos;t do it.&lt;/p&gt;
&lt;p&gt;So, what options are there? Fundamentally, you can do two things to verify the correct functionality of your system. The first thing you can do is push success notifications, in addition to or instead of failure ones. In the particular example of the internal tool, it could send out an email every time the periodic task completed successfully. Or every 30th time it completed successfully if it runs often. The point here is to let the admin(s) know that the system is working as expected. If suddenly the emails stop or begin reporting failures (e.g., &amp;quot;Tasks ran; 2 of 30 failed&amp;quot;) then appropriate actions can be taken to remedy the situation.&lt;/p&gt;
&lt;p&gt;But what if you don&apos;t want to spam your admin(s) with useless status reports that they&apos;ll just ignore anyway? Well, it should be part of their job to monitor this stuff, so don&apos;t be afraid to do it! Alternatively, you could use a pull-based approach to this problem. If you have an external monitoring solution set up, use it to get status reports from your system. You can have your system publish a report of its background activities on a special URL or a shared network location and then have your external monitoring solution periodically check that report for problems. For websites, this could even be achieved by using uptime monitoring services like &lt;a href=&quot;http://www.uptimerobot.com/&quot;&gt;Uptime Robot&lt;/a&gt;. You can have a special reporting URL show its status (as simple as &amp;quot;OK&amp;quot; or &amp;quot;ERROR&amp;quot;) based on dynamic determination of whether any failures occurred as well as whether the periodic activities actually ran. Then the uptime monitoring service can check for keywords (like &amp;quot;OK&amp;quot;) in the reporting page&apos;s contents to verify proper functionality or alert you should the keywords fail to match.&lt;/p&gt;
&lt;p&gt;Of course, if you have your own separate monitoring solution, you&apos;ll need to ensure that that solution itself is continuing to function properly. Yes, what I&apos;m getting at is that at some point you should still have periodic success notifications for certain critical services. After all, if your monitoring solution stops being able to alert you, that&apos;d be a major cause for concern.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Regarding company loyalty</title>
        <link>http://arktronic.com/weblog/2013-08-18/regarding-company-loyalty/</link>
        <pubDate>Mon, 19 Aug 2013 03:58:32 +0000</pubDate>
        <guid isPermaLink="false">ID node/36 on http://arktronic.com</guid>
        <description>&lt;p&gt;Doing some evening web surfing, I came across an intriguing Reddit post entitled &amp;quot;&lt;a href=&quot;http://www.reddit.com/r/programming/comments/1kmkwy/dont_be_loyal_to_your_company/&quot;&gt;Don&apos;t be loyal to your company&lt;/a&gt;&amp;quot; that pointed to &lt;a href=&quot;http://www.heartmindcode.com/blog/2013/08/loyalty-and-layoffs/&quot;&gt;a blog post&lt;/a&gt;. The blog is currently offline, probably due to the Reddit traffic, but a &lt;a href=&quot;http://webcache.googleusercontent.com/search?q=cache:qRMCnAJE2UoJ:www.heartmindcode.com/blog/2013/08/loyalty-and-layoffs/&amp;amp;hl=en&amp;amp;gl=uk&amp;amp;strip=1&quot;&gt;Google cache of the post&lt;/a&gt; exists. After reading that post, I felt a visceral need to respond, so that&apos;s what I&apos;m doing now.&lt;/p&gt;
&lt;p&gt;The author of the aforementioned blog post essentially claims that a corporation does not deserve loyalty because it cannot be loyal to you and will &lt;em&gt;necessarily&lt;/em&gt; let you go if doing so increases its bottom line; that the CEO&apos;s mandate is to make just those kinds of decisions; and that you have to take control of your career lest you be shackled to an oar (his words). I don&apos;t disagree with any of these points. But I do disagree with the overall message and the moral of the story, as it were.&lt;/p&gt;
&lt;p&gt;The message should, in my opinion, be to choose your place of employment wisely. That blog post pretty much summed up the reasons why I generally dislike big, soulless corporations. But there are other options out there! I currently work at a ~100 person consulting company. Previously, I was at a &amp;lt;10 person consulting company. And before that I was at university, but during that time I did briefly work at a fairly large insurance company. I got a taste of the large company ethos then, and it was enough to scare me away.&lt;/p&gt;
&lt;p&gt;Smaller companies are different. They place more value in the individual. There&apos;s little, if any, company politics and bureaucracy. And I think, for the most part, people are happier there. If I were to look at things more cynically, I could say that smaller companies have more incentive to keep employees happy because the loss of one employee at a 50-person company is more painful than the loss of one employee at a 50,000-person company. But I choose not to look at it that way.&lt;/p&gt;
&lt;p&gt;I think the reason I felt such a strong need to respond to the blog post in question is that it hits home on some level. I believe that if I were in that author&apos;s shoes, having gone through his experiences, I could be just as jaded as he is. In a sense, I&apos;m lucky to have had the chance to see that environment early on and seek opportunities elsewhere. People who know me have seen how cynical I can be about certain topics. But the topic of company loyalty is an exception. I truly believe that it is possible to find a company that isn&apos;t soulless, that does in fact care about its employees, and that can even deserve loyalty.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Farewell, Windows Phone</title>
        <link>http://arktronic.com/weblog/2013-03-29/farewell-windows-phone/</link>
        <pubDate>Fri, 29 Mar 2013 20:46:20 +0000</pubDate>
        <guid isPermaLink="false">ID node/35 on http://arktronic.com</guid>
        <description>&lt;p&gt;It was a gray afternoon, the kind that makes people long for rain just to break up the dreary monotony of the sky. I was nursing a scotch, which was by this point in time far too diluted for my taste, as the ice had long melted. As I lit my fifth cigarette I heard a dull thump outside. Must be the paper. Late as usual.&lt;/p&gt;
&lt;p&gt;The winter had been strangely warm this year. Not warm enough to enjoy going outside, but not so cold as to let snow lay on the cracked pavement for any meaningful amount of time. I shivered as I opened the creaking door, and the chilly air let itself wash over me. Bringing the Gazette inside I noticed the subtle smell of damp paper and ink. The Daily Gazette was the only halfway respectable newspaper in town so everybody had a subscription, and the publisher&apos;s executives didn&apos;t feel any particular need to enforce timeliness of deliveries or proper moisture protection.&lt;/p&gt;
&lt;p&gt;As I poured myself a fresh drink, I noticed out of the corner of my eye a familiar physiognomy. Could it be? After all this time?&lt;/p&gt;
&lt;p&gt;Intrigued, I put down the crystal glass, half filled with that nourishing amber liquor, and walked over to the pile of newspapers that was once known as my couch. Unfolding the newspaper with ever so slight trepidation, I froze. It was her.&lt;/p&gt;
&lt;p&gt;The year was 2004, and I was young and carefree. My interests at the time included mobile phones and software development. And she was at the center. Back then she was known as Windows Mobile 2003 SE. A long name, full of history. We hit it off immediately, and over time our love flourished. Having a troubled past, she had changed her name more than once. We pulled through those tough times of her becoming Windows Mobile 5, 6, and 6.5. During those years, she had adopted quite a few aliases, and I was intimately familiar with some more than others. Audiovox SMT5600/HTC Typhoon, HTC TyTN/HTC Hermes, and HTC Touch Pro/HTC Raphael/AT&amp;amp;T Fuze were my favorites.&lt;/p&gt;
&lt;p&gt;But one day, something happened. She had been away for a while on yet another job, and we lost touch. When we finally reunited, she was... different. She called herself Windows Phone 7 and seemed to rely less on her aliases. Previously when she adopted new identities, she was still the same person underneath the makeup, the same one I fell in love with. Not this time. Her hair color and style were changed. She had adopted a new, unfamiliar accent. Even her skin tone seemed somehow different. But more than that, she was distant.&lt;/p&gt;
&lt;p&gt;In the past we had shared everything with each other, the good and the bad. I could truly say that I knew her, and she trusted me enough to let me into her world - all of her documented and undocumented APIs were mine to explore. That trust was now gone. It was as if she had experienced something so terrible in her time away that she could no longer trust anyone at all. Except that wasn&apos;t entirely true. There were certain people who called themselves carriers and OEMs. She knew them before, but now she seemed to be perpetually closer to them than to me.&lt;/p&gt;
&lt;p&gt;Still, we made it work. Writing software for her was still the best mobile development experience in the world. I came to truly enjoy and even be inspired by her new focus on overall user experience, something I had intuitively done before, but never really focused on. And occasionally, she would still let me use some of her APIs that she normally kept to herself, which was affectionately called &amp;quot;jailbreaking&amp;quot;.&lt;/p&gt;
&lt;p&gt;Unfortunately, even that limited, secret trust went away as she started calling herself Windows Phone 8. Her most impressive alias, Nokia Lumia 920, was a work of art indeed. Beautiful, with the world&apos;s first optical image stabilization camera in a phone. I&apos;ve been with the 920 since she introduced this alias at the BUILD conference. She made me feel special, privileged to be the first to see her in this form. But as she refined her personality with new firmware updates for others, she held them back from me. So naturally, I had mixed feelings of relief, anticipation, and wonder as I saw her in the Gazette with the headline &amp;quot;Portico update finally available to Lumia 920s from BUILD&amp;quot;. It was a message for me. She still wanted us to be together. It felt like our love was renewed once again.&lt;/p&gt;
&lt;p&gt;Alas, it was not meant to last. I found out something about her update - something that, by itself, may have been all right, but combined with all her other changes, was just too much for me. She had betrayed my trust. She changed herself dynamically based on the SIM card that she thought she used. I could no longer use Wi-Fi tethering, as she thought my MVNO-provisioned SIM card was that of AT&amp;amp;T, and she began asking AT&amp;amp;T for permission to let me tether. I thought it was a fluke. I contacted the OEM that provided her Lumia 920 alias to ask about this, but they confirmed the worst. This was part of the Portico update.&lt;/p&gt;
&lt;p&gt;This was the proverbial straw that broke the camel&apos;s back. I didn&apos;t want to admit it publicly, but I missed terribly her openness from the old Windows Mobile days. This final blow of disabling tethering, especially when she didn&apos;t even realize the true origin of the SIM card, had shattered my hopes of getting her to open up to me again. It was finally time to move on to someone who would not be so closed to me. A girl who&apos;d been eyeing me for some time now. A girl named Android.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NOTE: The preceding was a (rather obvious) dramatization of more or less accurate events. Except, I don&apos;t smoke. Or drink scotch. Or subscribe to anything called the Daily Gazette. And I don&apos;t usually anthropomorphize my devices. However, I am indeed switching to Android. For added effect, &lt;a href=&quot;http://youtu.be/x_IImfpAIcI&quot;&gt;play this&lt;/a&gt; as background music.&lt;/strong&gt;&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Spend time on your UX</title>
        <link>http://arktronic.com/weblog/2012-12-12/spend-time-on-your-ux/</link>
        <pubDate>Wed, 12 Dec 2012 21:39:27 +0000</pubDate>
        <guid isPermaLink="false">ID node/33 on http://arktronic.com</guid>
        <description>&lt;p&gt;You&apos;re working on a killer new app. Or a small niche website. Or really any kind of human-facing software. If you&apos;re like most developers, your primary focus is on functionality: before anything else, it has to &lt;em&gt;work&lt;/em&gt;. How can anybody disagree with that? If it doesn&apos;t work, then what&apos;s the point? Functionality is indeed paramount to success (even for ridiculous outliers like &lt;a href=&quot;http://www.forbes.com/sites/limyunghui/2012/11/21/color-app-tragedy-how-to-become-mice-nuts/&quot;&gt;Color&lt;/a&gt;), but I would argue that it is equally important to develop a great user experience (UX) before releasing the first public version. &amp;quot;Release early, release often&amp;quot; is a great model in many circumstances, but it should never serve as an excuse for poor quality - the &amp;quot;I&apos;ll just fix it in the next point release&amp;quot; mentality is very dangerous when overapplied.&lt;/p&gt;
&lt;p&gt;Let&apos;s step back a bit. Why do I think that a great UX is just as important as functionality? The single word answer is &lt;em&gt;perception&lt;/em&gt;. I&apos;m working under the assumption that a major goal of whatever it is you&apos;re making is that your users actually like it. Unfortunately, making users like your software is not a simple and clear-cut task. A great user experience is required - yes, required - for general likability. Take, for example, Windows Vista. For the most part, it has been disliked. The reasons for this are numerous, but functionality is not among those reasons, as far as I know. I remember Vista being somewhat slow, severely lacking in third-party drivers, and extremely annoying due to the introduction of User Account Control (UAC). Why was it slow? Probably because a lot of features were added to it since XP and turned on by default. Why weren&apos;t there many third-party drivers? Because Vista was the first widely used 64-bit Microsoft client operating system, which meant that a lot of drivers simply didn&apos;t exist, as they all had to be recompiled and signed. At the same time, Microsoft improved the Windows driver architecture, causing incompatibility for certain classes of drivers, and manufacturers were slow to respond. And, of course, UAC popping up its confirmation dialog for every minor settings change and every install was technically more secure, but obviously flawed. Windows Vista was technically a lot more functional and secure than Windows XP, but because of its bad UX, it is now remembered as a failure.&lt;/p&gt;
&lt;p&gt;But here is the really important thing. By the time Vista SP2 came out, all of the initial issues were pretty much fixed or had widely known workarounds, and yet the negative &lt;em&gt;perception&lt;/em&gt; of Vista remained. Windows Vista&apos;s original bad UX forever tarnished its image.&lt;/p&gt;
&lt;p&gt;The moral, which should be pretty obvious by now, is that first impressions really do matter, and when somebody finds your incredibly functional yet barely usable public 1.0 release, that person just might give up on your software altogether and put it on a mental blacklist. Cutting features is reasonable. Cutting UX is not. (Of course, there are always exceptions to the rule. I&apos;m not saying that this is absolutely the only way to go. I just want everyone to think hard about the choices they&apos;re making instead of simply going with the flow.)&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Digital trust issues</title>
        <link>http://arktronic.com/weblog/2012-10-04/digital-trust-issues/</link>
        <pubDate>Fri, 05 Oct 2012 02:12:24 +0000</pubDate>
        <guid isPermaLink="false">ID node/31 on http://arktronic.com</guid>
        <description>&lt;p&gt;I will freely admit that I have digital trust issues. In fact, I will go so far as to say that if you don&apos;t have these issues as well, then you are either not in the software engineering field, or you are being willfully ignorant. Allow me to explain my terminology and position.&lt;/p&gt;
&lt;p&gt;Many incredible computing advances are being made every day. The latest piece of &amp;quot;that is so freaking cool&amp;quot; news is &lt;a href=&quot;http://news.yahoo.com/california-governor-signs-driverless-cars-bill-225332278--finance.html&quot;&gt;autonomous cars&lt;/a&gt;. Naturally, California is at the forefront of this emerging field, with Google as the star of the show. I&apos;m very excited, as a geek, to see this technology advance to the point of being truly usable and useful. But personally, I&apos;m terrified of these things becoming popular. The main reason for that is, I know what kind of people wrote the software that runs those cars: software engineers. And I have a hard time trusting their code. The sad truth is that most software engineers out there in the world are bad at their jobs. Some are too lazy; some are unable to solve logical problems that they face every day; some are just not passionate about what they do. That&apos;s right, even that last one is a big problem: if when a good software engineer finds a problem with code tangential to their current task, they will either fix it or at the very least note/report it. This is how overall quality of software improves on a day to day basis. However, software engineers who are not passionate about what they do will just ignore the problem they noticed and assume that someone else will find it and take care of it. And that is how bugs creep in.&lt;/p&gt;
&lt;p&gt;I suppose my digital trust issues are a reflection of my corporate trust issues, that big corporations tend to look for ways to make (and save) as much money as possible in the short term. This is why they tend to outsource development to the lowest bidders and, ultimately, end up regretting those decisions when software comes back half-baked, deadlines are missed, and their clients/customers are unhappy. Incidentally, this is why I am very picky about my employers - I will not work for such &amp;quot;lowest bidder&amp;quot; shops.&lt;/p&gt;
&lt;p&gt;There are other companies that have their own in-house software development teams. Unfortunately, unless these companies are technically oriented and relatively small in size, they will tend to hire the bad software engineers. An in-depth discussion of the reasons for this trend is out of scope of this post. It is sufficient to say that HR departments are usually not trained to detect intricacies of the software engineering mindset to weed out the bad from the good; really good software engineers tend to stay away from jobs they consider to be boring (even if they are very important); and sometimes the bad software engineers are actually trained to look appealing to unsuspecting companies (résumé keywords and such).&lt;/p&gt;
&lt;p&gt;So, here is the logic behind my digital trust issues. There are good software engineers and bad software engineers. Big corporations tend to, in one way or another, use bad software engineers to write their code. This code ends up on production systems, from corporate portals to online banking websites to &lt;a href=&quot;http://en.wikipedia.org/wiki/SCADA&quot;&gt;SCADA&lt;/a&gt; systems. When such systems become popular (or critical) enough, they start attracting hackers. The worse the code on these systems, the easier it is for hackers to exploit it. And I don&apos;t even want to think about what can happen if an autonomous car is hacked. The possibility of remotely hacking future &amp;quot;connected&amp;quot; or networked cars is scarier yet. You get the idea. And if you don&apos;t, watch some &lt;em&gt;Ghost in the Shell&lt;/em&gt;. It paints a pretty realistic picture of a future world of connected machines and connected humans - and the scary things that hackers could potentially do in such a world.&lt;/p&gt;
&lt;p&gt;Along the same lines, this is also why I&apos;ve yet to enable auto-pay on any of my bills. Giving multiple companies my banking information to store for use every month to automatically withdraw funds sounds like a recipe for disaster. If even one of those companies has its data compromised, then there is suddenly a very real possibility of my bank account being emptied. You can usually escape liability for fraudulent credit card transactions, but it&apos;s not that painless with checking and savings accounts. Okay, so another reason why I don&apos;t enable auto-pay is so that I actually look at my bills to see if there are any discrepancies; otherwise, I just wouldn&apos;t bother looking at them at all. But the point stands - the code running all these systems is of unknown quality.&lt;/p&gt;
&lt;p&gt;What can be done to improve this situation? Honestly, I don&apos;t know. There are automated systems like McAfee Secure that scan for vulnerabilities remotely and then display a &lt;a href=&quot;http://en.wikipedia.org/wiki/Trust_seal&quot;&gt;trust seal&lt;/a&gt; on their clients&apos; websites to let end-users know that everything is okay. Of course, such systems can only detect very basic issues, and only to a limited extent. Poking randomly at the public endpoints of systems can only yield so much information. In order to truly be sure of code quality to a reasonable extent, you have to actually look at said code. But what company is going to let random people look at their source code? I suppose one option to verify code quality would be to bring in a trusted and unbiased third party that specializes in source code analysis. But I don&apos;t know of any such entities, and I doubt that many companies would hire them if there&apos;s the possibility of those companies&apos; code being publicly labeled as insecure or otherwise bad.&lt;/p&gt;
&lt;p&gt;This is an interesting situation, and I don&apos;t have any amazing revolutionary ideas to improve it. But until something radical happens, my digital trust issues will not go away.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Authentication, part two: are your users' passwords secure?</title>
        <link>http://arktronic.com/weblog/2012-09-20/authentication-part-two-are-your-users-passwords-secure/</link>
        <pubDate>Fri, 21 Sep 2012 03:26:23 +0000</pubDate>
        <guid isPermaLink="false">ID node/30 on http://arktronic.com</guid>
        <description>&lt;p&gt;In &lt;em&gt;Authentication, part one&lt;/em&gt; I discussed the pros and cons of single sign-on, and if you&apos;ve decided to use an SSO solution, that&apos;s great. However, if SSO doesn&apos;t fit your requirements, then you&apos;ll need to take care of storing your users&apos; passwords. The first thing you should do when determining how to store passwords is apply the &lt;a href=&quot;http://en.wikipedia.org/wiki/You_ain%27t_gonna_need_it&quot;&gt;YAGNI principle&lt;/a&gt;. In other words, avoid overengineering. If your website or app is only going to be used internally at your company, behind a firewall, by four people, then you really don&apos;t need a very secure password system. You might even be able to get away with storing those passwords as plaintext. Of course, that won&apos;t do if your website or app is exposed to the Internet or if it may be used by a significant number of people.&lt;/p&gt;
&lt;p&gt;Before delving into password storage security, I&apos;d like to touch on a related topic - password complexity requirements. Many people are familiar with silly corporate password policies of &amp;quot;minimum 17 letters, 11 numbers, two special characters, three spaces, and a Greek god&amp;quot;. If you&apos;re going down the path of dictating strict requirements, then you&apos;re better off requiring passphrases instead of forcing very specific minimal character type counts on your users. Passphrases may take slightly longer to type, but they are much easier to remember than complex nonsensical passwords and, consequently, much less likely to be written down on a sticky note. Alternately, you can require a minimum password complexity based on password entropy calculations, as long as you also check for dictionary words and adjust the complexity ratings accordingly.&lt;/p&gt;
&lt;p&gt;So, what&apos;s a good, secure way to store users&apos; passwords? It helps to think about this in &amp;quot;levels&amp;quot; of security:&lt;/p&gt;
&lt;p&gt;Level 0 - Plaintext - Horrible.&lt;br /&gt;
Level 1 - Reversible encryption (AES, etc.) - There&apos;s no good reason to do this.&lt;br /&gt;
Level 2 - Basic hashing (MD5, SHA-256) - This is not considered secure anymore. Stop it.&lt;br /&gt;
Level 3 - Hashing and salting - Pretty standard today, but not great.&lt;br /&gt;
Level 4 - Hashing and salting using modern PBKDFs - This is best.&lt;/p&gt;
&lt;p&gt;Let&apos;s go through each level. As I explained earlier, Level 0 should only ever be used on tiny, insignificant internal projects because it is entirely insecure.&lt;/p&gt;
&lt;p&gt;Level 1 is marginally better because the password isn&apos;t stored as plaintext, but it is not a good idea. Usually developers do this because they want to be able to recover user passwords. However, it is generally accepted that such functionality introduces unacceptable risk. If the set of encrypted passwords is stolen by an attacker, then figuring out the encryption key will enable said attacker to decrypt all of the passwords at once.&lt;/p&gt;
&lt;p&gt;Level 2 prevents decryption of passwords by using one-way encryption, a.k.a. hashing. When the user authenticates, you use the same hashing algorithm on their entered password and simply check if the hashes match. Unfortunately, this is still not secure. People have created what&apos;s known as &amp;quot;rainbow tables&amp;quot;, or large lists of hashes and their corresponding plaintext values. It becomes trivial to run the hashes through rainbow tables and get many, if not most, passwords back as plaintext.&lt;/p&gt;
&lt;p&gt;Level 3 makes the use of rainbow tables infeasible by appending or prepending random characters to user passwords. Those random characters are called a salt. You store the salt along with the hash resulting from the concatenation of the user&apos;s password and the salt. When the user authenticates, you perform the same concatenation of the stored salt to the entered password and then check if the hashes match. Because (good) salts are long and random, rainbow tables are highly unlikely to have any plaintext that happens to match the concatenation of the salt and the actual password. This can be slightly enhanced by using two salts - one randomly generated salt stored with the hash, and one static salt stored in the filesystem. That way, if the database is compromised, one of the salts is still hidden from the attacker. However, this is still not a great way to store passwords. Due to advances in &lt;a href=&quot;http://en.wikipedia.org/wiki/GPGPU&quot;&gt;GPGPU&lt;/a&gt; technology, it is becoming increasingly easy to brute-force even salted passwords. And if more than one password is discovered via brute-force, the second salt is easily identified. The reason that the various hashing techniques in this level (and the previous one) are problematic is that hashing functions were designed to be efficient and fast. This is bad when you&apos;re trying to obstruct brute-force attacks. But fear not, as there is a solution to this problem.&lt;/p&gt;
&lt;p&gt;Level 4 is considered good by today&apos;s standards. Password-based key derivation functions (PBKDFs) such as &lt;a href=&quot;http://en.wikipedia.org/wiki/PBKDF2&quot;&gt;PBKDF2&lt;/a&gt;, &lt;a href=&quot;http://en.wikipedia.org/wiki/Bcrypt&quot;&gt;bcrypt&lt;/a&gt;, and &lt;a href=&quot;http://www.tarsnap.com/scrypt.html&quot;&gt;scrypt&lt;/a&gt; deliberately slow down the generation of a hash (or key) in a configurable manner. PBKDF2 and bcrypt allow you to specify the CPU cost of hash generation, while scrypt allows you to specify the CPU cost, memory cost, and parallelization factor. Configured properly, these functions introduce a negligible slowdown in the user authentication process, while at the same time thwarting brute-force attacks by making each hashing attempt take a slightly longer time (and optionally use up more memory). Since brute-forcing normally requires millions upon millions of hashing attempts, even a slight delay per attempt makes the process virtually impossible. And, when hardware becomes faster and memory cheaper, the configuration can be easily altered to require more CPU and more memory per attempt. Existing passwords could then be silently migrated over to the new configuration by re-hashing them upon successful user login.&lt;/p&gt;
&lt;p&gt;So there you have it. If you want to securely store your users&apos; passwords, you know what to do. I was curious about modern methods of secure password storage, so I researched the topic a bit, and ended up with this blog post. In the process, I also implemented &lt;a href=&quot;https://bitbucket.org/Arktronic/keyderivation&quot;&gt;PBKDF2 and scrypt&lt;/a&gt; (the latter relies on the former) in .NET. Feel free to use that code and to contribute!&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Authentication, part one: what choices are out there?</title>
        <link>http://arktronic.com/weblog/2012-09-18/authentication-part-one-what-choices-are-out-there/</link>
        <pubDate>Tue, 18 Sep 2012 04:15:59 +0000</pubDate>
        <guid isPermaLink="false">ID node/29 on http://arktronic.com</guid>
        <description>&lt;p&gt;Authentication is something that virtually every developer these days has dealt with at least once - and sometimes has purposely avoided increasing that particular counter. Security in general is hard to get right, and authentication is, arguably, at its heart. There is precious little out there today that has no need of authentication, so one would think that (1) by now there would be excellent, vetted, and widely-used authentication systems that can be plugged in to any app, and that (2) everyone uses such systems. Unfortunately, neither of those is actually true. While single sign-on (SSO) systems do exist, they are hardly universal or ubiquitous. There are the big OAuth providers like Twitter and Facebook, whose authorization services are often used by social startups for the purpose of authentication. There is the ability to use a Google account (and countless other OpenID endpoints) strictly for authentication purposes. And, of course, there&apos;s Microsoft&apos;s Passport, er, Live ID, er, Microsoft account.&lt;/p&gt;
&lt;p&gt;There are two big issues with these systems. The first one is trust, and it is a multifaceted issue. Do your users trust the system you&apos;ve chosen? Do you trust that system itself to be secure and keep your users&apos; authentication information confidential? Do you trust your users to utilize that system in a secure manner? These are all important questions that must be answered &amp;quot;yes&amp;quot; in order to even begin considering SSO. The first trust question tends to be the simplest. Unless a significant number of your users is computer-savvy, user trust can be considered implicit. Only if there exists a very widely publicized reason not to trust a system would there be resistance from &amp;quot;typical&amp;quot; people. The second trust question is a bit tougher and requires some research into the chosen system. Twitter appears to be taking reasonably good care of their users&apos; account security, and the same can be said of Google. However, if you choose to implement OpenID, then there are absolutely no guarantees as to how secure any of the other endpoints out there may be. And if a user&apos;s account is compromised on your website/app because their OpenID endpoint was compromised, chances are the user will still blame you. Finally, the third trust question is more open-ended and may not have a clear answer. If the system you chose potentially allows users to create three-letter passwords, for example, then maybe you wouldn&apos;t trust your users to utilize that system appropriately. Again, all of these questions must be carefully considered when determining whether to use a single sign-on system.&lt;/p&gt;
&lt;p&gt;The second big issue with SSO is that users must already have accounts with the SSO provider. This is generally a good thing for internal corporate applications and a bad thing everywhere else. Unless your website or app&apos;s primary purpose revolves around a system that can be used as an SSO provider (e.g., a Twitter client), then you have no reason to require that a user sign up for some unrelated service. That extra signup step becomes annoying and unnecessary, and users will not respond positively to it.&lt;/p&gt;
&lt;p&gt;Of course, there are other potential reasons not to go with SSO. Sometimes custom authentication is required for extra-paranoid security. Sometimes there are exotic requirements, such as cached offline login, that generally cannot be met by SSO systems. But if your website or app has no such issues, and if trust is not a problem and an existing user account is virtually guaranteed, then by all means go with an SSO system. It&apos;s much easier to focus on your product&apos;s core functionality when you don&apos;t have to worry about authentication. (Okay, you still have to worry about it, but only to the extent of implementing the chosen SSO system&apos;s protocol correctly.)&lt;/p&gt;
&lt;p&gt;The next post will tackle issues of handling authentication yourself without SSO.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>A good Windows console environment</title>
        <link>http://arktronic.com/weblog/2012-09-08/good-windows-console-environment/</link>
        <pubDate>Sun, 09 Sep 2012 00:39:20 +0000</pubDate>
        <guid isPermaLink="false">ID node/28 on http://arktronic.com</guid>
        <description>&lt;p&gt;Sometimes I really miss the Linux shell on my Windows computers. I rely on too much Windows-specific stuff to actually switch to Linux for my everyday computing activities - not to mention Windows Phone development - but I do want more power from my commandline than cmd.exe provides by itself. I&apos;ve customized my console experience pretty heavily on my Windows 8 laptop, and I&apos;m pretty happy with it now. This post is both to help others do the same if they so wish, and to help me remember what I did for next time.&lt;/p&gt;
&lt;p&gt;I&apos;ve made some, shall we say, odd choices in this configuration, and I&apos;ll explain why along the way.&lt;/p&gt;
&lt;p&gt;First of all, here&apos;s all that needs to be installed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Cygwin - because Bash is quite a bit more powerful than cmd.exe, and standalone MinGW-based versions are really outdated&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;bash - should be installed by default anyway&lt;/li&gt;
&lt;li&gt;openssh - because I prefer it over PuTTY/Plink for connecting to git and hg&lt;/li&gt;
&lt;li&gt;screen - this is used as a workaround for a strange Cygwin bug; I&apos;ll explain later&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ConEmu - a very powerful open source console wrapper&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Python 2.7 - optional; for hg integration
There are primarily two reasons for choosing ConEmu. Aside from the obvious FOSS stuff, ConEmu (1) can resize the console horizonally, unlike cmd.exe, and (2) has excellent power-user features, such as customizable &amp;quot;Shell here&amp;quot;-type Explorer integration and a nice tabbed interface. MinTTY, which comes with Cygwin, is unlikely to get a tabbed interface and isn&apos;t very customizable. Like its name implies, it&apos;s minimal.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For simplicity&apos;s sake, I installed Cygwin in C:\Cygwin. Everything else was installed in default locations.&lt;/p&gt;
&lt;p&gt;By default, ConEmu&apos;s installer enables the &amp;quot;Inject ConEmuHk&amp;quot; option for better compatibility. I found that it&apos;s pretty slow, so I disabled it.&lt;/p&gt;
&lt;p&gt;After the above apps have been installed, comes the configuration.&lt;/p&gt;
&lt;p&gt;In order to somewhat streamline the ConEmu configuration, I created a regular batch file that ConEmu launches both for a regular shell and for a &amp;quot;start in this directory&amp;quot; style shell. For the latter, I also created a shell script that takes care of going to the correct path when a file is selected instead of a directory. Here they are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C:\Cygwin\constart.bat&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@echo off  
C:\Cygwin\bin\screen.exe C:\Cygwin\bin\bash.exe -l -i %*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;C:\Cygwin\conhere.sh&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/sh  
if [ -d &amp;quot;$1&amp;quot; ]; then  
    cd &amp;quot;$1&amp;quot;  
else  
    cd &amp;quot;`dirname \&amp;quot;$1\&amp;quot;`&amp;quot;  
fi  
bash  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That %* parameter in the batch file passes all commandline arguments given to the batch file onto Bash.&lt;/p&gt;
&lt;p&gt;The reason I launch screen instead of launching Bash directly is a bit complicated. It&apos;s done in order to avoid a strange issue that causes Ctrl-C to not function when executing native Windows apps, such as ping. There are mailing list emails (&lt;a href=&quot;http://cygwin.com/ml/cygwin/2012-03/msg00105.html&quot;&gt;one&lt;/a&gt;, &lt;a href=&quot;http://cygwin.com/ml/cygwin/2012-03/msg00142.html&quot;&gt;two&lt;/a&gt;) that somewhat explain this bug and claim that it&apos;s fixed, respectively. However, it&apos;s not fixed for me. So, in order to use /dev/pty# instead of /dev/cons# through a regular shell, I&apos;m cheating and launching it in screen, which is a virtual console. If there&apos;s a better way to do this, I&apos;d really like to know.&lt;/p&gt;
&lt;p&gt;ConEmu&apos;s startup commandline in its settings screen, under Startup, is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;C:\Cygwin\constart.bat -new_console:an
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The -new_console parameter is interpreted by ConEmu itself; see its &lt;a href=&quot;http://code.google.com/p/conemu-maximus5/wiki/NewConsole&quot;&gt;documentation&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;Under Features/Integration, I configured the &amp;quot;ConEmu Here&amp;quot; context menu integration to the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/single /cmd c:\cygwin\constart.bat -c &amp;quot;/conhere.sh \&amp;quot;`cygpath -u &apos;%L&apos;`\&amp;quot;&amp;quot; -new_console:an
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Yes, that is one confusing command with lots of varying quotes. If there&apos;s an easier one, please tell me. Essentially, it launches Bash and makes it go to the current directory, signified by &apos;%L&apos;. It&apos;s in single quotes in order to handle spaces in directory names. The cygpath command is used to translate the Windows directory name to the Cygwin equivalent. The escaped double quotes surrounding that command are also for handling spaces in directory names. Finally, Bash is launched in interactive mode on the last line of conhere.sh instead of just quitting after changing directories.&lt;/p&gt;
&lt;p&gt;Past all that, everything comes down to personal choices of Bash customization with functions, aliases, and so forth.&lt;/p&gt;
&lt;p&gt;I disabled screen&apos;s Ctrl-A hook by adding the line &amp;quot;bind ^a&amp;quot; to /etc/screenrc. That turns screen into, for all intents and purposes, a regular virtual console.&lt;/p&gt;
&lt;p&gt;Since nano has strange cursor scrolling behavior in Cygwin, I&apos;m using &lt;a href=&quot;http://ne.dsi.unimi.it/&quot;&gt;ne&lt;/a&gt; instead. It&apos;s nice, and it comes with a binary specifically compiled for Cygwin.&lt;/p&gt;
&lt;p&gt;For git and hg integration, I created functions in the global bashrc file for changing the $PS1 variable:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Enable Hg integration  
__hg_ps1() {  
    hg prompt &amp;quot;{ on {branch}}{ at {bookmark}}{status}&amp;quot; 2&amp;gt; /dev/null  
}  
# Enable Git integration  
source /cygdrive/c/Program\ Files\ \(x86\)/Git/etc/git-completion.bash  
ps1def() {  
    export PS1=&apos;\[\e]0;\w\a\]\n\[\e[32m\]\u@\h \[\e[33m\]\w\[\e[0m\]\n\$ &apos;  
}  
ps1hg() {  
    export PS1=&apos;\[\e]0;\w\a\]\n\[\e[32m\]\u@\h \[\e[33m\]\w\[\e[36m\]$(__hg_ps1)\[\e[0m\]\n\$ &apos;  
}  
ps1git() {  
    export PS1=&apos;\[\e]0;\w\a\]\n\[\e[32m\]\u@\h \[\e[33m\]\w\[\e[36m\]$(__git_ps1)\[\e[0m\]\n\$ &apos;  
}  
  
# Set a default prompt of: user@host current_directory {hg/git/none}  
ps1hg
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The hg integration requires installation of &lt;a href=&quot;http://sjl.bitbucket.org/hg-prompt/&quot;&gt;hg-prompt&lt;/a&gt; and Python to run it.&lt;/p&gt;
&lt;p&gt;Finally, I added some convenient aliases:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;alias ls=&apos;ls --color=auto&apos;  
alias la=&apos;ls -lha&apos;  
alias 2w=&apos;cygpath -w&apos;  
alias 2u=&apos;cygpath -au&apos;  
alias nano=&apos;echo &amp;quot;Using ne instead of nano!&amp;quot;; sleep 2s; ne&apos;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That&apos;s about it. I might be missing a few things, but for the most part, this is a very usable console environment that&apos;s better than the default Windows one. If you have any suggestions for improvement, I gladly welcome them.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>How to do a clean install of Windows 7 or 8 on Samsung Chronos laptops</title>
        <link>http://arktronic.com/weblog/2012-08-05/how-do-clean-install-windows-7-or-8-samsung-chronos-laptops/</link>
        <pubDate>Mon, 06 Aug 2012 01:04:26 +0000</pubDate>
        <guid isPermaLink="false">ID node/27 on http://arktronic.com</guid>
        <description>&lt;p&gt;First, some background. I&apos;ve got a new Samsung Series 7 NP700Z3A-S06US notebook (that name just rolls off the tongue, doesn&apos;t it?) and the first thing I always do when I get a new computer that I didn&apos;t build myself is wipe everything off the hard drive(s) and install the operating system from scratch. This applies even to the &amp;quot;Microsoft Signature&amp;quot; computers, which are supposed to be bloatware-free, but still contain too much unnecessary stuff for my taste. So, when I looked over the installed software on this machine, I decided that I might as well do my usual thing and wipe it. That&apos;s where trouble struck.&lt;/p&gt;
&lt;p&gt;This particular notebook computer, as well as similar Samsung models, contains an 8GB SSD (in addition to the 1TB HDD) that is used as a cache to speed up Windows and apps. Samsung uses Diskeeper&apos;s ExpressCache software for that purpose. Unfortunately, this SSD causes a rather large problem for the Windows installer. For whatever reason, Windows refuses to install its little &amp;quot;System Reserved&amp;quot; partition on the HDD, downright refusing to proceed with the installation if the SSD is already partitioned for ExpressCache. Once I discovered this, the only choice I had at that point was to repartition the SSD and let Windows install itself. However, things weren&apos;t that simple. After the Windows installer rebooted, the notebook went into a boot loop. I&apos;ve never seen an x86-based computer do that before. I&apos;ve seen many a boot error message, but never a boot loop. It appears that the BIOS really doesn&apos;t want to boot off the SSD, which is where Windows decided to install its boot partition. I had to figure out where to go from there - how to get the HDD into a state where both Windows and Samsung&apos;s BIOS were happy, and the SSD free for ExpressCache use.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aside: I had some hardware-related trouble with this notebook, and in the process of trying to get it repaired discovered that removing or replacing the hard drive voids the warranty. I think that&apos;s idiotic. I had to wipe the HDD before sending the notebook in because it had sensitive data on it, instead of just removing the HDD.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;WARNING: The following steps involve dangerous commands that delete lots and lots of data. I&apos;m not responsible if you delete your precious memories. Only you are responsible.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are the approximate steps I took to get everything working again:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Boot off the Windows DVD or USB installation media&lt;/li&gt;
&lt;li&gt;Choose the &amp;quot;Repair&amp;quot; option, and the command prompt afterwards (the way to get there is different between Windows 7 and 8)&lt;/li&gt;
&lt;li&gt;Type in &lt;strong&gt;diskpart&lt;/strong&gt; to get into the partition tool&lt;/li&gt;
&lt;li&gt;Use the commands &lt;strong&gt;list disk&lt;/strong&gt; and &lt;strong&gt;list part&lt;/strong&gt; to determine which disk is what. For me, Disk 0 was the HDD and Disk 1 was the SSD. The following instructions assume this&lt;/li&gt;
&lt;li&gt;Select the HDD: &lt;strong&gt;sel disk 0&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Delete all partitions on it: &lt;strong&gt;clean&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create a 100MB partition for Windows 7 (change to 350MB for Windows 8): &lt;strong&gt;create part primary size=100&lt;/strong&gt; (or &lt;strong&gt;size=350&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;Format it: &lt;strong&gt;format fs=ntfs quick&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Assign it a letter: &lt;strong&gt;assign letter=f&lt;/strong&gt; (if F: is in use, pick another one. Use &lt;strong&gt;list vol&lt;/strong&gt; to see all volumes and their letters)&lt;/li&gt;
&lt;li&gt;Create a partition that fills the rest of the disk: &lt;strong&gt;create part primary&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Format it and assign it letter &amp;quot;C&amp;quot;, as above&lt;/li&gt;
&lt;li&gt;Reboot back into the Windows installation media and install Windows into the large partition that was just created&lt;/li&gt;
&lt;li&gt;Again, reboot into the Windows installation media and go into the repair command prompt&lt;/li&gt;
&lt;li&gt;Use diskpart&apos;s &lt;strong&gt;list vol&lt;/strong&gt; and &lt;strong&gt;assign&lt;/strong&gt; commands to ensure that both the boot partition (F:) and the Windows partition (C:) still have drive letters&lt;/li&gt;
&lt;li&gt;Select the boot partition (&lt;strong&gt;sel part 1&lt;/strong&gt;) and mark it as active: &lt;strong&gt;active&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Exit diskpart and type in: &lt;strong&gt;bcdboot c:\windows /s f:&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Reboot and you should be good to go!&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After booting into Windows, you should be able to safely repartition the SSD to ExpressCache&apos;s liking, without killing Windows.&lt;/p&gt;
&lt;p&gt;So who is to blame for this foolishness - Samsung or Microsoft? Yes. They are both to blame. Samsung should not have screwed up the system configuration to the point that using the standard Windows installer causes such problems, and Microsoft should not have made the installer so damned picky about which partitions must reside in which disks and in what order.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>How to properly get a Windows Phone app's assembly version</title>
        <link>http://arktronic.com/weblog/2012-05-16/how-properly-get-windows-phone-apps-assembly-version/</link>
        <pubDate>Wed, 16 May 2012 13:40:16 +0000</pubDate>
        <guid isPermaLink="false">ID node/26 on http://arktronic.com</guid>
        <description>&lt;p&gt;I&apos;ve been seeing code floating around that suggests that in order for you to get an assembly&apos;s version number in Windows Phone SDK 7.0/7.1 you have to call Assembly.ToString() or Assembly.FullName and then parse the output. &lt;strong&gt;Please don&apos;t do that.&lt;/strong&gt; There is a better, more stable, and more supported way to get the information you seek:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;new System.Reflection.AssemblyName(System.Reflection.Assembly.GetExecutingAssembly().FullName)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That will give you an AssemblyName object, which has not only the version number, but other information about your assembly as well.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>On Visual Studio 11's redesign awkwardness</title>
        <link>http://arktronic.com/weblog/2012-05-09/visual-studio-11s-redesign-awkwardness/</link>
        <pubDate>Wed, 09 May 2012 15:31:24 +0000</pubDate>
        <guid isPermaLink="false">ID node/25 on http://arktronic.com</guid>
        <description>&lt;p&gt;Most people by now are aware of the design changes from Visual Studio 2010 to VS11 Beta, and from &lt;a href=&quot;http://blogs.msdn.com/b/visualstudio/archive/2012/05/08/visual-studio-11-user-interface-updates-coming-in-rc.aspx&quot;&gt;Beta to RC&lt;/a&gt;. There were quite a few complaints about the Beta design, not the least of which included the lack of colors and the ALL CAPS tool window title bars and tabs. Now with the RC, the biggest complaint is that the ALL CAPS weren&apos;t removed completely, but were instead moved to the menus. So why is all of this going on? Why is Microsoft seemingly blind to what users are saying?&lt;/p&gt;
&lt;p&gt;I believe that the root cause of the redesign awkwardness that Visual Studio is experiencing is the Metro style. Don&apos;t get me wrong - I love Metro. It&apos;s crisp, clean, and beautiful. Unfortunately, at least to my knowledge, it has never been applied to something as complex as an IDE before. Most Metro-style apps I&apos;ve seen are, by comparison, extremely simple. They have nice, large buttons and lots of white space. They look great. And they tend to be information-centric (remember - &amp;quot;content, not chrome&amp;quot;). But they are not Visual Studio. Visual Studio is a large beast. More than that, it&apos;s got a wide range of functionality that needs to be exposed to its users, us developers, in order to be as useful as possible. It has an MDI, and lots of toolbars, status bars, tabs, and menus. That clashes with the simplicity of Metro. So what&apos;s Microsoft to do? On one hand, there are probably orders from above to make everything look Metro-style for consistency&apos;s sake. Makes sense. But on the other hand, Visual Studio must be good at its primary job - offering an awesome development experience - which means that complexity must be surfaced because, frankly, developers need it.&lt;/p&gt;
&lt;p&gt;So Microsoft ends up making strange design decisions to satisfy both requirements. Metro relies heavily on ALL CAPS? Let&apos;s throw them in somewhere. Metro focuses more on monochrome iconography than multi-colored images? Make everything black and white. What&apos;s the answer then? Should Metro be abandoned for a complex app like an IDE? Should designers take a hard look at both Visual Studio and the Metro guidelines and come up with a better vision for unifying them - a complete UX overhaul, perhaps? I don&apos;t know the answer. I just know that Metro should not be applied haphazardly.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>My phone, my rules</title>
        <link>http://arktronic.com/weblog/2012-04-14/my-phone-my-rules/</link>
        <pubDate>Sat, 14 Apr 2012 21:08:41 +0000</pubDate>
        <guid isPermaLink="false">ID node/23 on http://arktronic.com</guid>
        <description>&lt;p&gt;This week I got a Samsung Focus Flash. It&apos;s a nice upgrade from my first-generation LG Optimus 7. Although I have a Nokia Lumia 710, I can&apos;t actually use it because it still doesn&apos;t support tethering. While the Focus Flash does support tethering (or &amp;quot;internet sharing&amp;quot;), the functionality is tied to AT&amp;amp;T. I don&apos;t have AT&amp;amp;T; I&apos;m using an &lt;a href=&quot;http://en.wikipedia.org/wiki/Mobile_virtual_network_operator&quot;&gt;MVNO&lt;/a&gt; as my cellular provider. That means I simply can&apos;t activate tethering. Additionally, I can&apos;t update the OS to the latest released build, 8107 at time of writing, because AT&amp;amp;T refuses to update their current phones to anything beyond 7720 until Windows Phone codename &amp;quot;Tango&amp;quot; comes out. Well, that&apos;s not good enough for me.&lt;/p&gt;
&lt;p&gt;The phone&apos;s software is tied to AT&amp;amp;T, and I don&apos;t like that. I shouldn&apos;t have to be tied to AT&amp;amp;T&apos;s rules just because that&apos;s how the phone was initially configured. So I decided to fix these issues plaguing my phone. It turned out to be pretty easy. Here are the steps I took to enable tethering and updating.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I dev-unlocked the phone. Since I have an App Hub account, it was a piece of cake.&lt;/li&gt;
&lt;li&gt;I interop-unlocked the phone, following the Samsung guide in &lt;a href=&quot;http://forum.xda-developers.com/showthread.php?t=1271963&quot;&gt;this XDA thread&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;I installed the &lt;a href=&quot;http://www.wp7roottools.com/&quot;&gt;WP7 Root Tools&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;To enable tethering without asking for AT&amp;amp;T&apos;s permission, I used the Root Tools&apos; registry editor to make the following changes:
&lt;ol&gt;
&lt;li&gt;[HKLM\Comm\InternetSharing\Settings] OpenMarketEnabled=dword:1&lt;/li&gt;
&lt;li&gt;[HKLM\Comm\InternetSharing\Settings] EntitlementURI=&amp;quot;./Vendor/MSFT/Registry/HKLM/Comm/InternetSharing/Settings/OpenMarketEnabled&amp;quot; (without quotes)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;To enable OS updates without AT&amp;amp;T&apos;s software blocks, I made these registry changes:
&lt;ol&gt;
&lt;li&gt;[HKLM\System\Platform\DeviceTargetingInfo] MobileOperator=&amp;quot;000-88&amp;quot; (without quotes)&lt;/li&gt;
&lt;li&gt;[HKLM\System\Platform\DeviceTargetingInfo] MOName=&amp;quot;OPN&amp;quot; (without quotes)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Finally, to ensure the settings took hold, I rebooted the phone.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After applying the above changes, I could freely tether as well as update the phone to build 8107 through Zune. Please note that I am &lt;strong&gt;not&lt;/strong&gt; advocating following the above steps because changing registry values is extremely dangerous and can potentially brick your device. However, if you&apos;re like me, and you refuse to use an artificially crippled device, you could un-cripple it.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Why the mobile "write once, run everywhere" mentality is misguided</title>
        <link>http://arktronic.com/weblog/2012-01-26/why-mobile-write-once-run-everywhere-mentality-misguided/</link>
        <pubDate>Fri, 27 Jan 2012 02:22:32 +0000</pubDate>
        <guid isPermaLink="false">ID node/22 on http://arktronic.com</guid>
        <description>&lt;p&gt;Every so often I see articles and news blurbs about yet another product that allows people to create a mobile app once and automagically publish it on all of the major smartphone platforms. Recently, I&apos;ve seen lots of buzz around PhoneGap becoming fully-featured in regards to Windows Phone. And just today I saw an &lt;a href=&quot;http://developers.slashdot.org/story/12/01/26/1937214/yahoos-project-to-disrupt-mobile-publishing&quot;&gt;article on Slashdot&lt;/a&gt; about Yahoo! getting into this space. Although, as a developer and a techy, I love the idea of being able to write an app and quickly have it available on multiple platforms, I must say I do not approve of actually doing it.&lt;/p&gt;
&lt;p&gt;I have two simple reasons for my opinion, one minor and one major. They are, respectively, performance and user experience. Let&apos;s start with the minor one, performance. In order to write cross-platform code, virtually all of the current solutions require such code to be written in JavaScript. Simply put, that makes apps run slowly. JS engines are improving at an impressive rate, yes, but there is just too much overhead when using such tactics as opposed to running native (or as close to native as possible) code. In other words, less overhead yields faster code execution. However, most of the time, this point tends to be moot. Rich visuals, combined with ever-improving JavaScript execution speeds, tend to cancel out any user-detectible delay. Obviously, when apps have to do intensive processing on their own their performance will suffer, but most apps don&apos;t actually do that. They&apos;re either simple enough to not need to perform such tasks, or they offload processing to a much more powerful server somewhere in the cloud. And let&apos;s not forget Mono. While the Android and iOS versions certainly aren&apos;t cheap, they allow the exact same backend code to run on both of those platforms as well as Windows Phone.&lt;/p&gt;
&lt;p&gt;Now let&apos;s discuss the real issue. Arguably the most important aspect of a mobile app is the user experience. If the app is slow or unresponsive or crashes a lot, that will be a major detriment to a user&apos;s enjoyment of said app. Depending on the severity of the issues, a user&apos;s response could range from opening the app less frequently to actively avoiding opening the app unless really necessary to just uninstalling the damned app. Of course, all of that is likely to be combined with negative reviews.&lt;/p&gt;
&lt;p&gt;So why does user experience suffer when using &amp;quot;write once, run everywhere&amp;quot; tools? Performance, as discussed above, is certainly a factor. But another factor is how well the app meshes with the rest of the platform. I don&apos;t just mean taking advantage of the appropriate platform APIs, which is itself problematic when they are so disparate among the different platforms. I mean the look and feel of apps. That&apos;s right, the stuff that so many of us developers hate dealing with, the interface and user interaction. Take a look at how the majority of iPhone apps look. The common buttons, the common paradigms. Now look at Windows Phone apps. (Android UX is sort of all over the place, especially with the radical transition Ice Cream Sandwich brings, so it doesn&apos;t make for a good example.) How can an app appear beautiful on Windows Phone when it looks like an iPhone app? All that unnecessary chrome, the radically different tab interface instead of pivots, and of course the back button on the screen, all seriously detract from the overall app experience.&lt;/p&gt;
&lt;p&gt;But why do so many of these tools pop up? Because they sound like an incredibly attractive proposition. Lazy developers (and I do not use the term lazy in a negative light here) love it because they can concentrate on making more apps in less time. Companies with tight finances can afford to release apps without spending a fortune on development. These are valid reasons, but if you have the option to make proper native apps, you should absolutely make that choice. Your users will thank you.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Good app vs. great app: resilience</title>
        <link>http://arktronic.com/weblog/2011-08-02/good-app-vs-great-app-resilience/</link>
        <pubDate>Tue, 02 Aug 2011 04:57:36 +0000</pubDate>
        <guid isPermaLink="false">ID node/20 on http://arktronic.com</guid>
        <description>&lt;p&gt;I strongly believe that a distinguishing mark between a good app and a great app is resilience, or in other words, its ability to adapt to unusual conditions. Naturally, it&apos;s up to the app&apos;s architects and developers to make it resilient, but too often I see apps that break with the slightest change of an upstream API. This has been observed not only with small, relatively unknown apps, but also with some high profile ones, such as the official Facebook app for Windows Phone. Why is this behavior so prevalent? The way I see it, there are three separate causes, any one of which can create this problem: lack of experience, thoughtlessness, and (of course) laziness.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The causes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let&apos;s start with lack of experience. This happens more to young developers, obviously, because they haven&apos;t yet had projects suddenly start failing on them for no apparent reason. Seasoned developers are guaranteed to have experienced this, and quite often due to a third party screwing up the APIs they are accessing. Unfortunately, not much can be done to alleviate a lack of experience aside from, well, gaining some experience! The more clever developers will gain experience by watching others fail and learning from those mistakes. The average developers will gain experience by failing themselves and learning from that. The below average devs fall into the second cause, thoughtlessness.&lt;/p&gt;
&lt;p&gt;When people don&apos;t learn from their own mistakes, they are bound to repeat them. In the narrow situation that&apos;s being discussed here, I&apos;m going to call that thoughtlessness. When architects/developers should be aware of potential issues (considering their experience), and yet do nothing about them, it&apos;s thoughtless. In fact, I would go so far as to say that they are bad architects/developers. I would not want to work with them. Or look at their code. I do not know if anything can be done with such people to make them better. If there is, I haven&apos;t come across it. And that&apos;s sad.&lt;/p&gt;
&lt;p&gt;Now we come to laziness. Larry Wall famously wrote that laziness is one of three &lt;a href=&quot;http://en.wikipedia.org/wiki/Larry_Wall#Virtues_of_a_programmer&quot;&gt;great virtues of a programmer&lt;/a&gt;. However, when laziness isn&apos;t tempered with the other two, impatience and hubris, it quickly goes from virtue to vice. I have occasionally been guilty of this myself. Sometimes, when I really want to get an app functioning, I&apos;ll forgo good techniques in favor of quick turnaround. I (usually) force myself to fix particularly bad code later on. Many others do not. When people release apps that were lazily coded, those apps should be expected to easily break. Sometimes small utilities with a very limited scope are fine to release without much error handling. They just aren&apos;t that important. But a public app that is submitted to an app store should never fall into that category. If developers are taking the time to release their apps to the public, &lt;em&gt;especially&lt;/em&gt; if those apps aren&apos;t free, they must be resilient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;A real world counter-example&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I must confess, the catalyst for my writing this blog post is that one of my apps has proven to be resilient this past weekend, and I realized that many others would not have coded it to be so. ArkWords, my free dictionary/thesaurus/translator for Windows Phone, has a popular feature called Word of the Day. It&apos;s pretty self-explanatory. What&apos;s interesting about it, though, is that I wrote a web service that sends a Live Tile image with the current word as well as the day of the week that&apos;s associated with that word. My upstream provider, the awesome &lt;a href=&quot;http://www.wordnik.com&quot;&gt;Wordnik.com&lt;/a&gt;, has always released new WotD entries during weekdays, so the Live Tile for Friday would show &amp;quot;Fri/Sat/Sun&amp;quot; for the day because otherwise people would think something is wrong when it&apos;s Sunday and they&apos;re seeing the word from Friday. The app itself has a highlighted sentence at the top of the WotD definition stating, to which day(s) the current word applies. As you might have guessed, this weekend Wordnik, for whatever reason, decided to release new WotD entries both Saturday and Sunday. Both ArkWords and the web service handled this change perfectly. Obviously, &amp;quot;Fri/Sat/Sun&amp;quot; for Friday&apos;s word of the day wasn&apos;t entirely accurate anymore, but the Live Tile and the app displayed Saturday&apos;s and Sundays words of the day exactly as they should have, with proper day labels and everything.&lt;/p&gt;
&lt;p&gt;I think this is a prime example of app resilience. I&apos;m not trying to praise my own skills here. I only want people realize that it&apos;s important to make their apps resilient. Lots and lots of apps consume third party APIs. Developers and architects must be aware of the dangers associated with their use. How will &lt;em&gt;your&lt;/em&gt; app react to an upstream change?&lt;/p&gt;
</description>
        </item>

        <item>
        <title>A redone blog</title>
        <link>http://arktronic.com/weblog/2011-07-25/redone-blog/</link>
        <pubDate>Tue, 26 Jul 2011 01:26:26 +0000</pubDate>
        <guid isPermaLink="false">ID node/5 on http://arktronic.com</guid>
        <description>&lt;p&gt;I have recreated my arktronic.com blog in a new version of Drupal. The old one started having some issues that I don&apos;t care to fix, and recreating it was the easiest solution. Some of the old posts have been added back because I have deemed them useful in one way or another. Comments did not make it, however. I always intend to blog more when I make any kind of site change, but in the past that hasn&apos;t really worked out. We&apos;ll see what happens now.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Get your network IPv6 ready with ease</title>
        <link>http://arktronic.com/weblog/2011-06-07/get-your-network-ipv6-ready-ease/</link>
        <pubDate>Wed, 08 Jun 2011 04:51:30 +0000</pubDate>
        <guid isPermaLink="false">ID node/19 on http://arktronic.com</guid>
        <description>&lt;p&gt;June 8, 2011 is World IPv6 Day. If you don&apos;t know what that means (and you want to find out), then &lt;a href=&quot;http://www.worldipv6day.org/&quot;&gt;here&lt;/a&gt; &lt;a href=&quot;http://en.wikipedia.org/wiki/World_IPv6_Day&quot;&gt;are&lt;/a&gt; &lt;a href=&quot;http://www.zdnet.com/blog/networking/what-is-world-ipv6-day-and-why-it-matters/1148&quot;&gt;some&lt;/a&gt; &lt;a href=&quot;http://arstechnica.com/business/news/2011/06/google-yahoo-facebook-turn-on-ipv6-for-a-day-tomorrow.ars&quot;&gt;links&lt;/a&gt;. This post is targeted toward tech-savvy people who (1) haven&apos;t had a chance to IPv6-enable their networks, (2) want to do it, and (3) don&apos;t want it to be a big hassle. I am one such person, and I decided to forgo #3 in order to help others to do this.&lt;/p&gt;
&lt;p&gt;Chances are, your ISP doesn&apos;t give you native IPv6 addresses. In this case, you still have the ability to access IPv6 resources on the Internet. You just have to go through an intermediary. There are multiple methods and protocols to do this - Teredo, 6to4, and 6in4, to name a few. In this post I&apos;ll focus on the one I used, 6to4. The reason I used it is that it&apos;s very easy to set up as well as to test whether the method is available to you.&lt;/p&gt;
&lt;p&gt;Let&apos;s start with the requirements. First, you&apos;ll need a router that is compatible with the Tomato custom firmware. See the compatibility table &lt;a href=&quot;http://tomatousb.org/doc:build-types&quot;&gt;here&lt;/a&gt;. Since you&apos;re IPv6-enabling your entire network, this must be done at the router level. If your router doesn&apos;t support Tomato, then this guide will be of limited use to you. Second, the client computers on your network should be IPv6 ready. Modern operating systems come with IPv6 fully functional. Third, you&apos;ll need to check whether 6to4 is available to you. It&apos;s very simple: just ping the IPv4 address 192.88.99.1. If you can successfully ping it, then you can use 6to4! And fourth, you must have a way to let your router recognize your external (public) IPv4 address as its own WAN IP. I have AT&amp;amp;T U-verse, so in my case, I just need to have the 2Wire gateway put my Tomato router in DMZ mode. Different ISPs and gateways/modems work differently, so YMMV.&lt;/p&gt;
&lt;p&gt;Once you&apos;ve verified the requirements, you&apos;ll need to flash your router. There are plenty of tutorials on how to do this in case it&apos;s not obvious. Also, I must add the mandatory warning that I AM NOT RESPONSIBLE for what you might do to your hardware or software and the issues it might cause. Flashing can be a dangerous procedure and you may end up with a bricked router. Don&apos;t say I didn&apos;t warn you. Flash it with a version of the &lt;a href=&quot;http://tomatousb.org/links&quot;&gt;Toastman compiles&lt;/a&gt;. I used &amp;quot;tomato-K26USB-1.28.7475.2MIPSR2-Toastman-RT-VPN.trx&amp;quot; for my Asus RT-N16. The default gateway is 192.168.1.1 and the default username/password combo is admin/admin.&lt;/p&gt;
&lt;p&gt;After configuring your standard router settings, go to the Overview page and make sure &amp;quot;IP Address&amp;quot; under &amp;quot;WAN&amp;quot; shows your public IP. If it doesn&apos;t, then you&apos;ll need to figure out why and fix it. See above for what I did. Then go to the IPv6 page under the Basic section. Choose &amp;quot;6to4 Anycast Relay&amp;quot; and leave the rest of the fields as they are. Save the configuration.&lt;/p&gt;
&lt;p&gt;Believe it or not, you&apos;re pretty much done. At this point, if you refresh your IPv6-ready computer&apos;s network settings (maybe do a DHCP release/renew just in case), you should have a public and fully functional IPv6 address. By default, Tomato blocks all TCP and UDP packets to your IPv6 devices. However, it doesn&apos;t block ICMPv6 Echo, otherwise known as Ping. If you want your router and your client computers to not receive IPv6 pings from the Internet, do the following. Go to the Scripts page under Administration, and select the Firewall tab. Add the following two lines to it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip6tables -I INPUT -i v6to4 -p icmpv6 --icmpv6-type echo-request -j DROP  
ip6tables -I FORWARD -i v6to4 -p icmpv6 --icmpv6-type echo-request -j DROP
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line prevents the router from responding to pings from the Internet, and the second one does the same for all the clients. Save the configuration and reboot the router. Congratulations, you&apos;re done! Wasn&apos;t that easy?&lt;/p&gt;
</description>
        </item>

        <item>
        <title>New Windows Phone Marketplace policies</title>
        <link>http://arktronic.com/weblog/2011-05-06/new-windows-phone-marketplace-policies/</link>
        <pubDate>Sat, 07 May 2011 04:02:44 +0000</pubDate>
        <guid isPermaLink="false">ID node/18 on http://arktronic.com</guid>
        <description>&lt;p&gt;Microsoft has released updated application certification requirements for submitting apps to the Marketplace that, according to &lt;a href=&quot;http://windowsteamblog.com/windows_phone/b/wpdev/archive/2011/05/06/more-clarity-and-self-test-help-on-our-content-policies-and-technical-requirements.aspx&quot;&gt;this blog post&lt;/a&gt;, will go into effect on June 3 (after the release of Mango tools).&lt;/p&gt;
&lt;p&gt;This is, of course, important news for anyone with apps in the Marketplace as well as anyone planning on submitting new apps. Let&apos;s take a look at the major differences between the previous version and the new one.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.14  Your application must have distinct, substantial and legitimate content and purpose other than merely launching a webpage.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a new requirement. It appears to be aimed at apps that are simple WebBrowser containers pointing to a publicly available site. Pretty self-explanatory.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.7 (part)  Applications that enable legal gambling in the applicable jurisdiction where legal gambling is allowed may be permitted, subject to the Application Provider&apos;s acceptance of additional contract terms.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is an addition to the illegal gambling clause to clarify that legal gambling is generally not an issue.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4.7  Application Tile Image&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This section seems to have been added for the singular purpose of telling developers to make their small and large app tiles actually have someting to do with their apps. Seems kind of obvious to me, but I suppose this rule wouldn&apos;t have been made had someone not tried to violate it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.1.3  Application Responsiveness: If an application performs an operation that causes the device to appear to be unresponsive for more than three seconds, such as downloading data over a network connection, the application must display a visual progress or busy indicator.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This was changed from the previous &amp;quot;5.1.3  Application Does not Hang&amp;quot;. It now has a well-defined limit of three seconds maximum before an app is required to display some kind of &amp;quot;busy&amp;quot; indicator. This makes sense from a UX perspective, although the only real difference between the two versions is the explicit 3 second rule, so it&apos;s not *that *different.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5.2.3  (missing?)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Um, yeah, I have a feeling Microsoft will fix this little omission.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6.5.4  The SoundEffect class must not be used to play a continuous background music track in an application.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This was changed from a &amp;quot;should&amp;quot; in a note to a separate &amp;quot;must&amp;quot; clause. MediaPlayer is for music, and SoundEffect is for, well, sound effects.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;These are all the major changes I noticed, ignoring small wording changes for clarification purposes and the like. Overall, this is not a major policy change.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Relax - Microsoft has not banned open source from Marketplace</title>
        <link>http://arktronic.com/weblog/2011-02-17/relax-microsoft-has-not-banned-open-source-from-marketplace/</link>
        <pubDate>Thu, 17 Feb 2011 15:22:40 +0000</pubDate>
        <guid isPermaLink="false">ID node/17 on http://arktronic.com</guid>
        <description>&lt;p&gt;This recent &lt;a href=&quot;http://news.slashdot.org/story/11/02/17/1429229/Microsoft-Bans-Open-Source-From-the-Windows-Market&quot;&gt;Slashdot article&lt;/a&gt; is sure to cause some hubbub. As usual (when it comes to anything Microsoft), it&apos;s completely inaccurate. The only licenses that have been banned are GPLv3 and its derivatives and equivalents, including LGPLv3, and Affero GPLv3. Why these particular licenses, and why specifically version 3?&lt;/p&gt;
&lt;p&gt;Because version 3 of the GPL family of licenses includes what has been dubbed the &amp;quot;anti-Tivoization&amp;quot; clause. Tivoization, from the name TiVo, is what that company did to its hardware in order to prevent unauthorized firmware modifications. In essence, they released the complete source code to the firmware that runs on TiVo boxes, but compiling such source code does not yield binaries that can run on the TiVo. That is because the authorized, official binary code is modified by TiVo to include a digital signature that must be accepted by the hardware before said code is allowed to run. GPLv3 includes a clause that prohibits this behavior.&lt;/p&gt;
&lt;p&gt;Microsoft must therefore ban licenses with an &amp;quot;anti-Tivoization&amp;quot; clause because both the Xbox and Windows Phone 7 hardware perform &amp;quot;Tivoization&amp;quot;. They only accept code that has been signed by Microsoft (unless the hardware is developer unlocked).&lt;/p&gt;
&lt;p&gt;So don&apos;t fret. All weak copyleft licenses and very liberal licenses such as MIT/X11 are perfectly fine for use in Xbox and WP7 code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Upon closer reading of the App Hub agreement &lt;a href=&quot;http://create.msdn.com/en-us/home/legal/Windows_Phone_Marketplace_Application_Provider_Agreement&quot;&gt;legalese&lt;/a&gt;, it looks like all copyleft licenses are banned - not just GPLv3, but all versions of the GPL, as well as MPL and even Microsoft&apos;s own Ms-RL. However, other, permissive free software licenses, such as BSD, MIT/X11, Apache, and Microsoft&apos;s Ms-PL can indeed be used in WP7 and Xbox software.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>An idea for curbing WP7 piracy</title>
        <link>http://arktronic.com/weblog/2010-12-29/idea-for-curbing-wp7-piracy/</link>
        <pubDate>Wed, 29 Dec 2010 20:03:17 +0000</pubDate>
        <guid isPermaLink="false">ID node/16 on http://arktronic.com</guid>
        <description>&lt;p&gt;Since it has been made &lt;a href=&quot;http://www.wpcentral.com/windows-phone-marketplace-app-security-cracked-proof-of-concept-video&quot;&gt;painfully obvious&lt;/a&gt; that Windows Phone 7 application piracy is possible, at least for developer unlocked devices, it&apos;s about time I outlined a fairly simple idea I had a couple of months back about curbing such piracy for a significant subset of the WP7 apps out there.&lt;/p&gt;
&lt;p&gt;First, the tl;dr version is as follows: &lt;strong&gt;Microsoft should provide an API to get (or verify) app purchasers&apos; anonymized Live IDs and/or Device Unique IDs.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, the explanation. Whenever a user purchases an app via the Marketplace, they must do so through their Live ID. Because their Live ID is associated to the app purchase, there should be a one-to-one relationship between a single app purchase and a Live ID. Assuming that the anonymized Live IDs (ANIDs) that are already available to app developers can be determined/calculated by Microsoft&apos;s services in the cloud, then all Microsoft has to do is expose an API that lets app developers check whether the current user&apos;s ANID is associated with a verified purchase.&lt;/p&gt;
&lt;p&gt;The reason I said earlier that this would apply to a subset of apps and not all of them is that such a validity check should only be performed server-side -- otherwise, an app that performs it locally can easily be cracked to NOP (ignore) the code performing this check. Because of this, only apps that rely on a cloud service would gain a significant benefit from doing ANID validity checking. And the reason I used the term &amp;quot;significant&amp;quot; is that Microsoft has already been pushing for more cloud functionality within apps, so the encouragement is already there to some extent.&lt;/p&gt;
&lt;p&gt;Finally, if ANIDs cannot be calculated by Microsoft outside of the phone, then the same idea would still apply to Device Unique IDs. Since a check is already being done to ensure that an app is not installed on more than five (I believe) devices associated with a single Live ID, Microsoft has to already be storing all active Device IDs per Live ID. Exposing an API to check for the validity of a Device ID based on its parent Live ID would provide the same benefit.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>The ugly side of Windows Phone 7: Marketplace</title>
        <link>http://arktronic.com/weblog/2010-11-23/ugly-side-windows-phone-7-marketplace/</link>
        <pubDate>Wed, 24 Nov 2010 04:30:54 +0000</pubDate>
        <guid isPermaLink="false">ID node/15 on http://arktronic.com</guid>
        <description>&lt;p&gt;Before I start my rant, let me preface this post by saying that I do really like WP7, from a consumer perspective (it&apos;s very responsive, good looking, and just plain fun to use) and a developer perspective (language and tools are a breeze to use, there&apos;s a lot of helpful info online, and the community is great).&lt;/p&gt;
&lt;p&gt;However, it&apos;s not all good. There are some severe limitations to what is allowed. For example, there&apos;s no third party multitasking, no raw socket access, and no clipboard. Yes, at the very least the clipboard thing will be addressed in the first update sometime in early 2011, I know. These limitations, however, are due to a very demanding schedule of releasing a completely redesigned mobile operating system. In other words, they&apos;re understandable, and they will be fixed in future updates.&lt;/p&gt;
&lt;p&gt;But there is another limitation that is not so understandable. It cannot be attributed to the release schedule of the OS because it is not new. It is, as the title of this post suggests, the Marketplace. It has existed officially since 2009 and leaks of it have been seen since 2008. And yet, this software is still unrefined. More than that, it&apos;s buggy.&lt;/p&gt;
&lt;p&gt;Let&apos;s start with the consumer perspective. The biggest issue is random freezing in the Marketplace app. This happened all the time with the pre-production device I had (but I can&apos;t complain about that - it&apos;s pre-production after all) and it still happens with my production device today! It&apos;s especially evident in areas of bad reception. When scrolling down a list of apps, the Marketplace freezes (hangs) until the next part of the list is downloaded. Sometimes this works properly and I get to the bottom of the list, where the &amp;quot;Loading...&amp;quot; text resides, but most of the time it freezes in the middle of me trying to scroll down, which is extremely irritating, to say the least. It baffles me that such a bug got through QA.&lt;/p&gt;
&lt;p&gt;Another major consumer issue is search. One would think that it would have been fixed by now, after all the problems that were experienced with Windows Mobile 6.5 Marketplace searching, but no. Keyword search still doesn&apos;t work. Also, for some reason, searching in a subsection of the Marketplace brings up results for all apps and games as well as Zune music, with no apparent section bias, which tends to result in a huge number of entries through which the user has to sift in order to find what they were actually looking for.&lt;/p&gt;
&lt;p&gt;The last consumer issue is the lack of an official Web-based app viewer. Yes, there&apos;s the Bing Visual Search with its astounding lack of deep linking, and the various third party app viewer websites, but an official one is sorely missing. Since one exists for Windows Mobile 6.5, I assume this omission is scheduling-related.&lt;/p&gt;
&lt;p&gt;Finally, let&apos;s move on to the developer perspective. For the most part, I&apos;ve been pleased with how easy it is to submit an app. The Silverlight-enhanced wizard actually seems to have been designed well. However, there are other issues. One is that developer verification is completely screwed up. There are forum posts all over the App Hub from developers complaining of being stuck at one stage or another. Personally, I&apos;m stuck at the stage of verifying bank information. According to a forum post that I can&apos;t find at the moment, that error is normal and will go away upon the first payout. That&apos;s just idiotic and reeks of awful coding and/or policies. Most organizations, when trying to verify bank account information, do one or two sub-dollar credits and immediately debit them back, and ask the user to verify the amounts of those credits. Microsoft, apparently, doesn&apos;t want to do that, and will instead verify bank details when it&apos;s actually time to pay the developer. Not very professional, especially if there end up being verification issues.&lt;/p&gt;
&lt;p&gt;Another problem is that app &amp;quot;fulfillment&amp;quot; tracking is, well, non-functional. According to yet another forum post, it will become functional in January 2011. Why? Nobody knows. If I didn&apos;t have any way of tracking app usage for Network Suite, I&apos;d be extremely angry. As it happens, I do have a way to do that, since most requests have to go through my server, and I can log them to the extent of figuring out unique users. So I&apos;m less upset about that than I could have been.&lt;/p&gt;
&lt;p&gt;Overall, the Windows Phone Marketplace is a disappointment right now, but if Microsoft really is serious about WP7 (and I believe they are) then these issues should be fixed pretty darn quickly.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; I forgot to mention the most infuriating consumer-side bug. When viewing screenshots, they rotate (and consequently resize) with the phone&apos;s orientation. That shouldn&apos;t be happening because, chances are, if you&apos;re trying to look at a screenshot horizontally, it&apos;s probably a landscape-mode one!&lt;/p&gt;
</description>
        </item>

        <item>
        <title>To open source or not to open source</title>
        <link>http://arktronic.com/weblog/2010-11-18/to-open-source-or-not-to-open-source/</link>
        <pubDate>Fri, 19 Nov 2010 04:06:10 +0000</pubDate>
        <guid isPermaLink="false">ID node/14 on http://arktronic.com</guid>
        <description>&lt;p&gt;I guess I haven&apos;t blogged in a few months. Oops. I probably should have, just to write down what&apos;s been happening. Here&apos;s what&apos;s relevant to this post anyway: I got a Windows Phone 7 pre-production device a while ago, and right now I&apos;m using a production LG Optimus 7 as my everyday phone (and loving it).&lt;/p&gt;
&lt;p&gt;I have just released my first WP7 Marketplace app, &lt;a href=&quot;http://social.zune.net/redirect?type=phoneApp&amp;amp;id=98997f33-3af1-df11-9264-00237de2db9e&quot;&gt;Network Suite&lt;/a&gt;. I&apos;ve had a few sales already, less than a dozen, but I&apos;m not really expecting much sales from a tool like that. Since that app is now released, I&apos;m considering making a game for WP7 as my next project. More than that, I want to open source it with the intention of having the developer community help out with additional features.&lt;/p&gt;
&lt;p&gt;However, therein lies my dilemma. I have already done something like this in the past, and it has not worked out the way I&apos;d hoped. My Windows Mobile app ArkSwitch, which has tens of thousands of downloads, is used by a lot of people, and is installed by default in most custom ROMs today, was open sourced soon after its release in the hopes that the community would help me fix bugs and add features. I have had exactly &lt;strong&gt;one&lt;/strong&gt; person do that at one point in time. That&apos;s it. People do download the source code; I can see that on CodePlex. But nobody else is offering to help in any way.&lt;/p&gt;
&lt;p&gt;So why should I open source my new game? On one hand, I want to do it regardless of the amount of help I get just so that there&apos;s more WP7 code out there that other developers can look at and learn from. That&apos;s important. On the other hand, though, I&apos;d probably want to charge a small amount for the game in the Marketplace, and there is nothing preventing an unscrupulous developer from taking that exact same code and submitting it to the Marketplace as well with little or no changes and a different price (or the same price, or free, whatever). I know for a fact that I would be rather upset at such an occurrence. Not because I would be losing potential sales (well, a little because of that) but mainly because this developer did none of the work and is getting rewarded for copying that of others, be it monetarily or in terms of recognition.&lt;/p&gt;
&lt;p&gt;So I am seeking advice from my fellow developers. What do you think about open sourcing a project like this? Am I being petty about this? Jaded? Naive? Please let me know your thoughts on this subject, either via comments below or on &lt;a href=&quot;http://twitter.com/Arktronic&quot;&gt;Twitter&lt;/a&gt;. I would appreciate it immensely.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Yes, I would like a WP7 dev device, please</title>
        <link>http://arktronic.com/weblog/2010-06-15/yes-i-would-like-wp7-dev-device-please/</link>
        <pubDate>Tue, 15 Jun 2010 15:35:17 +0000</pubDate>
        <guid isPermaLink="false">ID node/13 on http://arktronic.com</guid>
        <description>&lt;p&gt;This blog post shall serve as my semi-official request to receive a Windows Phone 7 development device from Microsoft. So, why should I receive one of these devices? The reasons are quite simple.&lt;/p&gt;
&lt;p&gt;First of all, I have a passion for mobile development. I have developed applications for Windows Mobile 5/6/6.5.3 both as part of my day job, and as a hobby. In fact, one of the latter projects is &lt;a href=&quot;http://arkswitch.codeplex.com/&quot;&gt;on CodePlex&lt;/a&gt;. That particular project is not quite portable to WP7 for obvious reasons, but I do have various ideas for new projects, as well as a new project I&apos;m currently working on (that I&apos;m not prepared to discuss on a public blog just yet).&lt;/p&gt;
&lt;p&gt;Second, I have already invested time (and money) into researching and developing for WP7. I attended MIX10 for the purpose of learning first-hand about the WP7 development story. I have also almost-successfully ported the Bouncy Castle C# cryptography library to WP7 - see &lt;a href=&quot;http://social.msdn.microsoft.com/Forums/en-US/windowsphone7series/thread/83b82a73-b7c8-4913-b311-a32b81b345e8/&quot;&gt;this discussion&lt;/a&gt; for details and the download.&lt;/p&gt;
&lt;p&gt;And finally, I believe that I can improve the Windows Phone 7 end-user experience by creating useful applications for said users to download and enjoy on their brand-new devices.&lt;/p&gt;
&lt;p&gt;Please contact me &lt;a href=&quot;http://twitter.com/arktronic&quot;&gt;on Twitter&lt;/a&gt; or via email at my first name (listed below) -at- &lt;a href=&quot;http://indyalt.net&quot;&gt;IndyALT.NET&lt;/a&gt; -- by the way, I&apos;m currently the VP of this user group.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Here&apos;s &lt;a href=&quot;http://www.1800pocketpc.com/2010/06/23/windows-phone-7-app-arkwords.html&quot;&gt;a (p)review&lt;/a&gt; of my first WP7 app, ArkWords. It&apos;s not the &amp;quot;new project&amp;quot; mentioned above, but it&apos;s something I decided to make, well, just because.&lt;/p&gt;
&lt;p&gt;Thanks a lot,&lt;br /&gt;
Sasha&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
</description>
        </item>

        <item>
        <title>On software licenses</title>
        <link>http://arktronic.com/weblog/2010-05-13/on-software-licenses/</link>
        <pubDate>Fri, 14 May 2010 03:51:10 +0000</pubDate>
        <guid isPermaLink="false">ID node/12 on http://arktronic.com</guid>
        <description>&lt;p&gt;Recently, the &lt;a href=&quot;http://www.joindiaspora.com/&quot;&gt;Diaspora&lt;/a&gt; project has been making waves on the Internet. Personally, I&apos;m glad someone is taking privacy seriously for a change, but that&apos;s not what this post is about. One thing about the project that caught my attention is that their Kickstarter page says that they promise to release the source code under &amp;quot;aGPL&amp;quot;, also known as &lt;a href=&quot;http://en.wikipedia.org/wiki/Affero_General_Public_License&quot;&gt;Affero GPL&lt;/a&gt;. This is a modification of the well-known &lt;a href=&quot;http://en.wikipedia.org/wiki/GNU_General_Public_License&quot;&gt;General Public License&lt;/a&gt; that removes a potential loophole in the &amp;quot;standard&amp;quot; GNU GPL that may allow for proprietary modifications to code as long as it is not distributed to others - such as in the case of hosted Web services and applications.&lt;/p&gt;
&lt;p&gt;Closing such a loophole is a good idea for the purposes of forcing software source code to be freely available, but I take issue with its effect on freedom. In fact, this doesn&apos;t just apply to the AGPL, but to most &lt;a href=&quot;http://en.wikipedia.org/wiki/Copyleft&quot;&gt;copyleft&lt;/a&gt; licenses. My issue is that, while users of copyleft-licensed software may have more freedom (as in speech) than with proprietary software, developers have to face severe restrictions on licensing not only their modifications, but any other code that links to the copyleft software.&lt;/p&gt;
&lt;p&gt;I don&apos;t have a big problem with requiring modifications to be released under an open source license. After all, I released ArkSwitch under Ms-RL. In my opinion, if you want people to help improve your software, you can license it in a way that ensures the improvements are made freely available. However, even that impinges on other developers&apos; freedom, though to a lesser extent.&lt;/p&gt;
&lt;p&gt;I do have a big problem with requiring any and all software that merely links with copyleft code to be itself released under a copyleft license. That is simply not freedom. Why should I be barred from making closed software that links to open source code and selling it? That jars my thought processes. When I think of open source, I instinctively think of freedom. In a selfish sense, that includes freedom for me to do what I want with said open source code (within reason, of course). Licenses such as the GPL and, even more so, the AGPL, while claiming freedom, do quite the opposite for programmers&apos; rights.&lt;/p&gt;
&lt;p&gt;If you truly want to make your software free and open source, do not choose a restrictive copyleft license. At a minimum, use the LGPL, which allows for linking to proprietary software. As I stated earlier, Ms-RL does so as well. However, for even less restrictions and, consequently, more freedom, choose the Apache, BSD, or MIT/X11 license.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Thoughts on ArkSwitch, my first "popular" app</title>
        <link>http://arktronic.com/weblog/2010-01-23/thoughts-on-arkswitch-my-first-popular-app/</link>
        <pubDate>Sat, 23 Jan 2010 20:55:42 +0000</pubDate>
        <guid isPermaLink="false">ID node/11 on http://arktronic.com</guid>
        <description>&lt;p&gt;I think I can safely say that, to date, I&apos;ve not really released anything that I could consider popular. Things like The Vista Syn are useful tools for a very limited audience. That&apos;s exactly what I expected when I released ArkSwitch, a finger-friendly Windows Mobile 6.5.1+ task manager. After all - it&apos;s a task manager, which there are plenty of out there; for an unreleased version of WinMo; and it doesn&apos;t even have the coolest features of other task managers, like taking over the X button.&lt;/p&gt;
&lt;p&gt;Well, that&apos;s not entirely how it happened.&lt;/p&gt;
&lt;p&gt;I released the app around 11 PM on 2010-01-14 on the &lt;a href=&quot;http://forum.xda-developers.com&quot;&gt;xda-developers&lt;/a&gt; forum, which is pretty much the best Windows Mobile forum out there, even though it (currently) only has HTC-manufactured devices in its device-specific sections. It got a few downloads and some feedback posts, which I answered. I got a rather odd, generic-looking, PM (private message) the next day, saying that somebody put the app up on their website and that I am &amp;quot;now famous&amp;quot;, which I found humorous since it was posted to a website I&apos;ve never heard of. Regardless, people kept commenting and offering suggestions for ArkSwitch. I released two newer versions with more enhancements. And a few days after that, I decided to do a search for &amp;quot;arkswitch&amp;quot;.&lt;/p&gt;
&lt;p&gt;I did not expect to find what I did. First of all, it was apparently available on a few mobile warez sites, which is a bit strange, because I released ArkSwitch as freeware. But stranger yet, I found a &lt;em&gt;lot&lt;/em&gt; of mentions on various mobile software-related websites! Not only that, but it was in multiple languages, too! I found news entries and forums threads in Russian, Spanish, French, Polish, Turkish, Arabic, and Chinese. There are probably more; I didn&apos;t go through too many pages of results.&lt;/p&gt;
&lt;p&gt;Frankly, I am rather surprised. I never expected ArkSwitch to take off like that. At time of writing, just the latest version has over 1700 downloads on xda-developers alone, and hundreds more on the various other sites that are hosting it themselves. I am, of course, pleased that people are enjoying my work. This experience only inspires me to do more things like it. I have been meaning to work on a couple more apps, but I&apos;ve been lazy about it. Maybe now I will actually do it.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Home automation project, part IV</title>
        <link>http://arktronic.com/weblog/2009-11-15/home-automation-project-part-iv/</link>
        <pubDate>Sun, 15 Nov 2009 16:14:03 +0000</pubDate>
        <guid isPermaLink="false">ID node/10 on http://arktronic.com</guid>
        <description>&lt;p&gt;In &lt;em&gt;Part I&lt;/em&gt;, &lt;em&gt;Part II&lt;/em&gt;, and &lt;em&gt;Part III&lt;/em&gt;, I covered mostly the reasoning behind my home automation project, and its hardware components. Now it&apos;s time to conquer the project&apos;s most challenging aspect, the software. I suppose the reason that I consider software to be the most challenging aspect is that I&apos;m a software engineer, and as such, I always feel the need to tweak the hell out of any software system I might go with, or even create my own from scratch. Hardware engineering isn&apos;t aligned too well with my skillset, which is why I very rarely do any hardware hacking that might involve precision soldering and the like. Therefore, I usually end up accepting hardware limitations as a fact of life (unless they&apos;re just too egregious), but I have a hard time accepting software limitations, especially when there&apos;s something I can do about them.&lt;/p&gt;
&lt;p&gt;I have a few criteria for the software I would like to use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inexpensive - after investing so much in the hardware (I have no doubt that it will add up quickly), I don&apos;t want to spend a ton on the software&lt;/li&gt;
&lt;li&gt;Customizable - I want to be able to do pretty much anything I want with certain events, times, etc.&lt;/li&gt;
&lt;li&gt;Extensible - if there&apos;s some customization that can&apos;t be done by default, I want to be able to extend the software myself&lt;/li&gt;
&lt;li&gt;Compatible - I don&apos;t want the software to only be compatible with a single PC controller, or a single series of products&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition to the generic criteria above, here are the features I want the software to have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actions executed based on time, events, or a combination thereof&lt;/li&gt;
&lt;li&gt;Customizable actions including controlling Z-Wave devices as well as external programs&lt;/li&gt;
&lt;li&gt;Web-based interface (HTTPS!) with either a mobile-friendly layout or actual mobile apps, including Windows Mobile and possibly Android - don&apos;t care about iPhone&lt;/li&gt;
&lt;li&gt;Easy to understand current and historical status reports&lt;/li&gt;
&lt;li&gt;Ability to record and display available video streams&lt;/li&gt;
&lt;li&gt;OPTIONAL: TTS support&lt;/li&gt;
&lt;li&gt;OPTIONAL: speech recognition support - I really don&apos;t need this, but it would be cool&lt;/li&gt;
&lt;li&gt;Intuitive set up and control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&apos;ve found four software solutions for Z-Wave control that look at least a little promising: ThinkEssentials from ControlThink, HSPRO from HomeSeer, Web-Link II from HAI, and the open source LinuxMCE project. HSPRO is the first to go, as it costs a whopping $600. Hell no.&lt;/p&gt;
&lt;p&gt;Web-Link looks pretty interesting, but apparently it can&apos;t &amp;quot;configure&amp;quot; or &amp;quot;program&amp;quot; the HAI system, which in this case would be a Z-Wave system - I&apos;m not even sure what the difference between &amp;quot;configure&amp;quot; and &amp;quot;program&amp;quot; is in this context. It&apos;s certainly cheaper than HSPRO, but it&apos;s still around $300, and I couldn&apos;t use it to do system setup or to extend its functionality (as far as I can tell), so it&apos;s out as well.&lt;/p&gt;
&lt;p&gt;ThinkEssentials seems to be a nice program, pretty cheap (around $50-$90), and ControlThink provides an SDK as well. Unfortunately, any program using the SDK cannot run at the same time as ThinkEssentials on the same hardware, plus the SDK has some severe limitations, both control-wise and licensing-wise. Since SDK-based apps have to be separate from ThinkEssentials, I&apos;d pretty much end up writing my own Z-Wave control suite using their limited SDK, which I don&apos;t really want to do. ThinkEssentials is, therefore, out.&lt;/p&gt;
&lt;p&gt;All that&apos;s left now is LinuxMCE. It is extremely powerful, with many features ranging from PVR to home automation to telecommunications. It has its own Z-Wave driver that is compatible with many PC controllers and many devices. It even has a specialized interface for Windows Mobile. Oh, and it&apos;s free and open source. The problem is, I&apos;m not sure I&apos;d be able to extend it easily. This software is so complex that I&apos;d have to understand very many aspects of it before I could do anything useful to it. In addition, the documentation for the software - a wiki - is perpetually incomplete and/or out of date, something which open source projects are notorious for. So while LinuxMCE is the most promising software, I don&apos;t think I want to go with it considering its issues.&lt;/p&gt;
&lt;p&gt;The only option l have left, that I hinted at in the beginning of this post, is writing my own software from scratch. It would be a pretty giant challenge to get Z-Wave control to work, not to mention all the features I listed above.&lt;/p&gt;
&lt;p&gt;If I am to write my own software, I must have access to Z-Wave technical information. Unfortunately, the official SDK is under NDA and costs over $1000. That is most definitely out of the question (unless Zensys/Sigma Design decide to be nice to me and lower the price). One alternative is to make use of the LinuxMCE documentation on the Z-Wave protocol, which was reverse-engineered by monitoring COM ports. I&apos;m not sure if the information in their wiki carries a restrictive license, though, so that might be an issue. Another alternative is for me to reverse-engineer the protocol myself. Not an easy task, but I&apos;ve done communications reverse engineering before.&lt;/p&gt;
&lt;p&gt;Assuming I&apos;m writing my own software, I need to determine the requirements, limitations, and other basics. First of all, I know that I want to write this in C#. That limits me to either Windows or the (rather large) subset of the .NET Framework supported by Mono. The good thing about writing Mono-compatible code is that I could run it on a Marvell SheevaPlug, a wall wart-sized computer with a 1.2GHz ARM CPU that uses somewhere between 2W and 7W of power. It would be energy efficient and reliable, having no moving parts (as far as I know) and a stable Linux operating system. Mono compiles for ARM, too. The bad thing about writing Mono-compatible code is that I have to take into account Mono&apos;s limitations and implementation quirks.&lt;/p&gt;
&lt;p&gt;As for application requirements, most of them have already been outlined above as part of my software criteria. This software should be modular enough that it could be extended relatively easily to control non-Z-Wave home automation technologies. In fact, as the criteria imply that I want to be able to execute any action available to the computer, perhaps Z-Wave should be implemented through a generic control interface, along with other interfaces, such as executing external programs, calling Web services, etc. At this point, I have to step back and ask myself: am I making this project too complicated? Is a whole event framework that encapsulates Z-Wave, generic actions and events, and basic interfaces for powerful extensibility all just a little too much? Well, let&apos;s see.&lt;/p&gt;
&lt;p&gt;An event framework shouldn&apos;t be too hard to write. You have events, actions, and mappings between the two. &lt;em&gt;Sounds&lt;/em&gt; easy. Of course, I&apos;ve never actually written one, so there is a high probability of me simply being ignorant of some very complex behaviors that I&apos;d have to address. Then, once the basic event framework is ready, it needs to have some default actions coded (e.g. execute a program, call a Web service, execute a Z-Wave command) as well as some default events (Z-Wave event occurred, date/time reached a set point, command received from some interface). Out of those, the Z-Wave actions and events would be the most difficult ones to write. After that, I&apos;d have to design and create the UX for the entire system.&lt;/p&gt;
&lt;p&gt;This is most definitely a big project, but out of all the things I need to do with it, the extensibility portion probably isn&apos;t a very hard one. That would mean that, to answer my earlier question, this isn&apos;t too much. Before I commit to this plan of action, however, I want to try out LinuxMCE to see if it can do &lt;em&gt;most&lt;/em&gt; of what I want. If it can, then I might just settle for it, assuming, of course, that it&apos;s intuitive enough, since I already know the documentation is lacking. The next step, then, is to get some Z-Wave hardware and try out LinuxMCE. That will take a while, since I have many more urgent tasks to do around the house, so the next update on this project will likely not happen any time soon.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Home automation project, part III</title>
        <link>http://arktronic.com/weblog/2009-11-10/home-automation-project-part-iii/</link>
        <pubDate>Tue, 10 Nov 2009 15:22:46 +0000</pubDate>
        <guid isPermaLink="false">ID node/9 on http://arktronic.com</guid>
        <description>&lt;p&gt;Although I&apos;m not 100% done with the lamp topic from &lt;em&gt;Part II&lt;/em&gt;, I think it&apos;s in a good enough state that I can move on to something else for now. The next topic is controlling window blinds, also known as an exercise in masochism. Existing solutions are obscenely expensive, starting around the $130 mark and going way past $500. I refuse to pay that much for automated window coverings. Luckily, there is an alternative.&lt;/p&gt;
&lt;p&gt;While I was researching this matter, I came across &lt;a href=&quot;http://www.instructables.com/id/Build-A-Motorized-Window-Blinds-Controller-For-Les/&quot;&gt;instructions&lt;/a&gt; on DIY window blind control for as little as $15. Needless to say, I was intrigued. The procedure, however, is neither quick, nor does it address Z-Wave specifically. It probably wouldn&apos;t be hard to connect a Z-Wave relay to this setup. However, considering the fact that I&apos;m not an electrical/electronics engineer and that these instructions themselves are not trivial to follow, I think it would be best if I just don&apos;t bother with automating window blinds at this time. That should save me quite a bit of sanity.&lt;/p&gt;
&lt;p&gt;Next on the list is sensing open doors and windows. I would want to do this with my front door, garage door, as well as fridge and freezer doors. Maybe I would monitor downstairs windows as well, but maybe not. For all of this, I can use the &lt;a href=&quot;http://www.asihome.com/ASIshop/product_info.php?cPath=564_572&amp;amp;products_id=3062&quot;&gt;HRDS1&lt;/a&gt; or the &lt;a href=&quot;http://store.homeseer.com/store/HM-DW001---Z-Wave-Door-Sensor-Homemanageables-P763C57.aspx&quot;&gt;HM-DW001&lt;/a&gt;. Once again, the devices aren&apos;t cheap, but they serve a good purpose. I could actually build my own security system using these, coupled with motion sensors and cameras. Speaking of motion sensing, that&apos;s next on the list. The &lt;a href=&quot;http://store.homeseer.com/store/HSM100---Z-Wave-Multi-Sensor-HomeSeer-P438C57.aspx&quot;&gt;HSM100&lt;/a&gt; appears to be an excellent device to get for motion sensing, temperature sensing, and light level sensing. Alternatively, I can use the &lt;a href=&quot;http://www.asihome.com/ASIshop/product_info.php?cPath=564_572&amp;amp;products_id=3959&quot;&gt;ZIR000&lt;/a&gt; or the &lt;a href=&quot;http://www.asihome.com/ASIshop/product_info.php?cPath=564_572&amp;amp;products_id=3064&quot;&gt;HRMD1&lt;/a&gt; just to sense movement.&lt;/p&gt;
&lt;p&gt;I&apos;ll skip sound sensing since I haven&apos;t found any pre-made devices that do it, and it&apos;s not important enough (at this time) to warrant further DIY research. Next is multi-room temperature sensing. The same HSM100 can be used for this purpose, or I could use the more specialized &lt;a href=&quot;http://store.homeseer.com/store/HM-TS001---Z-Wave-Temperature-Humidity-Sensor-Homemanageables-P764C57.aspx&quot;&gt;HM-TS001&lt;/a&gt;, which has temperature as well as humidity sensing. It would be great to place a couple of these downstairs as well as upstairs so that I could see the temperature difference, and if it&apos;s too great, I can open or close some vents to equalize the temperature. And no, I have absolutely no intention of automating the opening or closing of vents.&lt;/p&gt;
&lt;p&gt;The next item on the list is home theater control. This is already mostly taken care of, as I recently purchased the Logitech &lt;a href=&quot;http://www.amazon.com/dp/B001L1RSO8&quot;&gt;Harmony 890&lt;/a&gt;. I don&apos;t think I&apos;ll need to control my home theater setup remotely, so this solution should suffice. The last item on the list is door locks and garage control. Schlage is, as far as I know, the only company that&apos;s currently making Z-Wave door locks. However, they are rather expensive, and they require a monthly subscription to the Schlage Link online service (although &lt;a href=&quot;http://www.micasaverde.com/&quot;&gt;Mi Casa Verde&lt;/a&gt; and &lt;a href=&quot;http://www.homeseer.com/&quot;&gt;HomeSeer&lt;/a&gt; both have solutions that bypass this requirement). As for garage door control, Wayne-Dalton has a few products that let the garage door remote access Z-Wave scenes, but I haven&apos;t found anything that lets me open the garage door using Z-Wave. It would have to be as secure as the Schlage locks - using AES. However, the locks and garage door control are not very important to me at the moment, so I&apos;ll wait until there is more support for these products, and more products to choose from.&lt;/p&gt;
&lt;p&gt;That&apos;s the end of the list. The only thing left to discuss is the biggest challenge in this entire project - centralized control of all the Z-Wave devices. That will likely require multiple weblog posts, so I&apos;ll start on those later.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Home automation project, part II</title>
        <link>http://arktronic.com/weblog/2009-11-07/home-automation-project-part-ii/</link>
        <pubDate>Sat, 07 Nov 2009 22:02:03 +0000</pubDate>
        <guid isPermaLink="false">ID node/8 on http://arktronic.com</guid>
        <description>&lt;p&gt;Since I have decided to go with Z-Wave at the end of &lt;em&gt;Part I&lt;/em&gt;, the next step is to determine what hardware I&apos;ll need for the features I laid out (again in part I). Some hardware is obvious, like the various Z-Wave modules I&apos;ll have to buy, while other hardware is more ambiguous, like what my Internet-accessible controller will run on. Let&apos;s get the more obvious stuff out of the way first.&lt;/p&gt;
&lt;p&gt;The first feature I&apos;ll deal with is HVAC access. There are a few choices for this, but the only feasible one appears to be the Schlage-manufactured Trane &lt;a href=&quot;http://www.asihome.com/ASIshop/product_info.php?products_id=3941&quot;&gt;TZMT400&lt;/a&gt;, also known as &lt;a href=&quot;http://store.homeseer.com/store/TZEMT043AB32---Z-Wave-Thermostat-Trane-P720C183.aspx&quot;&gt;TZEMT043AB32&lt;/a&gt;. I&apos;m really not sure why it has multiple model numbers, so I&apos;ll just ignore that little problem. Other possibilities include the Wayne-Dalton WDTC-20 as well as its rebranded twin, the Intermatic CA8900 - people are saying that these devices are badly constructed - and the RCS brand thermostat stuff that is way too expensive.&lt;/p&gt;
&lt;p&gt;Next on the list is light control. Certain lights should be dimmable, while others should not. I also have to take into account support for &amp;quot;beaming,&amp;quot; which is required from other devices to the Schlage locks. Then again, the Trane thermostat supports beaming, and it is pretty close to my front door. If, however, the distance is too great, pretty much the only option I have is to buy a Schlage lamp module to talk to it, assuming I actually decide to use Schlage Z-Wave locks. So, what lamp modules and other lighting-related hardware should I buy? I&apos;ve been wrestling with this question since the Part I post, and I&apos;ve been unable to come up with a good solution.&lt;/p&gt;
&lt;p&gt;The issue is, I want to make use of dimmers, and ones like the &lt;a href=&quot;http://www.smarthome-products.com/p-460-intermatic-homesettings-ha20c-3-way-in-wall-switch-dimmer.aspx&quot;&gt;HA20C&lt;/a&gt; are fairly inexpensive (comparatively speaking), but they do not work with CFL or LED bulbs. Part of my energy savings plan is to use CFLs and LEDs wherever possible in order to save on electricity costs. All of the cheaper Z-Wave dimmers I&apos;ve seen have a minimum load requirement of 40W, which is obviously incompatible with 60W-equivalent 15W CFLs and 8W LEDs. If I don&apos;t use the cheaper dimmers, I have to use either more expensive dimmers or relay switches, such as the &lt;a href=&quot;http://www.smarthome-products.com/p-168-homepro-zrw103-z-wave-20-amp-3-way-relay-white.aspx&quot;&gt;ZRW103&lt;/a&gt;, which are twice as expensive as the cheaper dimmers, if not more. While this is a one-time investment (ignoring replacement of malfunctioning switches), it is still pretty expensive. Some people have reported limited success using cheaper dimmers with dimmable CFLs, but since the loads for them are still below the minimum requirements, those dimmers would likely fail quickly when used in such a fashion.&lt;/p&gt;
&lt;p&gt;What I will probably end up doing is using the &lt;a href=&quot;http://www.smarthome-products.com/p-654-homepro-zrf113-z-wave-isolated-contact-fixture-module.aspx&quot;&gt;ZRF113&lt;/a&gt; wherever applicable, supplemented by &lt;a href=&quot;http://www.smarthome-products.com/p-170-homepro-as101-z-wave-3-way-companion-switch.aspx&quot;&gt;AS101&lt;/a&gt; auxiliary switches. I might also use the &lt;a href=&quot;http://www.smarthome-products.com/p-163-intermatic-homesettings-ha05c-z-wave-indooroutdoor-screw-in-module.aspx&quot;&gt;HA05C&lt;/a&gt; in any locations that warrant its use. I think I will use ACT products whenever I can because (1) they are fairly inexpensive compared to other equivalent products, and (2) they are a local Indianapolis-based company, so I could easily talk to them if I have any issues or questions.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>Home automation project, part I</title>
        <link>http://arktronic.com/weblog/2009-11-02/home-automation-project-part-i/</link>
        <pubDate>Mon, 02 Nov 2009 21:51:46 +0000</pubDate>
        <guid isPermaLink="false">ID node/7 on http://arktronic.com</guid>
        <description>&lt;p&gt;I have wanted to do home automation for a number of years now, but various things prevented me from experimenting with it, not the least of them being me not owning a house. Now that I actually do own a house, I&apos;ve been looking at home automation technologies more and more, trying to decide what I can do, what I should do, what is feasible to do, and so forth. This is the first of a series of posts that serve as containers for both my rationale for and progress with this project.&lt;/p&gt;
&lt;p&gt;The first issue I should address is, why am I doing this? To be perfectly honest, I don&apos;t &lt;em&gt;need&lt;/em&gt; to automate anything in my house. However, there are many benefits to automating. The one most commonly cited is burglary prevention - that is, being able to turn on all the lights in the house at once with a single command. That&apos;s nice, but it&apos;s not a major concern for me. Here are some of the reasons I want to automate my house, not in any particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Geek factor - come on, everyone knows that this would be way cool&lt;/li&gt;
&lt;li&gt;Convenience - example: if I forgot to turn off the lights when I left the house, I can do it remotely&lt;/li&gt;
&lt;li&gt;Energy savings - being able to turn off all the lights and turn down the heating or AC when nobody&apos;s home would save a lot of electricity&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That brings us to the second issue - what features do I want in my system? Two of the above reasons bring about obvious feature requirements: remote (i.e., online) control, HVAC access, and light control. In addition to that, I want to have an integrated house monitoring system capable of detecting open windows/doors, movement, and potentially sound. Also, I would really like to control door locks, but that introduces a huge security issue that I have to take into account, so I may just have to postpone doing such a thing until later.&lt;/p&gt;
&lt;p&gt;Now, to list all the features as a... list.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Remote (online, Bluetooth, etc) control&lt;/li&gt;
&lt;li&gt;HVAC access with heat pump control&lt;/li&gt;
&lt;li&gt;Light control - on/off and dimming when applicable&lt;/li&gt;
&lt;li&gt;Shade control - controlling blinds in the house&lt;/li&gt;
&lt;li&gt;Open door/window sensing&lt;/li&gt;
&lt;li&gt;Movement sensing&lt;/li&gt;
&lt;li&gt;OPTIONAL: Sound sensing&lt;/li&gt;
&lt;li&gt;Multi-room temperature sensing (not necessarily accurate enough for presence detection, just comfort)&lt;/li&gt;
&lt;li&gt;OPTIONAL: home theater control - at least on/off - without killing power at the wall socket&lt;/li&gt;
&lt;li&gt;OPTIONAL: door lock and/or garage control&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above features can be broken down in a fairly clean way into hardware and software categories. In fact, pretty much the only category that isn&apos;t hardware is the first one - remote control. It&apos;s also the one posing the biggest challenge in terms of implementation. Almost everything else I can just buy and connect. However, since I have not found any reasonably-priced online home automation controllers, I will likely have to implement one myself.&lt;/p&gt;
&lt;p&gt;Another issue that must be discussed is underlying home automation technology. I have decided to go with Z-Wave after carefully evaluating my choices. Here&apos;s how I came to this decision. In the home automation market, there are powerline-based systems and wireless ones. While powerline technology offers more security than wireless, it is very susceptible to interference from, e.g., vacuum cleaners. In addition, the most popular powerline-based standard - X10 - is unreliable in that it doesn&apos;t communicate success or failure of commands, so you don&apos;t know whether your command reached its destination. Other standards, such as UPB and Insteon, fix certain X10 issues, but ultimately, they are all affected by noisy powerlines.&lt;/p&gt;
&lt;p&gt;On the wireless side, the most popular technologies are ZigBee and Z-Wave. ZigBee is based on IEEE 802.15.4, and is closer to an open standard than Z-Wave. However, there aren&apos;t that many products out there based on this technology, and those that are out are extremely expensive compared to their Z-Wave equivalents. I can see ZigBee eventually becoming popular and less expensive, but that won&apos;t happen for a while, so I can either wait an undetermined period of time, or just go with Z-Wave.&lt;/p&gt;
</description>
        </item>

        <item>
        <title>SQL Server sproc performance trouble?</title>
        <link>http://arktronic.com/weblog/2009-09-01/sql-server-sproc-performance-trouble/</link>
        <pubDate>Tue, 01 Sep 2009 17:58:06 +0000</pubDate>
        <guid isPermaLink="false">ID node/6 on http://arktronic.com</guid>
        <description>&lt;p&gt;Despite this post&apos;s title looking a bit like a spam subject line, this is a serious post about an issue we ran into today at work. We have a stored procedure that gathers some statistics for us, and a really strange thing was happening with it. When run from SSMS, it took less than one second to execute, but when run from code, it actually timed out while executing over two minutes. Why would it run so fast through SSMS and yet so slow through our code?&lt;/p&gt;
&lt;p&gt;The answer, it turns out, is a SQL Server feature called &amp;quot;parameter sniffing&amp;quot;. It is supposed to optimize the query by looking at the actual parameters that are being passed in, instead of generating a generic execution plan. However, sometimes it can cause performance issues instead of alleviating them. The reasons for that are discussed in the &lt;a href=&quot;http://blogs.msdn.com/queryoptteam/archive/2006/03/31/565991.aspx&quot;&gt;Microsoft Query Optimization Team&apos;s blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The fastest and easiest solution is to completely disable parameter sniffing. To do that, simply declare a local variable for each passed in parameter, assign the parameter values to those variables, and use them instead of the parameters inside the query. While that might not be the most efficient thing to do as it skips some optimization, it solves the problem at hand.&lt;/p&gt;
</description>
        </item>

</channel>
</rss>
